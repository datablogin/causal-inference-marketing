{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing Different: Identifying Causal Relationships Others Miss\n",
    "\n",
    "> *\"While others see complexity, innovators see clarity\"* - This tutorial develops your causal intuition to spot hidden relationships, unmask confounders, and question conventional wisdom.\n",
    "\n",
    "## The Art of Causal Vision\n",
    "\n",
    "Most analysts stop at correlation. They see patterns in data and assume they understand causality. **Innovators see deeper.** They question assumptions, hunt for hidden confounders, and discover causal mechanisms others miss entirely.\n",
    "\n",
    "### What You'll Develop\n",
    "\n",
    "- **Causal intuition** to spot spurious relationships\n",
    "- **Confounder detection** skills to unmask hidden variables\n",
    "- **Mechanism thinking** to understand how causality flows\n",
    "- **Diagnostic tools** to test your causal theories\n",
    "- **Skeptical mindset** to question apparent relationships\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Consider these \"obvious\" relationships:\n",
    "- Ice cream sales are correlated with drowning deaths\n",
    "- Countries with more storks have higher birth rates\n",
    "- Students who take notes on laptops perform worse\n",
    "- Companies with diverse boards are more profitable\n",
    "\n",
    "**What's really going on here? Let's find out.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: The Detective's Toolkit\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, os.path.join(project_root, \"libs\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Our causal detection arsenal\n",
    "from causal_inference.core.base import CovariateData, OutcomeData, TreatmentData\n",
    "from causal_inference.diagnostics.balance import check_covariate_balance\n",
    "from causal_inference.estimators.aipw import AIPWEstimator\n",
    "from causal_inference.estimators.causal_forest import CausalForestEstimator\n",
    "from causal_inference.estimators.g_computation import GComputationEstimator\n",
    "\n",
    "# Set up our detective workspace\n",
    "np.random.seed(42)\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "print(\"üîç Detective Mode: ACTIVATED\")\n",
    "print(\"üß† Causal Vision: ENHANCED\")\n",
    "print(\"üí° Ready to see what others miss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 1: The Ice Cream Paradox\n",
    "\n",
    "**The Apparent Relationship**: Ice cream sales are strongly correlated with drowning deaths. Does ice cream consumption cause drowning?\n",
    "\n",
    "**The Obvious Answer**: No, that's ridiculous!\n",
    "\n",
    "**The Innovation**: Let's prove it systematically and learn how to detect such spurious relationships in less obvious cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 1: The Ice Cream Paradox - Unmasking Spurious Correlation\n",
    "\n",
    "\n",
    "def generate_ice_cream_drowning_data(n_months=60):\n",
    "    \"\"\"\n",
    "    Generate data showing spurious correlation between ice cream and drowning\n",
    "    True confounder: Temperature/Season\n",
    "    \"\"\"\n",
    "    months = np.arange(n_months)\n",
    "\n",
    "    # Seasonal pattern: temperature varies sinusoidally\n",
    "    temperature = (\n",
    "        60 + 25 * np.sin(2 * np.pi * months / 12) + np.random.normal(0, 3, n_months)\n",
    "    )\n",
    "\n",
    "    # Ice cream sales driven by temperature (and marketing)\n",
    "    marketing_spend = np.random.normal(1000, 200, n_months)  # Random marketing\n",
    "    ice_cream_sales = (\n",
    "        50  # Base sales\n",
    "        + 3 * temperature  # Temperature drives sales\n",
    "        + 0.02 * marketing_spend  # Marketing effect\n",
    "        + np.random.normal(0, 10, n_months)\n",
    "    )  # Random variation\n",
    "\n",
    "    # Drowning deaths driven by temperature/season (NOT ice cream!)\n",
    "    # More people swim when it's warm -> more drowning risk\n",
    "    swimming_activity = 10 + 0.5 * temperature + np.random.normal(0, 2, n_months)\n",
    "    drowning_deaths = (\n",
    "        2  # Base rate\n",
    "        + 0.08 * swimming_activity  # Swimming activity causes drowning\n",
    "        + 0 * ice_cream_sales  # ICE CREAM DOES NOT CAUSE DROWNING!\n",
    "        + np.random.poisson(1, n_months)\n",
    "    )  # Random variation\n",
    "\n",
    "    # Create seasonal indicators\n",
    "    season = np.array(\n",
    "        [\n",
    "            \"Winter\",\n",
    "            \"Winter\",\n",
    "            \"Spring\",\n",
    "            \"Spring\",\n",
    "            \"Spring\",\n",
    "            \"Summer\",\n",
    "            \"Summer\",\n",
    "            \"Summer\",\n",
    "            \"Fall\",\n",
    "            \"Fall\",\n",
    "            \"Fall\",\n",
    "            \"Winter\",\n",
    "        ]\n",
    "    )\n",
    "    season_labels = [season[month % 12] for month in months]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"month\": months,\n",
    "            \"temperature\": temperature,\n",
    "            \"ice_cream_sales\": ice_cream_sales,\n",
    "            \"drowning_deaths\": drowning_deaths,\n",
    "            \"marketing_spend\": marketing_spend,\n",
    "            \"swimming_activity\": swimming_activity,\n",
    "            \"season\": season_labels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate the data\n",
    "ice_cream_data = generate_ice_cream_drowning_data(60)\n",
    "\n",
    "print(\"üç¶ Ice Cream and Drowning Dataset Generated\")\n",
    "print(\"Data spans 5 years (60 months) with seasonal variation\")\n",
    "print(\"\\nData preview:\")\n",
    "print(ice_cream_data.head())\n",
    "\n",
    "# Calculate the spurious correlation\n",
    "spurious_correlation = np.corrcoef(\n",
    "    ice_cream_data[\"ice_cream_sales\"], ice_cream_data[\"drowning_deaths\"]\n",
    ")[0, 1]\n",
    "\n",
    "print(\"\\nüö® SPURIOUS CORRELATION DETECTED!\")\n",
    "print(f\"üìä Ice cream sales vs drowning deaths: r = {spurious_correlation:.3f}\")\n",
    "print(\"üìà This appears to be a strong positive relationship!\")\n",
    "print(\"ü§î But we know ice cream doesn't cause drowning...\")\n",
    "\n",
    "# Visualize the apparent relationship\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Spurious correlation plot\n",
    "axes[0, 0].scatter(\n",
    "    ice_cream_data[\"ice_cream_sales\"],\n",
    "    ice_cream_data[\"drowning_deaths\"],\n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    ")\n",
    "z = np.polyfit(ice_cream_data[\"ice_cream_sales\"], ice_cream_data[\"drowning_deaths\"], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(\n",
    "    ice_cream_data[\"ice_cream_sales\"],\n",
    "    p(ice_cream_data[\"ice_cream_sales\"]),\n",
    "    \"r--\",\n",
    "    alpha=0.8,\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[0, 0].set_title(\n",
    "    f\"SPURIOUS: Ice Cream vs Drowning\\nr = {spurious_correlation:.3f} (MISLEADING!)\"\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Ice Cream Sales\")\n",
    "axes[0, 0].set_ylabel(\"Drowning Deaths\")\n",
    "\n",
    "# Time series revealing the pattern\n",
    "ax1 = axes[0, 1]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "line1 = ax1.plot(\n",
    "    ice_cream_data[\"month\"],\n",
    "    ice_cream_data[\"ice_cream_sales\"],\n",
    "    \"b-\",\n",
    "    label=\"Ice Cream Sales\",\n",
    "    linewidth=2,\n",
    ")\n",
    "line2 = ax2.plot(\n",
    "    ice_cream_data[\"month\"],\n",
    "    ice_cream_data[\"drowning_deaths\"],\n",
    "    \"r-\",\n",
    "    label=\"Drowning Deaths\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(\"Month\")\n",
    "ax1.set_ylabel(\"Ice Cream Sales\", color=\"b\")\n",
    "ax2.set_ylabel(\"Drowning Deaths\", color=\"r\")\n",
    "ax1.set_title(\"Time Series: Both Follow Seasonal Pattern\")\n",
    "\n",
    "# The true confounder: Temperature\n",
    "temp_ice_corr = np.corrcoef(\n",
    "    ice_cream_data[\"temperature\"], ice_cream_data[\"ice_cream_sales\"]\n",
    ")[0, 1]\n",
    "temp_drown_corr = np.corrcoef(\n",
    "    ice_cream_data[\"temperature\"], ice_cream_data[\"drowning_deaths\"]\n",
    ")[0, 1]\n",
    "\n",
    "axes[1, 0].scatter(\n",
    "    ice_cream_data[\"temperature\"],\n",
    "    ice_cream_data[\"ice_cream_sales\"],\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    label=f\"Ice Cream (r={temp_ice_corr:.2f})\",\n",
    ")\n",
    "axes[1, 0].scatter(\n",
    "    ice_cream_data[\"temperature\"],\n",
    "    ice_cream_data[\"drowning_deaths\"] * 20,\n",
    "    alpha=0.7,\n",
    "    color=\"red\",\n",
    "    label=f\"Drowning√ó20 (r={temp_drown_corr:.2f})\",\n",
    ")\n",
    "axes[1, 0].set_title(\"TRUE CAUSE: Temperature Drives Both\")\n",
    "axes[1, 0].set_xlabel(\"Temperature (¬∞F)\")\n",
    "axes[1, 0].set_ylabel(\"Value\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Seasonal breakdown\n",
    "seasonal_means = ice_cream_data.groupby(\"season\")[\n",
    "    [\"ice_cream_sales\", \"drowning_deaths\"]\n",
    "].mean()\n",
    "x = np.arange(len(seasonal_means))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 1].bar(\n",
    "    x - width / 2,\n",
    "    seasonal_means[\"ice_cream_sales\"] / 10,\n",
    "    width,\n",
    "    label=\"Ice Cream Sales (√∑10)\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = axes[1, 1].bar(\n",
    "    x + width / 2,\n",
    "    seasonal_means[\"drowning_deaths\"],\n",
    "    width,\n",
    "    label=\"Drowning Deaths\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "axes[1, 1].set_title(\"Seasonal Pattern: Both Peak in Summer\")\n",
    "axes[1, 1].set_xlabel(\"Season\")\n",
    "axes[1, 1].set_ylabel(\"Value\")\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(seasonal_means.index)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç DETECTIVE INSIGHTS:\")\n",
    "print(f\"üå°Ô∏è Temperature ‚Üî Ice Cream: r = {temp_ice_corr:.3f}\")\n",
    "print(f\"üå°Ô∏è Temperature ‚Üî Drowning: r = {temp_drown_corr:.3f}\")\n",
    "print(f\"üç¶ Ice Cream ‚Üî Drowning: r = {spurious_correlation:.3f} (SPURIOUS!)\")\n",
    "print(\"\\nüí° The real story: Temperature causes both ice cream sales AND drowning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasking the Spurious Relationship: Controlling for the Confounder\n",
    "\n",
    "print(\"üïµÔ∏è CAUSAL INVESTIGATION: Controlling for Temperature\")\n",
    "\n",
    "# Prepare data for causal analysis\n",
    "treatment = TreatmentData(\n",
    "    values=ice_cream_data[\"ice_cream_sales\"],\n",
    "    name=\"ice_cream_sales\",\n",
    "    treatment_type=\"continuous\",\n",
    ")\n",
    "\n",
    "outcome = OutcomeData(\n",
    "    values=ice_cream_data[\"drowning_deaths\"],\n",
    "    name=\"drowning_deaths\",\n",
    "    outcome_type=\"continuous\",\n",
    ")\n",
    "\n",
    "# The key: Include temperature as a confounder\n",
    "covariates = CovariateData(\n",
    "    values=ice_cream_data[[\"temperature\", \"marketing_spend\"]],\n",
    "    names=[\"temperature\", \"marketing_spend\"],\n",
    ")\n",
    "\n",
    "# Estimate without controlling for confounders (BIASED)\n",
    "naive_estimator = GComputationEstimator(model_type=\"linear\", bootstrap_samples=100)\n",
    "naive_estimator.fit(treatment, outcome)  # No covariates!\n",
    "naive_effect = naive_estimator.estimate_ate()\n",
    "\n",
    "print(\"\\n‚ùå NAIVE ESTIMATE (ignoring confounders):\")\n",
    "print(\n",
    "    f\"üìä Effect of 1 unit ice cream sales: {naive_effect.ate:.4f} more drowning deaths\"\n",
    ")\n",
    "print(\n",
    "    f\"üìà 95% CI: [{naive_effect.confidence_interval[0]:.4f}, {naive_effect.confidence_interval[1]:.4f}]\"\n",
    ")\n",
    "print(\"üö® This suggests ice cream causes drowning! (WRONG!)\")\n",
    "\n",
    "# Estimate controlling for confounders (UNBIASED)\n",
    "controlled_estimator = GComputationEstimator(model_type=\"linear\", bootstrap_samples=100)\n",
    "controlled_estimator.fit(treatment, outcome, covariates)  # Include covariates!\n",
    "controlled_effect = controlled_estimator.estimate_ate()\n",
    "\n",
    "print(\"\\n‚úÖ CONTROLLED ESTIMATE (accounting for temperature):\")\n",
    "print(\n",
    "    f\"üìä Effect of 1 unit ice cream sales: {controlled_effect.ate:.4f} more drowning deaths\"\n",
    ")\n",
    "print(\n",
    "    f\"üìà 95% CI: [{controlled_effect.confidence_interval[0]:.4f}, {controlled_effect.confidence_interval[1]:.4f}]\"\n",
    ")\n",
    "\n",
    "if abs(controlled_effect.ate) < 0.01:  # Near zero\n",
    "    print(\"‚ú® REVELATION: The effect disappears when we control for temperature!\")\n",
    "    print(\"üéØ Ice cream does NOT cause drowning!\")\n",
    "else:\n",
    "    print(\"ü§î Small residual effect remains - might be other confounders\")\n",
    "\n",
    "# Demonstrate with partial correlation\n",
    "\n",
    "# Partial correlation: Ice cream and drowning, controlling for temperature\n",
    "# This is equivalent to correlating the residuals after regressing out temperature\n",
    "\n",
    "# Regress ice cream on temperature\n",
    "ice_cream_temp_coef = np.polyfit(\n",
    "    ice_cream_data[\"temperature\"], ice_cream_data[\"ice_cream_sales\"], 1\n",
    ")\n",
    "ice_cream_residuals = ice_cream_data[\"ice_cream_sales\"] - np.polyval(\n",
    "    ice_cream_temp_coef, ice_cream_data[\"temperature\"]\n",
    ")\n",
    "\n",
    "# Regress drowning on temperature\n",
    "drowning_temp_coef = np.polyfit(\n",
    "    ice_cream_data[\"temperature\"], ice_cream_data[\"drowning_deaths\"], 1\n",
    ")\n",
    "drowning_residuals = ice_cream_data[\"drowning_deaths\"] - np.polyval(\n",
    "    drowning_temp_coef, ice_cream_data[\"temperature\"]\n",
    ")\n",
    "\n",
    "# Partial correlation\n",
    "partial_correlation = np.corrcoef(ice_cream_residuals, drowning_residuals)[0, 1]\n",
    "\n",
    "print(\"\\nüßÆ PARTIAL CORRELATION ANALYSIS:\")\n",
    "print(f\"üìä Raw correlation: {spurious_correlation:.4f} (MISLEADING)\")\n",
    "print(\n",
    "    f\"üéØ Partial correlation (controlling for temperature): {partial_correlation:.4f}\"\n",
    ")\n",
    "\n",
    "reduction = abs(spurious_correlation) - abs(partial_correlation)\n",
    "print(\n",
    "    f\"‚¨áÔ∏è Spurious correlation reduced by: {reduction:.4f} ({reduction/abs(spurious_correlation)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Visualize the revelation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Before: Spurious correlation\n",
    "axes[0].scatter(\n",
    "    ice_cream_data[\"ice_cream_sales\"],\n",
    "    ice_cream_data[\"drowning_deaths\"],\n",
    "    alpha=0.7,\n",
    "    s=50,\n",
    ")\n",
    "z = np.polyfit(ice_cream_data[\"ice_cream_sales\"], ice_cream_data[\"drowning_deaths\"], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(\n",
    "    ice_cream_data[\"ice_cream_sales\"],\n",
    "    p(ice_cream_data[\"ice_cream_sales\"]),\n",
    "    \"r--\",\n",
    "    alpha=0.8,\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[0].set_title(\n",
    "    f\"Before: Raw Correlation\\nr = {spurious_correlation:.3f} (MISLEADING!)\",\n",
    "    color=\"red\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Ice Cream Sales\")\n",
    "axes[0].set_ylabel(\"Drowning Deaths\")\n",
    "\n",
    "# After: Controlling for temperature\n",
    "axes[1].scatter(ice_cream_residuals, drowning_residuals, alpha=0.7, s=50)\n",
    "if abs(partial_correlation) > 0.1:\n",
    "    z = np.polyfit(ice_cream_residuals, drowning_residuals, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1].plot(\n",
    "        ice_cream_residuals, p(ice_cream_residuals), \"g--\", alpha=0.8, linewidth=2\n",
    "    )\n",
    "axes[1].axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "axes[1].axvline(x=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "axes[1].set_title(\n",
    "    f\"After: Controlling for Temperature\\nr = {partial_correlation:.3f} (TRUE RELATIONSHIP!)\",\n",
    "    color=\"green\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Ice Cream Sales (residuals)\")\n",
    "axes[1].set_ylabel(\"Drowning Deaths (residuals)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ CAUSAL DETECTIVE SUCCESS!\")\n",
    "print(\"üîç We unmasked the spurious relationship!\")\n",
    "print(\"üå°Ô∏è Temperature was the hidden confounder all along!\")\n",
    "print(\"üí° This is how we 'see different' - by looking for hidden variables!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2: The Laptop Learning Paradox\n",
    "\n",
    "**The Observed Pattern**: Students who take notes on laptops perform worse than those using pen and paper.\n",
    "\n",
    "**The Quick Conclusion**: Laptops hurt learning!\n",
    "\n",
    "**The Innovation**: Let's dig deeper and discover the hidden mechanisms at work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 2: The Laptop Learning Paradox - Hidden Mechanisms\n",
    "\n",
    "\n",
    "def generate_laptop_learning_data(n_students=500):\n",
    "    \"\"\"\n",
    "    Generate student performance data with hidden confounders\n",
    "    \"\"\"\n",
    "    # Student characteristics (hidden confounders)\n",
    "    baseline_ability = np.random.normal(75, 15, n_students)  # Base academic ability\n",
    "    tech_comfort = np.random.beta(2, 3, n_students)  # Tech comfort (0-1)\n",
    "    engagement_level = np.random.normal(0.7, 0.2, n_students)  # Class engagement\n",
    "\n",
    "    # Course characteristics\n",
    "    course_difficulty = np.random.choice(\n",
    "        [\"Easy\", \"Medium\", \"Hard\"], n_students, p=[0.3, 0.4, 0.3]\n",
    "    )\n",
    "    course_type = np.random.choice(\n",
    "        [\"Math\", \"Literature\", \"Science\"], n_students, p=[0.33, 0.33, 0.34]\n",
    "    )\n",
    "\n",
    "    # Note-taking method choice (NOT random!)\n",
    "    # Students choose laptops based on tech comfort and course type\n",
    "    laptop_propensity = (\n",
    "        0.3  # Base propensity\n",
    "        + 0.4 * tech_comfort  # Tech-comfortable students prefer laptops\n",
    "        + 0.2 * (course_type == \"Math\")  # Math students prefer laptops\n",
    "        + -0.1 * (course_difficulty == \"Hard\")  # Avoid laptops in hard courses\n",
    "        + np.random.normal(0, 0.15, n_students)\n",
    "    )  # Random variation\n",
    "\n",
    "    uses_laptop = (laptop_propensity > 0.5).astype(int)\n",
    "\n",
    "    # True causal mechanisms:\n",
    "    # 1. Laptops reduce deep processing (direct effect: -3 points)\n",
    "    # 2. BUT laptops help with organization (+2 points for organized students)\n",
    "    # 3. Distraction effect depends on engagement (-5 points for low engagement)\n",
    "\n",
    "    laptop_direct_effect = -3  # Reduced deep processing\n",
    "    organization_benefit = 2 * tech_comfort  # Organized students benefit\n",
    "    distraction_penalty = -5 * (\n",
    "        engagement_level < 0.5\n",
    "    )  # Low engagement = more distraction\n",
    "\n",
    "    # Final test scores\n",
    "    test_score = (\n",
    "        baseline_ability  # Base ability\n",
    "        + laptop_direct_effect * uses_laptop  # Direct laptop effect\n",
    "        + organization_benefit * uses_laptop  # Organization benefit\n",
    "        + distraction_penalty * uses_laptop  # Distraction penalty\n",
    "        + 10 * engagement_level  # Engagement helps performance\n",
    "        + -5 * (course_difficulty == \"Hard\")  # Hard courses lower scores\n",
    "        + 5 * (course_type == \"Math\") * tech_comfort  # Tech helps in math\n",
    "        + np.random.normal(0, 5, n_students)\n",
    "    )  # Random noise\n",
    "\n",
    "    test_score = np.clip(test_score, 0, 100)  # Realistic score range\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"uses_laptop\": uses_laptop,\n",
    "            \"test_score\": test_score,\n",
    "            \"baseline_ability\": baseline_ability,\n",
    "            \"tech_comfort\": tech_comfort,\n",
    "            \"engagement_level\": engagement_level,\n",
    "            \"course_difficulty\": course_difficulty,\n",
    "            \"course_type\": course_type,\n",
    "            \"laptop_propensity\": laptop_propensity,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate the data\n",
    "laptop_data = generate_laptop_learning_data(500)\n",
    "\n",
    "print(\"üíª Laptop Learning Dataset Generated\")\n",
    "print(\"Students choose laptops based on tech comfort and course characteristics\")\n",
    "print(\"\\nData preview:\")\n",
    "print(laptop_data.head())\n",
    "\n",
    "# Calculate the observed correlation\n",
    "laptop_users = laptop_data[laptop_data[\"uses_laptop\"] == 1][\"test_score\"]\n",
    "paper_users = laptop_data[laptop_data[\"uses_laptop\"] == 0][\"test_score\"]\n",
    "observed_difference = laptop_users.mean() - paper_users.mean()\n",
    "\n",
    "print(\"\\nüìä OBSERVED PATTERN:\")\n",
    "print(f\"üìù Paper users average score: {paper_users.mean():.1f}\")\n",
    "print(f\"üíª Laptop users average score: {laptop_users.mean():.1f}\")\n",
    "print(f\"üìâ Raw difference: {observed_difference:.1f} points\")\n",
    "\n",
    "if observed_difference < 0:\n",
    "    print(\"üö® Laptops appear to HURT performance!\")\n",
    "else:\n",
    "    print(\"‚ú® Laptops appear to HELP performance!\")\n",
    "\n",
    "print(\"ü§î But is this the whole story?\")\n",
    "\n",
    "# Visualize the apparent relationship and hidden patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Raw comparison\n",
    "axes[0, 0].boxplot([paper_users, laptop_users], labels=[\"Paper\", \"Laptop\"])\n",
    "axes[0, 0].set_title(\n",
    "    f\"Raw Comparison\\nLaptops appear worse by {abs(observed_difference):.1f} points\"\n",
    ")\n",
    "axes[0, 0].set_ylabel(\"Test Score\")\n",
    "\n",
    "# Selection bias: Who chooses laptops?\n",
    "laptop_choosers = laptop_data[laptop_data[\"uses_laptop\"] == 1]\n",
    "paper_choosers = laptop_data[laptop_data[\"uses_laptop\"] == 0]\n",
    "\n",
    "axes[0, 1].scatter(\n",
    "    laptop_choosers[\"tech_comfort\"],\n",
    "    laptop_choosers[\"baseline_ability\"],\n",
    "    alpha=0.6,\n",
    "    label=\"Laptop Users\",\n",
    "    s=30,\n",
    ")\n",
    "axes[0, 1].scatter(\n",
    "    paper_choosers[\"tech_comfort\"],\n",
    "    paper_choosers[\"baseline_ability\"],\n",
    "    alpha=0.6,\n",
    "    label=\"Paper Users\",\n",
    "    s=30,\n",
    ")\n",
    "axes[0, 1].set_title(\"Selection Bias: Who Chooses Laptops?\")\n",
    "axes[0, 1].set_xlabel(\"Tech Comfort\")\n",
    "axes[0, 1].set_ylabel(\"Baseline Ability\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Engagement levels by note-taking method\n",
    "axes[1, 0].boxplot(\n",
    "    [paper_choosers[\"engagement_level\"], laptop_choosers[\"engagement_level\"]],\n",
    "    labels=[\"Paper\", \"Laptop\"],\n",
    ")\n",
    "axes[1, 0].set_title(\"Engagement Levels by Method\")\n",
    "axes[1, 0].set_ylabel(\"Engagement Level\")\n",
    "\n",
    "# Course type distribution\n",
    "course_crosstab = pd.crosstab(\n",
    "    laptop_data[\"course_type\"], laptop_data[\"uses_laptop\"], normalize=\"columns\"\n",
    ")\n",
    "course_crosstab.plot(kind=\"bar\", ax=axes[1, 1], stacked=True)\n",
    "axes[1, 1].set_title(\"Course Type Distribution\")\n",
    "axes[1, 1].set_xlabel(\"Course Type\")\n",
    "axes[1, 1].set_ylabel(\"Proportion\")\n",
    "axes[1, 1].legend([\"Paper\", \"Laptop\"])\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze selection patterns\n",
    "print(\"\\nüîç SELECTION BIAS ANALYSIS:\")\n",
    "print(f\"üíª Laptop users - Tech comfort: {laptop_choosers['tech_comfort'].mean():.2f}\")\n",
    "print(f\"üìù Paper users - Tech comfort: {paper_choosers['tech_comfort'].mean():.2f}\")\n",
    "print(\n",
    "    f\"üíª Laptop users - Baseline ability: {laptop_choosers['baseline_ability'].mean():.1f}\"\n",
    ")\n",
    "print(\n",
    "    f\"üìù Paper users - Baseline ability: {paper_choosers['baseline_ability'].mean():.1f}\"\n",
    ")\n",
    "print(f\"üíª Laptop users - Engagement: {laptop_choosers['engagement_level'].mean():.2f}\")\n",
    "print(f\"üìù Paper users - Engagement: {paper_choosers['engagement_level'].mean():.2f}\")\n",
    "print(\"\\nüéØ Key insight: Laptop and paper users are systematically different!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Analysis: Accounting for Selection Bias\n",
    "\n",
    "print(\"üïµÔ∏è CAUSAL INVESTIGATION: Controlling for Student Characteristics\")\n",
    "\n",
    "# Prepare data for causal analysis\n",
    "treatment = TreatmentData(\n",
    "    values=laptop_data[\"uses_laptop\"], name=\"uses_laptop\", treatment_type=\"binary\"\n",
    ")\n",
    "\n",
    "outcome = OutcomeData(\n",
    "    values=laptop_data[\"test_score\"], name=\"test_score\", outcome_type=\"continuous\"\n",
    ")\n",
    "\n",
    "# Create comprehensive covariate set\n",
    "covariates_df = laptop_data[\n",
    "    [\"baseline_ability\", \"tech_comfort\", \"engagement_level\"]\n",
    "].copy()\n",
    "\n",
    "# Add dummy variables for categorical variables\n",
    "course_dummies = pd.get_dummies(laptop_data[\"course_difficulty\"], prefix=\"difficulty\")\n",
    "type_dummies = pd.get_dummies(laptop_data[\"course_type\"], prefix=\"type\")\n",
    "\n",
    "# Combine all covariates\n",
    "all_covariates = pd.concat([covariates_df, course_dummies, type_dummies], axis=1)\n",
    "\n",
    "covariates = CovariateData(values=all_covariates, names=list(all_covariates.columns))\n",
    "\n",
    "print(f\"Controlling for {len(all_covariates.columns)} covariates:\")\n",
    "print(\"‚Ä¢ Baseline ability, tech comfort, engagement\")\n",
    "print(\"‚Ä¢ Course difficulty and type\")\n",
    "\n",
    "# Check covariate balance before adjustment\n",
    "print(\"\\n‚öñÔ∏è COVARIATE BALANCE CHECK:\")\n",
    "balance_result = check_covariate_balance(treatment, covariates)\n",
    "balance_table = balance_result[\"balance_table\"]\n",
    "\n",
    "print(\"Variables with large imbalances:\")\n",
    "imbalanced = balance_table[abs(balance_table[\"SMD\"]) > 0.2]\n",
    "for idx, row in imbalanced.iterrows():\n",
    "    print(f\"‚Ä¢ {row['Variable']}: SMD = {row['SMD']:.3f}\")\n",
    "\n",
    "# Estimate causal effect using multiple methods\n",
    "print(\"\\nüî¨ CAUSAL ESTIMATION METHODS:\")\n",
    "\n",
    "# Method 1: G-computation\n",
    "g_comp = GComputationEstimator(model_type=\"random_forest\", bootstrap_samples=100)\n",
    "g_comp.fit(treatment, outcome, covariates)\n",
    "g_comp_effect = g_comp.estimate_ate()\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ G-COMPUTATION (Random Forest):\")\n",
    "print(f\"üìä Causal effect: {g_comp_effect.ate:.2f} points\")\n",
    "print(\n",
    "    f\"üìà 95% CI: [{g_comp_effect.confidence_interval[0]:.2f}, {g_comp_effect.confidence_interval[1]:.2f}]\"\n",
    ")\n",
    "print(f\"üéØ Significant: {g_comp_effect.is_significant}\")\n",
    "\n",
    "# Method 2: AIPW (Doubly Robust)\n",
    "aipw = AIPWEstimator(\n",
    "    outcome_model=\"random_forest\", propensity_model=\"logistic\", bootstrap_samples=100\n",
    ")\n",
    "aipw.fit(treatment, outcome, covariates)\n",
    "aipw_effect = aipw.estimate_ate()\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ AIPW (Doubly Robust):\")\n",
    "print(f\"üìä Causal effect: {aipw_effect.ate:.2f} points\")\n",
    "print(\n",
    "    f\"üìà 95% CI: [{aipw_effect.confidence_interval[0]:.2f}, {aipw_effect.confidence_interval[1]:.2f}]\"\n",
    ")\n",
    "print(f\"üéØ Significant: {aipw_effect.is_significant}\")\n",
    "\n",
    "# Compare with naive estimate\n",
    "print(\"\\nüìä COMPARISON OF ESTIMATES:\")\n",
    "print(f\"‚ùå Naive comparison: {observed_difference:.2f} points\")\n",
    "print(f\"üî¨ G-computation: {g_comp_effect.ate:.2f} points\")\n",
    "print(f\"üõ°Ô∏è AIPW (doubly robust): {aipw_effect.ate:.2f} points\")\n",
    "\n",
    "# Calculate bias correction\n",
    "bias_corrected = observed_difference - aipw_effect.ate\n",
    "print(\"\\nüéØ BIAS ANALYSIS:\")\n",
    "print(f\"üìâ Selection bias: {bias_corrected:.2f} points\")\n",
    "print(\n",
    "    f\"üìä Bias correction: {abs(bias_corrected/observed_difference)*100:.1f}% of observed effect\"\n",
    ")\n",
    "\n",
    "if abs(aipw_effect.ate) < abs(observed_difference):\n",
    "    print(\"\\n‚ú® REVELATION: The causal effect is smaller than the raw correlation!\")\n",
    "    if aipw_effect.ate > 0 and observed_difference < 0:\n",
    "        print(\n",
    "            \"üîÑ Direction reversal! Laptops might actually HELP when properly analyzed!\"\n",
    "        )\n",
    "    elif abs(aipw_effect.ate) < 2:  # Small effect\n",
    "        print(\"üìä The true effect is much smaller than it appeared!\")\n",
    "\n",
    "# Analyze heterogeneous effects\n",
    "print(\"\\nüé≠ HETEROGENEOUS EFFECTS ANALYSIS:\")\n",
    "\n",
    "# Effect by engagement level\n",
    "high_engagement = laptop_data[laptop_data[\"engagement_level\"] > 0.7]\n",
    "low_engagement = laptop_data[laptop_data[\"engagement_level\"] <= 0.5]\n",
    "\n",
    "high_eng_effect = (\n",
    "    high_engagement[high_engagement[\"uses_laptop\"] == 1][\"test_score\"].mean()\n",
    "    - high_engagement[high_engagement[\"uses_laptop\"] == 0][\"test_score\"].mean()\n",
    ")\n",
    "low_eng_effect = (\n",
    "    low_engagement[low_engagement[\"uses_laptop\"] == 1][\"test_score\"].mean()\n",
    "    - low_engagement[low_engagement[\"uses_laptop\"] == 0][\"test_score\"].mean()\n",
    ")\n",
    "\n",
    "print(f\"üî• High engagement students: {high_eng_effect:.1f} points\")\n",
    "print(f\"üò¥ Low engagement students: {low_eng_effect:.1f} points\")\n",
    "\n",
    "if high_eng_effect > low_eng_effect:\n",
    "    print(\"\\nüí° KEY INSIGHT: Laptops help engaged students but hurt distracted ones!\")\n",
    "    print(\"üéØ This explains the heterogeneous effects!\")\n",
    "\n",
    "print(\"\\nüèÜ CAUSAL DETECTIVE CONCLUSION:\")\n",
    "print(\"üïµÔ∏è The raw comparison was misleading due to selection bias\")\n",
    "print(\"üéØ True causal effects are more nuanced and context-dependent\")\n",
    "print(\"üß† This is why we need to 'see different' - control for confounders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 3: The Diversity Paradox in Corporate Performance\n",
    "\n",
    "**The Apparent Pattern**: Companies with diverse leadership teams are more profitable.\n",
    "\n",
    "**The Quick Conclusion**: Diversity causes higher profits!\n",
    "\n",
    "**The Innovation**: Let's uncover the complex causal mechanisms and confounders at play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 3: Corporate Diversity and Performance - Complex Causality\n",
    "\n",
    "\n",
    "def generate_corporate_diversity_data(n_companies=300):\n",
    "    \"\"\"\n",
    "    Generate corporate data with complex causal relationships\n",
    "    between diversity and performance\n",
    "    \"\"\"\n",
    "    # Company characteristics (confounders)\n",
    "    company_size = np.random.lognormal(8, 1.5, n_companies)  # Log-normal distribution\n",
    "    company_age = np.random.gamma(2, 10, n_companies)  # Gamma distribution for age\n",
    "    industry_innovation = np.random.beta(\n",
    "        2, 3, n_companies\n",
    "    )  # Innovation level by industry\n",
    "    market_competitiveness = np.random.uniform(0.2, 0.9, n_companies)\n",
    "\n",
    "    # Geographic and cultural factors\n",
    "    urban_location = np.random.binomial(1, 0.6, n_companies)  # Urban vs rural\n",
    "    coastal_location = np.random.binomial(1, 0.4, n_companies)  # Coastal premium\n",
    "\n",
    "    # CEO and leadership characteristics\n",
    "    ceo_experience = np.random.gamma(2, 5, n_companies)  # Years of experience\n",
    "    leadership_quality = np.random.beta(3, 2, n_companies)  # Leadership quality score\n",
    "\n",
    "    # Diversity is NOT randomly assigned!\n",
    "    # It's influenced by:\n",
    "    # 1. Location (urban/coastal areas more diverse)\n",
    "    # 2. Industry innovation (innovative industries embrace diversity)\n",
    "    # 3. Company size (larger companies have more diversity)\n",
    "    # 4. Leadership quality (better leaders value diversity)\n",
    "\n",
    "    diversity_propensity = (\n",
    "        0.2  # Base level\n",
    "        + 0.3 * urban_location  # Urban areas more diverse\n",
    "        + 0.2 * coastal_location  # Coastal areas more diverse\n",
    "        + 0.3 * industry_innovation  # Innovative industries\n",
    "        + 0.1 * np.log(company_size) / 10  # Larger companies\n",
    "        + 0.2 * leadership_quality  # Quality leaders value diversity\n",
    "        + np.random.normal(0, 0.1, n_companies)\n",
    "    )\n",
    "\n",
    "    diversity_score = np.clip(diversity_propensity, 0.1, 0.9)  # 0-1 scale\n",
    "\n",
    "    # True causal mechanisms for diversity effect:\n",
    "    # 1. Direct innovation benefit: +5% profit\n",
    "    # 2. Market expansion benefit: +3% profit\n",
    "    # 3. BUT communication costs: -2% profit\n",
    "    # 4. Interaction with innovation: diversity helps more in innovative industries\n",
    "\n",
    "    diversity_innovation_effect = 5 * diversity_score  # Innovation benefit\n",
    "    diversity_market_effect = 3 * diversity_score  # Market expansion\n",
    "    diversity_communication_cost = -2 * diversity_score  # Communication overhead\n",
    "    diversity_innovation_interaction = (\n",
    "        4 * diversity_score * industry_innovation\n",
    "    )  # Synergy\n",
    "\n",
    "    # Profit margin (our outcome of interest)\n",
    "    base_profit = 8  # Base profit margin %\n",
    "\n",
    "    profit_margin = (\n",
    "        base_profit\n",
    "        + diversity_innovation_effect  # Diversity innovation benefit\n",
    "        + diversity_market_effect  # Market expansion benefit\n",
    "        + diversity_communication_cost  # Communication costs\n",
    "        + diversity_innovation_interaction  # Synergy effect\n",
    "        + 3 * leadership_quality  # Leadership quality matters\n",
    "        + 2 * industry_innovation  # Industry innovation helps\n",
    "        + 1 * np.log(company_size) / 10  # Size advantages\n",
    "        + 2 * market_competitiveness  # Competition drives efficiency\n",
    "        + 1 * urban_location  # Urban premium\n",
    "        + np.random.normal(0, 2, n_companies)\n",
    "    )  # Random variation\n",
    "\n",
    "    profit_margin = np.clip(profit_margin, 1, 25)  # Realistic range\n",
    "\n",
    "    # Create categorical variables for analysis\n",
    "    high_diversity = (diversity_score > np.median(diversity_score)).astype(int)\n",
    "    large_company = (company_size > np.median(company_size)).astype(int)\n",
    "    innovative_industry = (industry_innovation > 0.6).astype(int)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"diversity_score\": diversity_score,\n",
    "            \"high_diversity\": high_diversity,\n",
    "            \"profit_margin\": profit_margin,\n",
    "            \"company_size\": company_size,\n",
    "            \"large_company\": large_company,\n",
    "            \"company_age\": company_age,\n",
    "            \"industry_innovation\": industry_innovation,\n",
    "            \"innovative_industry\": innovative_industry,\n",
    "            \"market_competitiveness\": market_competitiveness,\n",
    "            \"urban_location\": urban_location,\n",
    "            \"coastal_location\": coastal_location,\n",
    "            \"ceo_experience\": ceo_experience,\n",
    "            \"leadership_quality\": leadership_quality,\n",
    "            \"diversity_propensity\": diversity_propensity,\n",
    "        }\n",
    "    ), {\n",
    "        \"innovation_effect\": 5,\n",
    "        \"market_effect\": 3,\n",
    "        \"communication_cost\": -2,\n",
    "        \"net_effect\": 6,  # 5 + 3 - 2\n",
    "    }\n",
    "\n",
    "\n",
    "# Generate the data\n",
    "corp_data, true_effects = generate_corporate_diversity_data(300)\n",
    "\n",
    "print(\"üè¢ Corporate Diversity and Performance Dataset Generated\")\n",
    "print(f\"True net diversity effect: +{true_effects['net_effect']}% profit margin\")\n",
    "print(\"But diversity is not randomly assigned!\")\n",
    "print(\"\\nData preview:\")\n",
    "print(corp_data.head())\n",
    "\n",
    "# Calculate observed correlation\n",
    "diversity_correlation = np.corrcoef(\n",
    "    corp_data[\"diversity_score\"], corp_data[\"profit_margin\"]\n",
    ")[0, 1]\n",
    "high_div_companies = corp_data[corp_data[\"high_diversity\"] == 1]\n",
    "low_div_companies = corp_data[corp_data[\"high_diversity\"] == 0]\n",
    "observed_difference = (\n",
    "    high_div_companies[\"profit_margin\"].mean()\n",
    "    - low_div_companies[\"profit_margin\"].mean()\n",
    ")\n",
    "\n",
    "print(\"\\nüìä OBSERVED PATTERNS:\")\n",
    "print(f\"üìà Diversity-Profit correlation: r = {diversity_correlation:.3f}\")\n",
    "print(\n",
    "    f\"üèÜ High diversity companies: {high_div_companies['profit_margin'].mean():.1f}% profit\"\n",
    ")\n",
    "print(\n",
    "    f\"üìâ Low diversity companies: {low_div_companies['profit_margin'].mean():.1f}% profit\"\n",
    ")\n",
    "print(f\"‚¨ÜÔ∏è Raw difference: {observed_difference:.1f} percentage points\")\n",
    "print(\"‚ú® Diversity appears to boost profits!\")\n",
    "\n",
    "# Visualize the complex relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Main relationship\n",
    "axes[0, 0].scatter(corp_data[\"diversity_score\"], corp_data[\"profit_margin\"], alpha=0.6)\n",
    "z = np.polyfit(corp_data[\"diversity_score\"], corp_data[\"profit_margin\"], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(\n",
    "    corp_data[\"diversity_score\"], p(corp_data[\"diversity_score\"]), \"r--\", alpha=0.8\n",
    ")\n",
    "axes[0, 0].set_title(f\"Diversity vs Profit Margin\\nr = {diversity_correlation:.3f}\")\n",
    "axes[0, 0].set_xlabel(\"Diversity Score\")\n",
    "axes[0, 0].set_ylabel(\"Profit Margin (%)\")\n",
    "\n",
    "# Confounder 1: Company size\n",
    "size_diversity_corr = np.corrcoef(\n",
    "    corp_data[\"company_size\"], corp_data[\"diversity_score\"]\n",
    ")[0, 1]\n",
    "size_profit_corr = np.corrcoef(corp_data[\"company_size\"], corp_data[\"profit_margin\"])[\n",
    "    0, 1\n",
    "]\n",
    "axes[0, 1].scatter(\n",
    "    np.log(corp_data[\"company_size\"]), corp_data[\"diversity_score\"], alpha=0.6\n",
    ")\n",
    "axes[0, 1].set_title(f\"Size ‚Üí Diversity\\nr = {size_diversity_corr:.3f}\")\n",
    "axes[0, 1].set_xlabel(\"Log(Company Size)\")\n",
    "axes[0, 1].set_ylabel(\"Diversity Score\")\n",
    "\n",
    "# Confounder 2: Innovation level\n",
    "innovation_diversity_corr = np.corrcoef(\n",
    "    corp_data[\"industry_innovation\"], corp_data[\"diversity_score\"]\n",
    ")[0, 1]\n",
    "innovation_profit_corr = np.corrcoef(\n",
    "    corp_data[\"industry_innovation\"], corp_data[\"profit_margin\"]\n",
    ")[0, 1]\n",
    "axes[0, 2].scatter(\n",
    "    corp_data[\"industry_innovation\"], corp_data[\"diversity_score\"], alpha=0.6\n",
    ")\n",
    "axes[0, 2].set_title(f\"Innovation ‚Üí Diversity\\nr = {innovation_diversity_corr:.3f}\")\n",
    "axes[0, 2].set_xlabel(\"Industry Innovation\")\n",
    "axes[0, 2].set_ylabel(\"Diversity Score\")\n",
    "\n",
    "# Geographic patterns\n",
    "urban_div = corp_data.groupby(\"urban_location\")[\"diversity_score\"].mean()\n",
    "coastal_div = corp_data.groupby(\"coastal_location\")[\"diversity_score\"].mean()\n",
    "axes[1, 0].bar([\"Rural\", \"Urban\"], urban_div, alpha=0.8)\n",
    "axes[1, 0].set_title(\"Diversity by Location\")\n",
    "axes[1, 0].set_ylabel(\"Average Diversity Score\")\n",
    "\n",
    "# Leadership quality patterns\n",
    "leadership_diversity_corr = np.corrcoef(\n",
    "    corp_data[\"leadership_quality\"], corp_data[\"diversity_score\"]\n",
    ")[0, 1]\n",
    "axes[1, 1].scatter(\n",
    "    corp_data[\"leadership_quality\"], corp_data[\"diversity_score\"], alpha=0.6\n",
    ")\n",
    "axes[1, 1].set_title(f\"Leadership ‚Üí Diversity\\nr = {leadership_diversity_corr:.3f}\")\n",
    "axes[1, 1].set_xlabel(\"Leadership Quality\")\n",
    "axes[1, 1].set_ylabel(\"Diversity Score\")\n",
    "\n",
    "# Interaction effects\n",
    "innovative_companies = corp_data[corp_data[\"innovative_industry\"] == 1]\n",
    "traditional_companies = corp_data[corp_data[\"innovative_industry\"] == 0]\n",
    "innov_corr = np.corrcoef(\n",
    "    innovative_companies[\"diversity_score\"], innovative_companies[\"profit_margin\"]\n",
    ")[0, 1]\n",
    "trad_corr = np.corrcoef(\n",
    "    traditional_companies[\"diversity_score\"], traditional_companies[\"profit_margin\"]\n",
    ")[0, 1]\n",
    "\n",
    "axes[1, 2].scatter(\n",
    "    innovative_companies[\"diversity_score\"],\n",
    "    innovative_companies[\"profit_margin\"],\n",
    "    alpha=0.6,\n",
    "    label=f\"Innovative (r={innov_corr:.2f})\",\n",
    ")\n",
    "axes[1, 2].scatter(\n",
    "    traditional_companies[\"diversity_score\"],\n",
    "    traditional_companies[\"profit_margin\"],\n",
    "    alpha=0.6,\n",
    "    label=f\"Traditional (r={trad_corr:.2f})\",\n",
    ")\n",
    "axes[1, 2].set_title(\"Diversity Effect by Industry Type\")\n",
    "axes[1, 2].set_xlabel(\"Diversity Score\")\n",
    "axes[1, 2].set_ylabel(\"Profit Margin (%)\")\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç CONFOUNDING ANALYSIS:\")\n",
    "print(f\"üèóÔ∏è Company size ‚Üî Diversity: r = {size_diversity_corr:.3f}\")\n",
    "print(f\"üí° Innovation ‚Üî Diversity: r = {innovation_diversity_corr:.3f}\")\n",
    "print(f\"üëî Leadership ‚Üî Diversity: r = {leadership_diversity_corr:.3f}\")\n",
    "print(f\"üåÜ Urban companies have {urban_div[1]:.2f} vs {urban_div[0]:.2f} diversity\")\n",
    "print(\"\\nüéØ Key insight: Diversity is correlated with many success factors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Causal Analysis: Unraveling Complex Causality\n",
    "\n",
    "print(\"üïµÔ∏è ADVANCED CAUSAL INVESTIGATION: Corporate Diversity Effects\")\n",
    "\n",
    "# Prepare comprehensive covariate set\n",
    "covariates_corp = corp_data[\n",
    "    [\n",
    "        \"company_size\",\n",
    "        \"company_age\",\n",
    "        \"industry_innovation\",\n",
    "        \"market_competitiveness\",\n",
    "        \"urban_location\",\n",
    "        \"coastal_location\",\n",
    "        \"ceo_experience\",\n",
    "        \"leadership_quality\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# Log-transform company size for better modeling\n",
    "covariates_corp[\"log_company_size\"] = np.log(covariates_corp[\"company_size\"])\n",
    "covariates_corp = covariates_corp.drop(\"company_size\", axis=1)\n",
    "\n",
    "treatment_corp = TreatmentData(\n",
    "    values=corp_data[\"high_diversity\"], name=\"high_diversity\", treatment_type=\"binary\"\n",
    ")\n",
    "\n",
    "outcome_corp = OutcomeData(\n",
    "    values=corp_data[\"profit_margin\"], name=\"profit_margin\", outcome_type=\"continuous\"\n",
    ")\n",
    "\n",
    "covariates_corp_data = CovariateData(\n",
    "    values=covariates_corp, names=list(covariates_corp.columns)\n",
    ")\n",
    "\n",
    "print(f\"Controlling for {len(covariates_corp.columns)} confounders:\")\n",
    "for col in covariates_corp.columns:\n",
    "    print(f\"‚Ä¢ {col}\")\n",
    "\n",
    "# Check balance\n",
    "print(\"\\n‚öñÔ∏è BALANCE CHECK BEFORE ADJUSTMENT:\")\n",
    "balance_result_corp = check_covariate_balance(treatment_corp, covariates_corp_data)\n",
    "balance_table_corp = balance_result_corp[\"balance_table\"]\n",
    "\n",
    "severe_imbalances = balance_table_corp[abs(balance_table_corp[\"SMD\"]) > 0.3]\n",
    "print(\"Variables with severe imbalances (SMD > 0.3):\")\n",
    "for idx, row in severe_imbalances.iterrows():\n",
    "    print(f\"‚Ä¢ {row['Variable']}: SMD = {row['SMD']:.3f}\")\n",
    "\n",
    "# Multiple causal estimation methods\n",
    "print(\"\\nüî¨ CAUSAL ESTIMATION WITH MULTIPLE METHODS:\")\n",
    "\n",
    "# Method 1: Linear G-computation\n",
    "g_comp_linear = GComputationEstimator(model_type=\"linear\", bootstrap_samples=100)\n",
    "g_comp_linear.fit(treatment_corp, outcome_corp, covariates_corp_data)\n",
    "linear_effect = g_comp_linear.estimate_ate()\n",
    "\n",
    "# Method 2: Random Forest G-computation\n",
    "g_comp_rf = GComputationEstimator(model_type=\"random_forest\", bootstrap_samples=100)\n",
    "g_comp_rf.fit(treatment_corp, outcome_corp, covariates_corp_data)\n",
    "rf_effect = g_comp_rf.estimate_ate()\n",
    "\n",
    "# Method 3: AIPW\n",
    "aipw_corp = AIPWEstimator(\n",
    "    outcome_model=\"random_forest\", propensity_model=\"logistic\", bootstrap_samples=100\n",
    ")\n",
    "aipw_corp.fit(treatment_corp, outcome_corp, covariates_corp_data)\n",
    "aipw_corp_effect = aipw_corp.estimate_ate()\n",
    "\n",
    "# Method 4: Causal Forest (heterogeneous effects)\n",
    "try:\n",
    "    causal_forest = CausalForestEstimator(\n",
    "        n_estimators=100,\n",
    "        bootstrap_samples=50,  # Reduced for speed\n",
    "    )\n",
    "    causal_forest.fit(treatment_corp, outcome_corp, covariates_corp_data)\n",
    "    forest_effect = causal_forest.estimate_ate()\n",
    "    has_forest = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Causal Forest not available: {e}\")\n",
    "    has_forest = False\n",
    "\n",
    "# Present results\n",
    "print(\"\\nüìä CAUSAL EFFECT ESTIMATES:\")\n",
    "print(f\"‚ùå Naive comparison: +{observed_difference:.2f} percentage points\")\n",
    "print(\n",
    "    f\"üìê Linear G-computation: +{linear_effect.ate:.2f} pp [CI: {linear_effect.confidence_interval[0]:.2f}, {linear_effect.confidence_interval[1]:.2f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"üå≥ Random Forest G-comp: +{rf_effect.ate:.2f} pp [CI: {rf_effect.confidence_interval[0]:.2f}, {rf_effect.confidence_interval[1]:.2f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"üõ°Ô∏è AIPW (doubly robust): +{aipw_corp_effect.ate:.2f} pp [CI: {aipw_corp_effect.confidence_interval[0]:.2f}, {aipw_corp_effect.confidence_interval[1]:.2f}]\"\n",
    ")\n",
    "if has_forest:\n",
    "    print(\n",
    "        f\"üå≤ Causal Forest: +{forest_effect.ate:.2f} pp [CI: {forest_effect.confidence_interval[0]:.2f}, {forest_effect.confidence_interval[1]:.2f}]\"\n",
    "    )\n",
    "\n",
    "print(f\"üéØ True net effect: +{true_effects['net_effect']:.0f} percentage points\")\n",
    "\n",
    "# Analyze bias\n",
    "best_estimate = aipw_corp_effect.ate  # Use doubly robust estimate\n",
    "selection_bias = observed_difference - best_estimate\n",
    "bias_percentage = (selection_bias / observed_difference) * 100\n",
    "\n",
    "print(\"\\nüéØ BIAS DECOMPOSITION:\")\n",
    "print(\n",
    "    f\"üìä Selection bias: +{selection_bias:.2f} pp ({bias_percentage:.1f}% of observed effect)\"\n",
    ")\n",
    "print(f\"üîç True causal effect: +{best_estimate:.2f} pp\")\n",
    "\n",
    "if selection_bias > 1:\n",
    "    print(\"\\n‚ö†Ô∏è MAJOR FINDING: Most of the observed effect is due to selection bias!\")\n",
    "    print(\"üè¢ Successful companies are more likely to be diverse AND profitable\")\n",
    "    print(\"‚ú® But diversity still has a genuine causal effect!\")\n",
    "\n",
    "# Heterogeneous effects analysis\n",
    "print(\"\\nüé≠ HETEROGENEOUS EFFECTS:\")\n",
    "\n",
    "# Effect by innovation level\n",
    "innovative = corp_data[corp_data[\"innovative_industry\"] == 1]\n",
    "traditional = corp_data[corp_data[\"innovative_industry\"] == 0]\n",
    "\n",
    "innov_effect = (\n",
    "    innovative[innovative[\"high_diversity\"] == 1][\"profit_margin\"].mean()\n",
    "    - innovative[innovative[\"high_diversity\"] == 0][\"profit_margin\"].mean()\n",
    ")\n",
    "trad_effect = (\n",
    "    traditional[traditional[\"high_diversity\"] == 1][\"profit_margin\"].mean()\n",
    "    - traditional[traditional[\"high_diversity\"] == 0][\"profit_margin\"].mean()\n",
    ")\n",
    "\n",
    "print(f\"üí° Innovative industries: +{innov_effect:.2f} pp\")\n",
    "print(f\"üè≠ Traditional industries: +{trad_effect:.2f} pp\")\n",
    "\n",
    "if innov_effect > trad_effect + 1:\n",
    "    print(\"\\nüî• KEY INSIGHT: Diversity has stronger effects in innovative industries!\")\n",
    "    print(\"üéØ This matches our theoretical expectation!\")\n",
    "\n",
    "print(\"\\nüèÜ CAUSAL DETECTIVE CONCLUSIONS:\")\n",
    "print(\"1Ô∏è‚É£ Raw correlation overstates the diversity effect due to selection bias\")\n",
    "print(\"2Ô∏è‚É£ Successful companies are more likely to embrace diversity\")\n",
    "print(\"3Ô∏è‚É£ Diversity has genuine causal benefits, but they're smaller than they appear\")\n",
    "print(\"4Ô∏è‚É£ Effects are heterogeneous - stronger in innovative contexts\")\n",
    "print(\"5Ô∏è‚É£ Multiple confounders create complex causal patterns\")\n",
    "print(\"\\nüí° This is why 'seeing different' means controlling for confounders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Innovator's Diagnostic Toolkit\n",
    "\n",
    "### üîç The Causal Detective's Checklist\n",
    "\n",
    "When you encounter an apparent causal relationship, ask yourself:\n",
    "\n",
    "#### 1. Selection Bias Detection\n",
    "- **Who or what gets the treatment?**\n",
    "- **Is treatment assignment random or systematic?**\n",
    "- **What characteristics predict treatment assignment?**\n",
    "\n",
    "#### 2. Confounder Hunting\n",
    "- **What other variables affect both treatment and outcome?**\n",
    "- **Are there seasonal, geographic, or temporal patterns?**\n",
    "- **What institutional or structural factors create correlations?**\n",
    "\n",
    "#### 3. Mechanism Investigation\n",
    "- **How exactly would the treatment cause the outcome?**\n",
    "- **Are there intermediate steps in the causal chain?**\n",
    "- **Could the relationship work in reverse?**\n",
    "\n",
    "#### 4. Heterogeneity Analysis\n",
    "- **Does the effect vary across subgroups?**\n",
    "- **When does the treatment work vs. not work?**\n",
    "- **What moderates the relationship?**\n",
    "\n",
    "### üõ†Ô∏è Diagnostic Tools You've Mastered\n",
    "\n",
    "1. **Partial Correlation Analysis**: Remove the influence of confounders\n",
    "2. **Covariate Balance Checks**: Detect selection bias patterns\n",
    "3. **Multiple Estimation Methods**: Triangulate causal effects\n",
    "4. **Heterogeneous Effects**: Understand when effects vary\n",
    "5. **Sensitivity Analysis**: Test robustness to assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Innovator's Diagnostic Function\n",
    "\n",
    "\n",
    "def causal_diagnostic_suite(\n",
    "    treatment_var, outcome_var, data, potential_confounders=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive diagnostic suite for causal relationships\n",
    "    \"\"\"\n",
    "    print(\"üîç CAUSAL DIAGNOSTIC SUITE ACTIVATED\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1. Raw correlation\n",
    "    raw_corr = np.corrcoef(data[treatment_var], data[outcome_var])[0, 1]\n",
    "    results[\"raw_correlation\"] = raw_corr\n",
    "\n",
    "    print(\"\\n1Ô∏è‚É£ RAW RELATIONSHIP:\")\n",
    "    print(f\"üìä Correlation: r = {raw_corr:.4f}\")\n",
    "\n",
    "    if abs(raw_corr) > 0.5:\n",
    "        print(\"üö® Strong correlation detected - investigate further!\")\n",
    "    elif abs(raw_corr) > 0.3:\n",
    "        print(\"‚ö†Ô∏è Moderate correlation - could be genuine or spurious\")\n",
    "    else:\n",
    "        print(\"üìâ Weak correlation - may not be meaningful\")\n",
    "\n",
    "    # 2. Distribution analysis\n",
    "    print(\"\\n2Ô∏è‚É£ DISTRIBUTION ANALYSIS:\")\n",
    "    if data[treatment_var].nunique() == 2:  # Binary treatment\n",
    "        treated = data[data[treatment_var] == 1][outcome_var]\n",
    "        control = data[data[treatment_var] == 0][outcome_var]\n",
    "        diff = treated.mean() - control.mean()\n",
    "        print(f\"üìà Treatment group mean: {treated.mean():.2f}\")\n",
    "        print(f\"üìâ Control group mean: {control.mean():.2f}\")\n",
    "        print(f\"‚ö° Raw difference: {diff:.2f}\")\n",
    "        results[\"raw_difference\"] = diff\n",
    "\n",
    "    # 3. Confounder analysis\n",
    "    if potential_confounders:\n",
    "        print(\"\\n3Ô∏è‚É£ CONFOUNDER ANALYSIS:\")\n",
    "        confounding_strength = []\n",
    "\n",
    "        for confounder in potential_confounders:\n",
    "            if confounder in data.columns:\n",
    "                # Correlation with treatment\n",
    "                conf_treat_corr = np.corrcoef(data[confounder], data[treatment_var])[\n",
    "                    0, 1\n",
    "                ]\n",
    "                # Correlation with outcome\n",
    "                conf_outcome_corr = np.corrcoef(data[confounder], data[outcome_var])[\n",
    "                    0, 1\n",
    "                ]\n",
    "                # Confounding strength = product of correlations\n",
    "                conf_strength = abs(conf_treat_corr * conf_outcome_corr)\n",
    "                confounding_strength.append(\n",
    "                    (confounder, conf_strength, conf_treat_corr, conf_outcome_corr)\n",
    "                )\n",
    "\n",
    "        # Sort by confounding strength\n",
    "        confounding_strength.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(\"üéØ Potential confounders (ranked by strength):\")\n",
    "        for conf, strength, treat_corr, out_corr in confounding_strength[:5]:\n",
    "            print(\n",
    "                f\"‚Ä¢ {conf}: Strength = {strength:.3f} (r_treat={treat_corr:.3f}, r_outcome={out_corr:.3f})\"\n",
    "            )\n",
    "\n",
    "        results[\"top_confounders\"] = confounding_strength[:3]\n",
    "\n",
    "    # 4. Partial correlation (if confounders provided)\n",
    "    if potential_confounders and len(potential_confounders) > 0:\n",
    "        print(\"\\n4Ô∏è‚É£ PARTIAL CORRELATION ANALYSIS:\")\n",
    "        try:\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "\n",
    "            # Select available confounders\n",
    "            available_confounders = [\n",
    "                c for c in potential_confounders if c in data.columns\n",
    "            ]\n",
    "            if available_confounders:\n",
    "                X_conf = data[available_confounders].fillna(\n",
    "                    data[available_confounders].mean()\n",
    "                )\n",
    "\n",
    "                # Regress treatment on confounders\n",
    "                reg_treat = LinearRegression().fit(X_conf, data[treatment_var])\n",
    "                treat_residuals = data[treatment_var] - reg_treat.predict(X_conf)\n",
    "\n",
    "                # Regress outcome on confounders\n",
    "                reg_outcome = LinearRegression().fit(X_conf, data[outcome_var])\n",
    "                outcome_residuals = data[outcome_var] - reg_outcome.predict(X_conf)\n",
    "\n",
    "                # Partial correlation\n",
    "                partial_corr = np.corrcoef(treat_residuals, outcome_residuals)[0, 1]\n",
    "                results[\"partial_correlation\"] = partial_corr\n",
    "\n",
    "                print(f\"üìä Raw correlation: {raw_corr:.4f}\")\n",
    "                print(f\"üéØ Partial correlation: {partial_corr:.4f}\")\n",
    "\n",
    "                reduction = abs(raw_corr) - abs(partial_corr)\n",
    "                pct_reduction = (\n",
    "                    (reduction / abs(raw_corr)) * 100 if raw_corr != 0 else 0\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"‚¨áÔ∏è Confounding reduction: {reduction:.4f} ({pct_reduction:.1f}%)\"\n",
    "                )\n",
    "\n",
    "                if pct_reduction > 50:\n",
    "                    print(\n",
    "                        \"üö® MAJOR CONFOUNDING DETECTED! Over 50% of correlation explained by confounders\"\n",
    "                    )\n",
    "                elif pct_reduction > 25:\n",
    "                    print(\"‚ö†Ô∏è Moderate confounding detected\")\n",
    "                else:\n",
    "                    print(\"‚úÖ Relationship appears robust to these confounders\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Partial correlation analysis failed: {e}\")\n",
    "\n",
    "    # 5. Diagnostic conclusions\n",
    "    print(\"\\n5Ô∏è‚É£ DIAGNOSTIC CONCLUSIONS:\")\n",
    "\n",
    "    if \"partial_correlation\" in results:\n",
    "        if abs(results[\"partial_correlation\"]) < abs(results[\"raw_correlation\"]) * 0.5:\n",
    "            print(\n",
    "                \"üîç LIKELY SPURIOUS: Relationship weakens substantially when controlling for confounders\"\n",
    "            )\n",
    "        elif (\n",
    "            abs(results[\"partial_correlation\"]) > abs(results[\"raw_correlation\"]) * 0.8\n",
    "        ):\n",
    "            print(\"‚ú® LIKELY GENUINE: Relationship robust to confounders\")\n",
    "        else:\n",
    "            print(\n",
    "                \"ü§î MIXED EVIDENCE: Some confounding present but relationship persists\"\n",
    "            )\n",
    "\n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if abs(raw_corr) > 0.3:\n",
    "        print(\"‚Ä¢ Collect more potential confounders\")\n",
    "        print(\"‚Ä¢ Use causal inference methods (IV, RDD, DiD)\")\n",
    "        print(\"‚Ä¢ Look for natural experiments\")\n",
    "        print(\"‚Ä¢ Conduct robustness checks\")\n",
    "    else:\n",
    "        print(\"‚Ä¢ Consider if relationship is meaningful\")\n",
    "        print(\"‚Ä¢ Look for non-linear effects\")\n",
    "        print(\"‚Ä¢ Check for interaction effects\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage with our ice cream data\n",
    "print(\"üç¶ EXAMPLE: Diagnosing Ice Cream and Drowning Relationship\")\n",
    "ice_cream_results = causal_diagnostic_suite(\n",
    "    treatment_var=\"ice_cream_sales\",\n",
    "    outcome_var=\"drowning_deaths\",\n",
    "    data=ice_cream_data,\n",
    "    potential_confounders=[\"temperature\", \"marketing_spend\", \"swimming_activity\"],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéØ This diagnostic suite helps you 'see different' by:\")\n",
    "print(\"‚Ä¢ Systematically checking for confounders\")\n",
    "print(\"‚Ä¢ Quantifying spurious correlation\")\n",
    "print(\"‚Ä¢ Providing actionable next steps\")\n",
    "print(\"‚Ä¢ Building your causal intuition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise: Become a Causal Detective\n",
    "\n",
    "Now it's your turn to practice \"seeing different\". Choose one of these scenarios and apply the diagnostic toolkit:\n",
    "\n",
    "### Scenario Options\n",
    "\n",
    "1. **The Coffee Productivity Paradox**\n",
    "   - *Observed*: Employees who drink more coffee are more productive\n",
    "   - *Question*: Does coffee cause productivity or vice versa?\n",
    "\n",
    "2. **The Social Media Depression Link**\n",
    "   - *Observed*: Heavy social media users report higher depression rates\n",
    "   - *Question*: Does social media cause depression?\n",
    "\n",
    "3. **The Expensive Wine Quality Correlation**\n",
    "   - *Observed*: More expensive wines get higher ratings\n",
    "   - *Question*: Does price influence perception of quality?\n",
    "\n",
    "4. **The Exercise Happiness Connection**\n",
    "   - *Observed*: People who exercise regularly are happier\n",
    "   - *Question*: Does exercise cause happiness?\n",
    "\n",
    "### Your Detective Mission\n",
    "\n",
    "For your chosen scenario:\n",
    "1. **List potential confounders**\n",
    "2. **Identify selection mechanisms**\n",
    "3. **Propose diagnostic tests**\n",
    "4. **Design an ideal study**\n",
    "5. **Predict what you'd find**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Detective Challenge Template\n",
    "\n",
    "detective_template = \"\"\"\n",
    "üïµÔ∏è CAUSAL DETECTIVE CHALLENGE\n",
    "================================\n",
    "\n",
    "üìã CHOSEN SCENARIO: [Fill in your choice]\n",
    "\n",
    "üéØ RESEARCH QUESTION:\n",
    "[What is the causal question you're investigating?]\n",
    "\n",
    "üîç POTENTIAL CONFOUNDERS:\n",
    "1. [List variables that might affect both treatment and outcome]\n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "\n",
    "‚öñÔ∏è SELECTION MECHANISMS:\n",
    "[How do people/units get assigned to treatment? Is it random or systematic?]\n",
    "\n",
    "üß™ DIAGNOSTIC TESTS I WOULD RUN:\n",
    "1. [What analyses would help you detect confounding?]\n",
    "2. \n",
    "3. \n",
    "\n",
    "üéõÔ∏è IDEAL STUDY DESIGN:\n",
    "[If you could design the perfect study, what would it look like?]\n",
    "\n",
    "üîÆ PREDICTIONS:\n",
    "‚Ä¢ Naive estimate: [What would raw correlation show?]\n",
    "‚Ä¢ After controlling for confounders: [What would causal effect be?]\n",
    "‚Ä¢ Heterogeneous effects: [Would effects vary across groups?]\n",
    "\n",
    "üí° KEY INSIGHTS:\n",
    "[What would this teach us about 'seeing different'?]\n",
    "\"\"\"\n",
    "\n",
    "print(detective_template)\n",
    "\n",
    "# Example: Coffee Productivity Analysis\n",
    "print(\"\\nüìù EXAMPLE ANALYSIS: Coffee Productivity Paradox\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "coffee_analysis = \"\"\"\n",
    "üéØ RESEARCH QUESTION:\n",
    "Does coffee consumption causally increase workplace productivity?\n",
    "\n",
    "üîç POTENTIAL CONFOUNDERS:\n",
    "1. Work schedule (early birds drink more coffee AND are more productive)\n",
    "2. Job type (demanding jobs ‚Üí more coffee AND higher productivity pressure)\n",
    "3. Personality (ambitious people drink more coffee AND work harder)\n",
    "4. Sleep quality (poor sleep ‚Üí more coffee, but worse productivity)\n",
    "5. Workplace culture (competitive environments encourage both)\n",
    "\n",
    "‚öñÔ∏è SELECTION MECHANISMS:\n",
    "Coffee consumption isn't random! It's driven by:\n",
    "‚Ä¢ Personal preferences and genetics\n",
    "‚Ä¢ Work demands and stress levels  \n",
    "‚Ä¢ Social norms and peer influence\n",
    "‚Ä¢ Access and cost considerations\n",
    "\n",
    "üß™ DIAGNOSTIC TESTS:\n",
    "1. Compare coffee drinkers vs non-drinkers on work schedules\n",
    "2. Look at productivity during coffee shortages/strikes\n",
    "3. Analyze within-person variation (same person, different coffee days)\n",
    "4. Check if relationship holds across different job types\n",
    "\n",
    "üéõÔ∏è IDEAL STUDY DESIGN:\n",
    "Randomized trial: Randomly assign workers to receive free coffee vs decaf\n",
    "for 3 months, measure productivity blind to treatment assignment.\n",
    "BUT: Ethical issues with caffeine withdrawal!\n",
    "\n",
    "ALTERNATIVE: Instrumental variable - use coffee shop closures/openings\n",
    "near workplaces as random variation in coffee access.\n",
    "\n",
    "üîÆ PREDICTIONS:\n",
    "‚Ä¢ Naive estimate: +15% productivity for coffee drinkers\n",
    "‚Ä¢ After controlling for confounders: +5% productivity\n",
    "‚Ä¢ Heterogeneous effects: Stronger for morning workers, weaker for anxious people\n",
    "\n",
    "üí° KEY INSIGHTS:\n",
    "‚Ä¢ Most \"productivity\" correlation is selection bias\n",
    "‚Ä¢ True causal effect exists but is smaller\n",
    "‚Ä¢ Context matters - coffee helps some people more than others\n",
    "‚Ä¢ Always ask \"Who chooses the treatment and why?\"\n",
    "\"\"\"\n",
    "\n",
    "print(coffee_analysis)\n",
    "\n",
    "print(\"\\nüèÜ YOUR TURN!\")\n",
    "print(\"Choose a scenario and fill out the template above.\")\n",
    "print(\"Practice 'seeing different' by questioning apparent relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Art of Seeing Different\n",
    "\n",
    "### What You've Mastered\n",
    "\n",
    "1. **Spurious Correlation Detection**\n",
    "   - Identified hidden confounders (temperature in ice cream example)\n",
    "   - Used partial correlation to unmask true relationships\n",
    "   - Learned to question \"obvious\" associations\n",
    "\n",
    "2. **Selection Bias Recognition**\n",
    "   - Discovered how non-random treatment assignment creates bias\n",
    "   - Analyzed complex selection mechanisms in corporate diversity\n",
    "   - Controlled for confounders using advanced methods\n",
    "\n",
    "3. **Mechanism Thinking**\n",
    "   - Explored heterogeneous effects across contexts\n",
    "   - Identified when treatments work vs. don't work\n",
    "   - Understood complex causal pathways\n",
    "\n",
    "4. **Diagnostic Skills**\n",
    "   - Built a comprehensive diagnostic toolkit\n",
    "   - Learned to systematically test causal claims\n",
    "   - Developed intuition for confounding patterns\n",
    "\n",
    "### The Innovator's Mindset\n",
    "\n",
    "**Traditional thinking**: \"These variables are correlated, so one causes the other.\"\n",
    "\n",
    "**Revolutionary thinking**: \"What hidden factors create this correlation? How can I isolate the true causal relationship?\"\n",
    "\n",
    "### Key Principles for Seeing Different\n",
    "\n",
    "1. **Question Everything**: Every correlation has a story behind it\n",
    "2. **Hunt for Confounders**: Look for variables that predict both treatment and outcome\n",
    "3. **Think About Selection**: Ask who gets treated and why\n",
    "4. **Test Robustness**: Use multiple methods to verify findings\n",
    "5. **Expect Heterogeneity**: Effects vary across contexts and populations\n",
    "\n",
    "### Next Steps in Your Innovation Journey\n",
    "\n",
    "Continue developing your causal vision with:\n",
    "- **Tutorial 3**: The Crazy Idea - Using ML for causal inference\n",
    "- **Tutorial 4**: Change Things - From analysis to intervention\n",
    "- **Tutorial 5**: Push Forward - Advanced causal methods\n",
    "\n",
    "---\n",
    "\n",
    "*\"While others see complexity, innovators see clarity.\" You now have the vision to see what others miss. Keep questioning, keep investigating, and keep seeing different!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
