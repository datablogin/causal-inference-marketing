{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Change Things: From Analysis to Intervention\n",
    "\n",
    "> *\"The people who think they can change the world are the ones who do\"* - This tutorial transforms causal insights into real-world interventions that create impact.\n",
    "\n",
    "## The Innovation Imperative\n",
    "\n",
    "Most data science stops at analysis. You discover relationships, build models, write reports. **Innovators go further.** They use causal insights to design interventions that actually change outcomes in the real world.\n",
    "\n",
    "### What You'll Master\n",
    "\n",
    "- **Intervention design** based on causal mechanisms\n",
    "- **Policy optimization** using causal insights\n",
    "- **A/B testing** with causal principles\n",
    "- **Adaptive interventions** that learn and improve\n",
    "- **Impact measurement** for real-world changes\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Moving from \"What causes what?\" to \"How do we change what?\"\n",
    "- Design interventions that actually work\n",
    "- Navigate complex systems and unintended consequences\n",
    "- Measure impact in messy real-world settings\n",
    "- Scale successful interventions\n",
    "\n",
    "**It's time to change things, not just analyze them!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: The Change-Maker's Arsenal\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, os.path.join(project_root, \"libs\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Our causal intervention toolkit\n",
    "from causal_inference.core.base import CovariateData, OutcomeData, TreatmentData\n",
    "from causal_inference.estimators.causal_forest import CausalForest\n",
    "\n",
    "# Set up the intervention laboratory\n",
    "np.random.seed(42)\n",
    "plt.style.use(\"default\")  # Use default style instead of deprecated seaborn style\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "print(\"üîß INTERVENTION LABORATORY: Activated\")\n",
    "print(\"üí° From Insights to Action: Ready\")\n",
    "print(\"üöÄ Time to change the world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Case Study 1: Designing Educational Interventions\n",
    "\n",
    "**The Setting**: You've discovered that personalized tutoring has heterogeneous effects - it helps struggling students but may actually hurt high-performers who become over-dependent.\n",
    "\n",
    "**The Challenge**: Design an intervention policy that maximizes overall learning outcomes.\n",
    "\n",
    "**The Innovation**: Use causal insights to create smart, targeted interventions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 1: Educational Intervention Design\n",
    "\n",
    "\n",
    "def generate_education_intervention_data(n_students=2000):\n",
    "    \"\"\"\n",
    "    Generate student data with heterogeneous tutoring effects\n",
    "    \"\"\"\n",
    "    # Student characteristics\n",
    "    baseline_ability = np.random.normal(75, 15, n_students)  # Baseline test scores\n",
    "    motivation = np.random.beta(2, 2, n_students)  # 0-1 scale\n",
    "    ses_background = np.random.gamma(2, 3, n_students)  # Socioeconomic status\n",
    "    prior_support = np.random.binomial(1, 0.4, n_students)  # Has support at home\n",
    "    learning_style = np.random.choice(\n",
    "        [\"visual\", \"auditory\", \"kinesthetic\"], n_students, p=[0.4, 0.35, 0.25]\n",
    "    )\n",
    "\n",
    "    # Treatment assignment (currently random, but we'll optimize this!)\n",
    "    receives_tutoring = np.random.binomial(1, 0.3, n_students)  # 30% get tutoring\n",
    "\n",
    "    # HETEROGENEOUS TREATMENT EFFECTS - The key insight!\n",
    "    # Tutoring helps struggling students but may hurt high performers\n",
    "\n",
    "    # Base effect depends on baseline ability (struggling students benefit more)\n",
    "    ability_effect = 15 * (baseline_ability < 70) + 8 * (baseline_ability < 80) * (\n",
    "        baseline_ability >= 70\n",
    "    )\n",
    "\n",
    "    # Motivation interaction (low motivation students benefit more from structure)\n",
    "    motivation_effect = 10 * (motivation < 0.4) + 5 * (motivation < 0.7) * (\n",
    "        motivation >= 0.4\n",
    "    )\n",
    "\n",
    "    # SES interaction (tutoring compensates for lack of home support)\n",
    "    ses_effect = 8 * (ses_background < 3) + 4 * (ses_background < 6) * (\n",
    "        ses_background >= 3\n",
    "    )\n",
    "\n",
    "    # NEGATIVE effects for high performers (over-dependence)\n",
    "    overhelp_penalty = -5 * (baseline_ability > 85) * (motivation > 0.7)\n",
    "\n",
    "    # Learning style match bonus\n",
    "    style_bonus = 3 * (\n",
    "        learning_style == \"visual\"\n",
    "    )  # Visual learners benefit most from tutoring\n",
    "\n",
    "    # Individual treatment effects\n",
    "    individual_effects = (\n",
    "        ability_effect + motivation_effect + ses_effect + overhelp_penalty + style_bonus\n",
    "    )\n",
    "\n",
    "    # Final test scores\n",
    "    noise = np.random.normal(0, 8, n_students)\n",
    "    test_score = (\n",
    "        baseline_ability\n",
    "        + individual_effects * receives_tutoring\n",
    "        + 5 * motivation  # Motivation always helps\n",
    "        + 2 * ses_background  # SES background effect\n",
    "        + 3 * prior_support  # Prior support effect\n",
    "        + noise\n",
    "    )\n",
    "\n",
    "    test_score = np.clip(test_score, 0, 100)  # Realistic test score range\n",
    "\n",
    "    # Create learning style dummies\n",
    "    style_dummies = pd.get_dummies(learning_style, prefix=\"style\")\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"student_id\": range(n_students),\n",
    "            \"baseline_ability\": baseline_ability,\n",
    "            \"motivation\": motivation,\n",
    "            \"ses_background\": ses_background,\n",
    "            \"prior_support\": prior_support,\n",
    "            \"receives_tutoring\": receives_tutoring,\n",
    "            \"test_score\": test_score,\n",
    "            \"true_effect\": individual_effects,\n",
    "            \"learning_style\": learning_style,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add learning style dummies\n",
    "    data = pd.concat([data, style_dummies], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Generate the data\n",
    "edu_data = generate_education_intervention_data(2000)\n",
    "\n",
    "print(\"üéì Educational Intervention Dataset Generated\")\n",
    "print(f\"üë• Students: {len(edu_data):,}\")\n",
    "print(f\"üìä Current tutoring rate: {edu_data['receives_tutoring'].mean():.1%}\")\n",
    "print(\n",
    "    f\"üé≠ Treatment effects range: {edu_data['true_effect'].min():.1f} to {edu_data['true_effect'].max():.1f} points\"\n",
    ")\n",
    "print(\"\\nData preview:\")\n",
    "print(\n",
    "    edu_data[\n",
    "        [\n",
    "            \"baseline_ability\",\n",
    "            \"motivation\",\n",
    "            \"receives_tutoring\",\n",
    "            \"test_score\",\n",
    "            \"true_effect\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "# Analyze current random policy\n",
    "current_avg_score = edu_data[\"test_score\"].mean()\n",
    "tutored_avg = edu_data[edu_data[\"receives_tutoring\"] == 1][\"test_score\"].mean()\n",
    "not_tutored_avg = edu_data[edu_data[\"receives_tutoring\"] == 0][\"test_score\"].mean()\n",
    "current_effect = tutored_avg - not_tutored_avg\n",
    "\n",
    "print(\"\\nüìä CURRENT RANDOM POLICY RESULTS:\")\n",
    "print(f\"üìà Overall average score: {current_avg_score:.1f}\")\n",
    "print(f\"üéì Tutored students: {tutored_avg:.1f}\")\n",
    "print(f\"üìö Non-tutored students: {not_tutored_avg:.1f}\")\n",
    "print(f\"‚ö° Naive treatment effect: {current_effect:.1f} points\")\n",
    "\n",
    "# Visualize the heterogeneity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Treatment effects by baseline ability\n",
    "ability_bins = pd.cut(edu_data[\"baseline_ability\"], bins=10)\n",
    "ability_effects = edu_data.groupby(ability_bins)[\"true_effect\"].mean()\n",
    "axes[0, 0].plot(\n",
    "    range(len(ability_effects)), ability_effects.values, \"o-\", linewidth=2, markersize=8\n",
    ")\n",
    "axes[0, 0].axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[0, 0].set_title(\n",
    "    \"Treatment Effect by Baseline Ability\\n(Negative for High Performers!)\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Ability Decile\")\n",
    "axes[0, 0].set_ylabel(\"True Treatment Effect\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Treatment effects by motivation\n",
    "motivation_bins = pd.cut(\n",
    "    edu_data[\"motivation\"],\n",
    "    bins=5,\n",
    "    labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"],\n",
    ")\n",
    "motivation_effects = edu_data.groupby(motivation_bins)[\"true_effect\"].mean()\n",
    "axes[0, 1].bar(motivation_effects.index, motivation_effects.values, alpha=0.8)\n",
    "axes[0, 1].set_title(\"Treatment Effect by Motivation Level\", fontweight=\"bold\")\n",
    "axes[0, 1].set_ylabel(\"True Treatment Effect\")\n",
    "axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Distribution of treatment effects\n",
    "axes[1, 0].hist(edu_data[\"true_effect\"], bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "axes[1, 0].axvline(\n",
    "    edu_data[\"true_effect\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {edu_data['true_effect'].mean():.1f}\",\n",
    ")\n",
    "axes[1, 0].axvline(0, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"No Effect\")\n",
    "negative_effects = (edu_data[\"true_effect\"] < 0).mean()\n",
    "axes[1, 0].set_title(\n",
    "    f\"Distribution of Treatment Effects\\n({negative_effects:.1%} have negative effects!)\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1, 0].set_xlabel(\"True Treatment Effect\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Learning style effects\n",
    "style_effects = edu_data.groupby(\"learning_style\")[\"true_effect\"].mean()\n",
    "axes[1, 1].bar(\n",
    "    style_effects.index,\n",
    "    style_effects.values,\n",
    "    alpha=0.8,\n",
    "    color=[\"skyblue\", \"lightcoral\", \"lightgreen\"],\n",
    ")\n",
    "axes[1, 1].set_title(\"Treatment Effect by Learning Style\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"True Treatment Effect\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHTS FOR INTERVENTION DESIGN:\")\n",
    "print(\n",
    "    f\"‚úÖ Struggling students (ability < 70) benefit most: +{edu_data[edu_data['baseline_ability'] < 70]['true_effect'].mean():.1f} points\"\n",
    ")\n",
    "print(\n",
    "    f\"‚ö†Ô∏è High performers (ability > 85) may be hurt: {edu_data[edu_data['baseline_ability'] > 85]['true_effect'].mean():.1f} points\"\n",
    ")\n",
    "print(\n",
    "    f\"üìä {negative_effects:.1%} of students have negative treatment effects under current policy\"\n",
    ")\n",
    "print(\"üí° Optimal policy should target tutoring based on student characteristics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Learn Treatment Effects Using Causal ML\n",
    "\n",
    "print(\"ü§ñ STEP 1: Learning Individual Treatment Effects\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare data for causal learning\n",
    "feature_cols = [\n",
    "    \"baseline_ability\",\n",
    "    \"motivation\",\n",
    "    \"ses_background\",\n",
    "    \"prior_support\",\n",
    "    \"style_auditory\",\n",
    "    \"style_kinesthetic\",\n",
    "    \"style_visual\",\n",
    "]\n",
    "\n",
    "treatment = TreatmentData(\n",
    "    values=edu_data[\"receives_tutoring\"],\n",
    "    name=\"receives_tutoring\",\n",
    "    treatment_type=\"binary\",\n",
    ")\n",
    "\n",
    "outcome = OutcomeData(\n",
    "    values=edu_data[\"test_score\"], name=\"test_score\", outcome_type=\"continuous\"\n",
    ")\n",
    "\n",
    "covariates = CovariateData(values=edu_data[feature_cols], names=feature_cols)\n",
    "\n",
    "print(\"üéØ Training causal forest to predict individual treatment effects\")\n",
    "print(f\"üìä Features: {len(feature_cols)} student characteristics\")\n",
    "print(f\"üë• Sample: {len(edu_data)} students\")\n",
    "\n",
    "# Train causal forest\n",
    "causal_forest = CausalForest(\n",
    "    n_estimators=200, min_samples_leaf=20, bootstrap_samples=100\n",
    ")\n",
    "\n",
    "causal_forest.fit(treatment, outcome, covariates)\n",
    "\n",
    "# Predict individual treatment effects using correct API\n",
    "X_features = edu_data[feature_cols].values\n",
    "predicted_effects, prediction_std = causal_forest.predict_cate(X_features)\n",
    "edu_data[\"predicted_effect\"] = predicted_effects\n",
    "\n",
    "# Evaluate prediction quality\n",
    "true_effects = edu_data[\"true_effect\"].values\n",
    "prediction_corr = np.corrcoef(predicted_effects, true_effects)[0, 1]\n",
    "prediction_rmse = np.sqrt(np.mean((predicted_effects - true_effects) ** 2))\n",
    "\n",
    "print(\"\\nüìà TREATMENT EFFECT PREDICTION QUALITY:\")\n",
    "print(f\"üéØ Correlation with true effects: r = {prediction_corr:.3f}\")\n",
    "print(f\"üìä RMSE: {prediction_rmse:.2f} points\")\n",
    "print(\n",
    "    f\"üíØ R¬≤: {prediction_corr**2:.3f} (explains {prediction_corr**2 * 100:.1f}% of variation)\"\n",
    ")\n",
    "\n",
    "if prediction_corr > 0.6:\n",
    "    print(\"üèÜ EXCELLENT prediction quality - ready to design optimal policy!\")\n",
    "elif prediction_corr > 0.4:\n",
    "    print(\"‚úÖ GOOD prediction quality - proceeding with optimization!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Moderate prediction quality - results may be approximate\")\n",
    "\n",
    "# Visualize prediction quality\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Predicted vs true effects\n",
    "axes[0].scatter(true_effects, predicted_effects, alpha=0.6, s=30)\n",
    "axes[0].plot(\n",
    "    [true_effects.min(), true_effects.max()],\n",
    "    [true_effects.min(), true_effects.max()],\n",
    "    \"r--\",\n",
    "    alpha=0.8,\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[0].set_title(\n",
    "    f\"Predicted vs True Effects\\nr = {prediction_corr:.3f}\", fontweight=\"bold\"\n",
    ")\n",
    "axes[0].set_xlabel(\"True Treatment Effect\")\n",
    "axes[0].set_ylabel(\"Predicted Treatment Effect\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution comparison\n",
    "axes[1].hist(true_effects, bins=25, alpha=0.7, label=\"True Effects\", density=True)\n",
    "axes[1].hist(\n",
    "    predicted_effects, bins=25, alpha=0.7, label=\"Predicted Effects\", density=True\n",
    ")\n",
    "axes[1].axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"No Effect\")\n",
    "axes[1].set_title(\"Distribution Comparison\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Treatment Effect\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Now we can design optimal intervention policies using these predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Design Optimal Intervention Policy\n",
    "\n",
    "print(\"üéØ STEP 2: Designing Optimal Intervention Policy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "def evaluate_policy(data, treatment_threshold, budget_constraint=0.3):\n",
    "    \"\"\"\n",
    "    Evaluate an intervention policy that treats students with predicted effect > threshold\n",
    "    \"\"\"\n",
    "    # Apply policy: treat if predicted effect > threshold\n",
    "    should_treat = data[\"predicted_effect\"] > treatment_threshold\n",
    "\n",
    "    # Check budget constraint\n",
    "    treatment_rate = should_treat.mean()\n",
    "    if treatment_rate > budget_constraint:\n",
    "        # If over budget, treat top students by predicted effect\n",
    "        n_to_treat = int(budget_constraint * len(data))\n",
    "        top_students = data.nlargest(n_to_treat, \"predicted_effect\").index\n",
    "        should_treat = data.index.isin(top_students)\n",
    "        treatment_rate = budget_constraint\n",
    "\n",
    "    # Calculate expected outcomes under this policy\n",
    "    # For treated: baseline + treatment effect\n",
    "    # For untreated: just baseline\n",
    "    baseline_scores = (\n",
    "        data[\"test_score\"] - data[\"true_effect\"] * data[\"receives_tutoring\"]\n",
    "    )  # Remove current treatment effect\n",
    "\n",
    "    expected_scores = baseline_scores + data[\"true_effect\"] * should_treat\n",
    "\n",
    "    policy_results = {\n",
    "        \"treatment_rate\": treatment_rate,\n",
    "        \"avg_score\": expected_scores.mean(),\n",
    "        \"treated_avg\": expected_scores[should_treat].mean()\n",
    "        if should_treat.sum() > 0\n",
    "        else 0,\n",
    "        \"untreated_avg\": expected_scores[~should_treat].mean()\n",
    "        if (~should_treat).sum() > 0\n",
    "        else 0,\n",
    "        \"total_benefit\": (data[\"true_effect\"] * should_treat).sum(),\n",
    "        \"students_helped\": (should_treat & (data[\"true_effect\"] > 0)).sum(),\n",
    "        \"students_harmed\": (should_treat & (data[\"true_effect\"] < 0)).sum(),\n",
    "    }\n",
    "\n",
    "    return policy_results, should_treat\n",
    "\n",
    "\n",
    "# Test different policy thresholds\n",
    "print(\"üîç Testing different intervention thresholds...\")\n",
    "\n",
    "thresholds = np.arange(-5, 15, 1)  # Range of effect thresholds\n",
    "policy_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    results, _ = evaluate_policy(edu_data, threshold, budget_constraint=0.3)\n",
    "    results[\"threshold\"] = threshold\n",
    "    policy_results.append(results)\n",
    "\n",
    "policy_df = pd.DataFrame(policy_results)\n",
    "\n",
    "# Find optimal policy\n",
    "optimal_idx = policy_df[\"avg_score\"].idxmax()\n",
    "optimal_threshold = policy_df.loc[optimal_idx, \"threshold\"]\n",
    "optimal_results, optimal_treatment = evaluate_policy(edu_data, optimal_threshold, 0.3)\n",
    "\n",
    "print(\"\\nüèÜ OPTIMAL POLICY DISCOVERED:\")\n",
    "print(f\"üéØ Threshold: Treat students with predicted effect > {optimal_threshold}\")\n",
    "print(f\"üë• Treatment rate: {optimal_results['treatment_rate']:.1%}\")\n",
    "print(f\"üìà Expected average score: {optimal_results['avg_score']:.1f}\")\n",
    "print(f\"‚úÖ Students helped: {optimal_results['students_helped']}\")\n",
    "print(f\"‚ùå Students harmed: {optimal_results['students_harmed']}\")\n",
    "print(f\"‚ö° Total benefit: {optimal_results['total_benefit']:.1f} points\")\n",
    "\n",
    "# Compare with current random policy\n",
    "current_total_benefit = (edu_data[\"true_effect\"] * edu_data[\"receives_tutoring\"]).sum()\n",
    "improvement = optimal_results[\"total_benefit\"] - current_total_benefit\n",
    "avg_improvement = optimal_results[\"avg_score\"] - current_avg_score\n",
    "\n",
    "print(\"\\nüìä IMPROVEMENT OVER RANDOM POLICY:\")\n",
    "print(f\"‚¨ÜÔ∏è Total benefit increase: {improvement:.1f} points\")\n",
    "print(f\"üìà Average score increase: {avg_improvement:.1f} points\")\n",
    "print(f\"üéØ Relative improvement: {improvement / abs(current_total_benefit) * 100:.1f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\n",
    "        \"üéâ BREAKTHROUGH: Optimal policy significantly outperforms random assignment!\"\n",
    "    )\n",
    "\n",
    "# Visualize policy comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Policy performance curve\n",
    "axes[0, 0].plot(\n",
    "    policy_df[\"threshold\"], policy_df[\"avg_score\"], \"o-\", linewidth=2, markersize=6\n",
    ")\n",
    "axes[0, 0].axvline(\n",
    "    optimal_threshold,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Optimal: {optimal_threshold}\",\n",
    ")\n",
    "axes[0, 0].axhline(\n",
    "    current_avg_score,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Current: {current_avg_score:.1f}\",\n",
    ")\n",
    "axes[0, 0].set_title(\"Policy Performance vs Threshold\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"Treatment Threshold\")\n",
    "axes[0, 0].set_ylabel(\"Expected Average Score\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Treatment rates\n",
    "axes[0, 1].plot(\n",
    "    policy_df[\"threshold\"],\n",
    "    policy_df[\"treatment_rate\"],\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    color=\"green\",\n",
    ")\n",
    "axes[0, 1].axhline(\n",
    "    0.3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Budget Constraint\"\n",
    ")\n",
    "axes[0, 1].set_title(\"Treatment Rate vs Threshold\", fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Treatment Threshold\")\n",
    "axes[0, 1].set_ylabel(\"Treatment Rate\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Who gets treated under optimal policy\n",
    "edu_data[\"optimal_treatment\"] = optimal_treatment\n",
    "treated_ability = edu_data[edu_data[\"optimal_treatment\"]][\"baseline_ability\"]\n",
    "not_treated_ability = edu_data[~edu_data[\"optimal_treatment\"]][\"baseline_ability\"]\n",
    "\n",
    "axes[1, 0].hist(\n",
    "    not_treated_ability, bins=20, alpha=0.7, label=\"Not Treated\", density=True\n",
    ")\n",
    "axes[1, 0].hist(treated_ability, bins=20, alpha=0.7, label=\"Treated\", density=True)\n",
    "axes[1, 0].set_title(\"Optimal Policy: Who Gets Treated?\", fontweight=\"bold\")\n",
    "axes[1, 0].set_xlabel(\"Baseline Ability\")\n",
    "axes[1, 0].set_ylabel(\"Density\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Treatment effects for treated vs not treated\n",
    "treated_effects = edu_data[edu_data[\"optimal_treatment\"]][\"true_effect\"]\n",
    "not_treated_effects = edu_data[~edu_data[\"optimal_treatment\"]][\"true_effect\"]\n",
    "\n",
    "axes[1, 1].boxplot(\n",
    "    [treated_effects, not_treated_effects], labels=[\"Treated\", \"Not Treated\"]\n",
    ")\n",
    "axes[1, 1].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[1, 1].set_title(\"Treatment Effects by Policy Assignment\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"True Treatment Effect\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° POLICY INSIGHTS:\")\n",
    "print(\"üéØ Optimal policy focuses on students with baseline ability 60-80\")\n",
    "print(\"‚ö†Ô∏è High performers (>85) correctly excluded to avoid harm\")\n",
    "print(\"‚úÖ Low performers get priority - equity AND efficiency!\")\n",
    "print(\"üöÄ Smart targeting beats random assignment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Case Study 2: Dynamic Pricing Optimization\n",
    "\n",
    "**The Setting**: An e-commerce platform wants to optimize pricing to maximize both revenue and customer satisfaction.\n",
    "\n",
    "**The Challenge**: Price sensitivity varies by customer, product, and context. Static pricing leaves money on the table and may alienate price-sensitive customers.\n",
    "\n",
    "**The Innovation**: Use causal ML to design personalized, dynamic pricing that adapts in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 2: Dynamic Pricing Optimization\n",
    "\n",
    "\n",
    "def generate_pricing_data(n_customers=3000, n_products=50):\n",
    "    \"\"\"\n",
    "    Generate customer purchase data with heterogeneous price sensitivity\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Customer characteristics\n",
    "    customer_income = np.random.lognormal(10, 0.8, n_customers)\n",
    "    customer_age = np.random.gamma(2, 20, n_customers)\n",
    "    customer_loyalty = np.random.beta(2, 3, n_customers)  # 0-1 scale\n",
    "    price_sensitivity = np.random.gamma(2, 0.5, n_customers)  # Higher = more sensitive\n",
    "\n",
    "    # Generate random customer-product interactions\n",
    "    n_interactions = 8000\n",
    "    customer_ids = np.random.choice(n_customers, n_interactions)\n",
    "    product_ids = np.random.choice(n_products, n_interactions)\n",
    "\n",
    "    # Product characteristics\n",
    "    base_prices = np.random.gamma(3, 20, n_products)  # Base product prices\n",
    "    product_quality = np.random.beta(3, 2, n_products)  # 0-1 quality scale\n",
    "\n",
    "    # Current pricing strategy (random discounts)\n",
    "    discount_rates = np.random.uniform(0, 0.3, n_interactions)  # 0-30% discounts\n",
    "    final_prices = base_prices[product_ids] * (1 - discount_rates)\n",
    "\n",
    "    # HETEROGENEOUS PRICE EFFECTS\n",
    "    # Base purchase probability\n",
    "    base_prob = (\n",
    "        0.1\n",
    "        + 0.3 * product_quality[product_ids]  # Quality increases probability\n",
    "        + 0.2 * customer_loyalty[customer_ids]  # Loyalty increases probability\n",
    "        + 0.1 * (customer_age[customer_ids] > 50)\n",
    "    )  # Older customers buy more\n",
    "\n",
    "    # Price effect varies by customer characteristics\n",
    "    # Income effect: Rich customers less price sensitive\n",
    "    income_effect = (\n",
    "        -0.5\n",
    "        * price_sensitivity[customer_ids]\n",
    "        * (1 / (1 + customer_income[customer_ids] / 50000))\n",
    "    )\n",
    "\n",
    "    # Loyalty effect: Loyal customers less price sensitive\n",
    "    loyalty_effect = (\n",
    "        -0.3 * price_sensitivity[customer_ids] * (1 - customer_loyalty[customer_ids])\n",
    "    )\n",
    "\n",
    "    # Age effect: Older customers less price sensitive\n",
    "    age_effect = (\n",
    "        -0.2 * price_sensitivity[customer_ids] * (customer_age[customer_ids] < 30) / 30\n",
    "    )\n",
    "\n",
    "    # Total price effect (discount increases purchase probability)\n",
    "    total_price_effect = (income_effect + loyalty_effect + age_effect) * discount_rates\n",
    "\n",
    "    # Final purchase probability\n",
    "    purchase_prob = np.clip(base_prob + total_price_effect, 0.01, 0.95)\n",
    "    purchased = np.random.binomial(1, purchase_prob, n_interactions)\n",
    "\n",
    "    # Revenue calculation\n",
    "    revenue_per_interaction = final_prices * purchased\n",
    "\n",
    "    # Customer satisfaction (decreases with high prices, increases with discounts)\n",
    "    satisfaction = (\n",
    "        5  # Base satisfaction\n",
    "        + 2 * discount_rates  # Discounts increase satisfaction\n",
    "        + 3 * product_quality[product_ids] * purchased  # Quality matters when purchased\n",
    "        + -1\n",
    "        * (\n",
    "            final_prices > base_prices[product_ids] * 0.8\n",
    "        )  # High prices decrease satisfaction\n",
    "        + np.random.normal(0, 0.5, n_interactions)\n",
    "    )  # Noise\n",
    "\n",
    "    satisfaction = np.clip(satisfaction, 1, 10)  # 1-10 scale\n",
    "\n",
    "    # Create dataset\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"customer_id\": customer_ids,\n",
    "            \"product_id\": product_ids,\n",
    "            \"customer_income\": customer_income[customer_ids],\n",
    "            \"customer_age\": customer_age[customer_ids],\n",
    "            \"customer_loyalty\": customer_loyalty[customer_ids],\n",
    "            \"price_sensitivity\": price_sensitivity[customer_ids],\n",
    "            \"base_price\": base_prices[product_ids],\n",
    "            \"product_quality\": product_quality[product_ids],\n",
    "            \"discount_rate\": discount_rates,\n",
    "            \"final_price\": final_prices,\n",
    "            \"purchased\": purchased,\n",
    "            \"revenue\": revenue_per_interaction,\n",
    "            \"satisfaction\": satisfaction,\n",
    "            \"true_price_effect\": total_price_effect\n",
    "            / discount_rates,  # Effect per unit discount\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Generate pricing data\n",
    "pricing_data = generate_pricing_data(3000, 50)\n",
    "\n",
    "print(\"üí∞ Dynamic Pricing Dataset Generated\")\n",
    "print(f\"üë• Customers: {pricing_data['customer_id'].nunique():,}\")\n",
    "print(f\"üõçÔ∏è Products: {pricing_data['product_id'].nunique()}\")\n",
    "print(f\"üìä Interactions: {len(pricing_data):,}\")\n",
    "print(f\"üíµ Current purchase rate: {pricing_data['purchased'].mean():.1%}\")\n",
    "print(f\"üí∞ Current revenue per interaction: ${pricing_data['revenue'].mean():.2f}\")\n",
    "print(f\"üòä Current satisfaction: {pricing_data['satisfaction'].mean():.2f}/10\")\n",
    "\n",
    "print(\"\\nData preview:\")\n",
    "print(\n",
    "    pricing_data[\n",
    "        [\n",
    "            \"customer_income\",\n",
    "            \"customer_age\",\n",
    "            \"price_sensitivity\",\n",
    "            \"discount_rate\",\n",
    "            \"purchased\",\n",
    "            \"revenue\",\n",
    "            \"satisfaction\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "# Analyze current pricing strategy\n",
    "print(\"\\nüìä CURRENT RANDOM PRICING ANALYSIS:\")\n",
    "\n",
    "# Revenue by discount level\n",
    "discount_bins = pd.cut(\n",
    "    pricing_data[\"discount_rate\"],\n",
    "    bins=5,\n",
    "    labels=[\"0-6%\", \"6-12%\", \"12-18%\", \"18-24%\", \"24-30%\"],\n",
    ")\n",
    "revenue_by_discount = pricing_data.groupby(discount_bins).agg(\n",
    "    {\"revenue\": \"mean\", \"purchased\": \"mean\", \"satisfaction\": \"mean\"}\n",
    ")\n",
    "\n",
    "print(\"Revenue by discount level:\")\n",
    "print(revenue_by_discount)\n",
    "\n",
    "# Visualize the pricing landscape\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Purchase rate by discount and customer income\n",
    "income_quintiles = pd.qcut(\n",
    "    pricing_data[\"customer_income\"], 5, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\"]\n",
    ")\n",
    "purchase_by_income_discount = (\n",
    "    pricing_data.groupby([income_quintiles, discount_bins])[\"purchased\"]\n",
    "    .mean()\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "sns.heatmap(purchase_by_income_discount, annot=True, cmap=\"Blues\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Purchase Rate by Income & Discount\", fontweight=\"bold\")\n",
    "axes[0, 0].set_ylabel(\"Income Quintile\")\n",
    "axes[0, 0].set_xlabel(\"Discount Rate\")\n",
    "\n",
    "# Price sensitivity distribution\n",
    "axes[0, 1].hist(\n",
    "    pricing_data[\"price_sensitivity\"], bins=30, alpha=0.7, edgecolor=\"black\"\n",
    ")\n",
    "axes[0, 1].set_title(\"Distribution of Price Sensitivity\", fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Price Sensitivity\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Revenue vs satisfaction tradeoff\n",
    "axes[1, 0].scatter(\n",
    "    pricing_data[\"satisfaction\"], pricing_data[\"revenue\"], alpha=0.5, s=20\n",
    ")\n",
    "axes[1, 0].set_title(\"Revenue vs Satisfaction Tradeoff\", fontweight=\"bold\")\n",
    "axes[1, 0].set_xlabel(\"Customer Satisfaction\")\n",
    "axes[1, 0].set_ylabel(\"Revenue per Interaction\")\n",
    "\n",
    "# Discount effect by customer loyalty\n",
    "loyalty_bins = pd.cut(\n",
    "    pricing_data[\"customer_loyalty\"], bins=3, labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "discount_effect_by_loyalty = (\n",
    "    pricing_data.groupby([loyalty_bins, discount_bins])[\"purchased\"].mean().unstack()\n",
    ")\n",
    "\n",
    "discount_effect_by_loyalty.plot(kind=\"bar\", ax=axes[1, 1], alpha=0.8)\n",
    "axes[1, 1].set_title(\"Discount Effect by Customer Loyalty\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"Purchase Rate\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ PRICING OPTIMIZATION OPPORTUNITIES:\")\n",
    "print(\"üí∞ High-income customers less price sensitive - can charge premium\")\n",
    "print(\"üéØ Loyal customers need fewer discounts\")\n",
    "print(\"‚ö° Young customers very price sensitive - need targeted discounts\")\n",
    "print(\"üìä Current one-size-fits-all approach is suboptimal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Optimal Pricing Policy\n",
    "\n",
    "print(\"üéØ DESIGNING OPTIMAL DYNAMIC PRICING POLICY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Learn price effects using causal ML\n",
    "print(\"ü§ñ Step 1: Learning heterogeneous price effects...\")\n",
    "\n",
    "# Prepare data for causal learning\n",
    "pricing_features = [\n",
    "    \"customer_income\",\n",
    "    \"customer_age\",\n",
    "    \"customer_loyalty\",\n",
    "    \"price_sensitivity\",\n",
    "    \"base_price\",\n",
    "    \"product_quality\",\n",
    "]\n",
    "\n",
    "# Discretize discount into treatment (high vs low discount)\n",
    "median_discount = pricing_data[\"discount_rate\"].median()\n",
    "pricing_data[\"high_discount\"] = (\n",
    "    pricing_data[\"discount_rate\"] > median_discount\n",
    ").astype(int)\n",
    "\n",
    "# Use revenue as outcome (combines purchase probability and price)\n",
    "treatment_pricing = TreatmentData(\n",
    "    values=pricing_data[\"high_discount\"], name=\"high_discount\", treatment_type=\"binary\"\n",
    ")\n",
    "\n",
    "outcome_pricing = OutcomeData(\n",
    "    values=pricing_data[\"revenue\"], name=\"revenue\", outcome_type=\"continuous\"\n",
    ")\n",
    "\n",
    "covariates_pricing = CovariateData(\n",
    "    values=pricing_data[pricing_features], names=pricing_features\n",
    ")\n",
    "\n",
    "# Train causal forest for pricing effects\n",
    "pricing_forest = CausalForest(\n",
    "    n_estimators=200, min_samples_leaf=20, bootstrap_samples=100\n",
    ")\n",
    "\n",
    "pricing_forest.fit(treatment_pricing, outcome_pricing, covariates_pricing)\n",
    "\n",
    "# Predict discount effects for each interaction using correct API\n",
    "X_pricing_features = pricing_data[pricing_features].values\n",
    "discount_effects, discount_std = pricing_forest.predict_cate(X_pricing_features)\n",
    "pricing_data[\"predicted_discount_effect\"] = discount_effects\n",
    "\n",
    "print(\"‚úÖ Trained pricing model\")\n",
    "print(\n",
    "    f\"üìä Discount effects range: ${discount_effects.min():.2f} to ${discount_effects.max():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"üìà Mean effect: ${discount_effects.mean():.2f} (positive = discounts increase revenue)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Design optimal pricing policy\n",
    "def optimal_pricing_policy(customer_data):\n",
    "    \"\"\"\n",
    "    Determine optimal discount for each customer-product interaction\n",
    "    \"\"\"\n",
    "    # Simple policy: Give high discount if predicted effect > threshold\n",
    "    discount_threshold = 0  # Only discount if it increases expected revenue\n",
    "\n",
    "    should_discount = customer_data[\"predicted_discount_effect\"] > discount_threshold\n",
    "\n",
    "    # Assign discount levels\n",
    "    optimal_discounts = np.where(should_discount, 0.25, 0.05)  # 25% vs 5%\n",
    "\n",
    "    return optimal_discounts, should_discount\n",
    "\n",
    "\n",
    "# Apply optimal policy\n",
    "optimal_discounts, should_discount = optimal_pricing_policy(pricing_data)\n",
    "pricing_data[\"optimal_discount\"] = optimal_discounts\n",
    "pricing_data[\"should_discount\"] = should_discount\n",
    "\n",
    "\n",
    "# Simulate outcomes under optimal policy\n",
    "def simulate_policy_outcome(data, discount_col=\"optimal_discount\"):\n",
    "    \"\"\"\n",
    "    Simulate revenue and satisfaction under a given discount policy\n",
    "    \"\"\"\n",
    "    discounts = data[discount_col].values\n",
    "\n",
    "    # Recalculate purchase probabilities\n",
    "    base_prob = (\n",
    "        0.1\n",
    "        + 0.3 * data[\"product_quality\"]\n",
    "        + 0.2 * data[\"customer_loyalty\"]\n",
    "        + 0.1 * (data[\"customer_age\"] > 50)\n",
    "    )\n",
    "\n",
    "    # Price effects\n",
    "    income_effect = (\n",
    "        -0.5 * data[\"price_sensitivity\"] * (1 / (1 + data[\"customer_income\"] / 50000))\n",
    "    )\n",
    "    loyalty_effect = -0.3 * data[\"price_sensitivity\"] * (1 - data[\"customer_loyalty\"])\n",
    "    age_effect = -0.2 * data[\"price_sensitivity\"] * (data[\"customer_age\"] < 30) / 30\n",
    "\n",
    "    total_price_effect = (income_effect + loyalty_effect + age_effect) * discounts\n",
    "\n",
    "    purchase_prob = np.clip(base_prob + total_price_effect, 0.01, 0.95)\n",
    "    expected_purchases = purchase_prob  # Expected value\n",
    "\n",
    "    # Revenue calculation\n",
    "    final_prices = data[\"base_price\"] * (1 - discounts)\n",
    "    expected_revenue = final_prices * expected_purchases\n",
    "\n",
    "    # Satisfaction calculation\n",
    "    expected_satisfaction = (\n",
    "        5\n",
    "        + 2 * discounts\n",
    "        + 3 * data[\"product_quality\"] * expected_purchases\n",
    "        + -1 * (final_prices > data[\"base_price\"] * 0.8)\n",
    "    )\n",
    "    expected_satisfaction = np.clip(expected_satisfaction, 1, 10)\n",
    "\n",
    "    return {\n",
    "        \"avg_revenue\": expected_revenue.mean(),\n",
    "        \"total_revenue\": expected_revenue.sum(),\n",
    "        \"purchase_rate\": expected_purchases.mean(),\n",
    "        \"avg_satisfaction\": expected_satisfaction.mean(),\n",
    "        \"discount_rate\": discounts.mean(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare policies\n",
    "current_results = {\n",
    "    \"avg_revenue\": pricing_data[\"revenue\"].mean(),\n",
    "    \"total_revenue\": pricing_data[\"revenue\"].sum(),\n",
    "    \"purchase_rate\": pricing_data[\"purchased\"].mean(),\n",
    "    \"avg_satisfaction\": pricing_data[\"satisfaction\"].mean(),\n",
    "    \"discount_rate\": pricing_data[\"discount_rate\"].mean(),\n",
    "}\n",
    "\n",
    "optimal_results = simulate_policy_outcome(pricing_data, \"optimal_discount\")\n",
    "\n",
    "print(\"\\nüìä POLICY COMPARISON:\")\n",
    "print(f\"{'Metric':<20} {'Current':<12} {'Optimal':<12} {'Improvement':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in [\"avg_revenue\", \"purchase_rate\", \"avg_satisfaction\", \"discount_rate\"]:\n",
    "    current = current_results[metric]\n",
    "    optimal = optimal_results[metric]\n",
    "    improvement = ((optimal - current) / current) * 100 if current != 0 else 0\n",
    "\n",
    "    if metric in [\"avg_revenue\"]:\n",
    "        print(f\"{metric:<20} ${current:<11.2f} ${optimal:<11.2f} {improvement:+.1f}%\")\n",
    "    elif metric in [\"purchase_rate\", \"discount_rate\"]:\n",
    "        print(f\"{metric:<20} {current:<11.1%} {optimal:<11.1%} {improvement:+.1f}%\")\n",
    "    else:\n",
    "        print(f\"{metric:<20} {current:<11.2f} {optimal:<11.2f} {improvement:+.1f}%\")\n",
    "\n",
    "# Calculate total business impact\n",
    "revenue_improvement = (\n",
    "    optimal_results[\"total_revenue\"] - current_results[\"total_revenue\"]\n",
    ")\n",
    "print(\"\\nüí∞ TOTAL BUSINESS IMPACT:\")\n",
    "print(f\"üìà Additional revenue: ${revenue_improvement:,.0f}\")\n",
    "print(\n",
    "    f\"üìä Revenue lift: {(revenue_improvement / current_results['total_revenue']) * 100:.1f}%\"\n",
    ")\n",
    "\n",
    "if revenue_improvement > 0:\n",
    "    print(\"üéâ SUCCESS: Optimal pricing increases both revenue and satisfaction!\")\n",
    "\n",
    "# Visualize the optimal policy\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Who gets high discounts under optimal policy?\n",
    "high_discount_customers = pricing_data[pricing_data[\"should_discount\"]]\n",
    "low_discount_customers = pricing_data[~pricing_data[\"should_discount\"]]\n",
    "\n",
    "axes[0, 0].hist(\n",
    "    low_discount_customers[\"customer_income\"],\n",
    "    bins=20,\n",
    "    alpha=0.7,\n",
    "    label=\"Low Discount (5%)\",\n",
    "    density=True,\n",
    ")\n",
    "axes[0, 0].hist(\n",
    "    high_discount_customers[\"customer_income\"],\n",
    "    bins=20,\n",
    "    alpha=0.7,\n",
    "    label=\"High Discount (25%)\",\n",
    "    density=True,\n",
    ")\n",
    "axes[0, 0].set_title(\"Optimal Policy: Discounts by Income\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"Customer Income\")\n",
    "axes[0, 0].set_ylabel(\"Density\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Price sensitivity patterns\n",
    "axes[0, 1].boxplot(\n",
    "    [\n",
    "        low_discount_customers[\"price_sensitivity\"],\n",
    "        high_discount_customers[\"price_sensitivity\"],\n",
    "    ],\n",
    "    labels=[\"Low Discount\", \"High Discount\"],\n",
    ")\n",
    "axes[0, 1].set_title(\"Price Sensitivity by Discount Policy\", fontweight=\"bold\")\n",
    "axes[0, 1].set_ylabel(\"Price Sensitivity\")\n",
    "\n",
    "# Expected revenue improvement by customer segment\n",
    "pricing_data[\"revenue_improvement\"] = (\n",
    "    simulate_policy_outcome(pricing_data, \"optimal_discount\")[\"avg_revenue\"]\n",
    "    - pricing_data[\"revenue\"]\n",
    ")\n",
    "\n",
    "loyalty_segments = pd.cut(\n",
    "    pricing_data[\"customer_loyalty\"], bins=3, labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "improvement_by_loyalty = pricing_data.groupby(loyalty_segments)[\n",
    "    \"predicted_discount_effect\"\n",
    "].mean()\n",
    "\n",
    "axes[1, 0].bar(improvement_by_loyalty.index, improvement_by_loyalty.values, alpha=0.8)\n",
    "axes[1, 0].set_title(\"Expected Revenue Effect by Loyalty\", fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"Predicted Discount Effect ($)\")\n",
    "\n",
    "# Policy performance summary\n",
    "policy_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Current\": [\n",
    "            current_results[\"avg_revenue\"],\n",
    "            current_results[\"avg_satisfaction\"],\n",
    "            current_results[\"discount_rate\"],\n",
    "        ],\n",
    "        \"Optimal\": [\n",
    "            optimal_results[\"avg_revenue\"],\n",
    "            optimal_results[\"avg_satisfaction\"],\n",
    "            optimal_results[\"discount_rate\"],\n",
    "        ],\n",
    "    },\n",
    "    index=[\"Revenue ($)\", \"Satisfaction\", \"Discount Rate\"],\n",
    ")\n",
    "\n",
    "policy_comparison.plot(kind=\"bar\", ax=axes[1, 1], alpha=0.8)\n",
    "axes[1, 1].set_title(\"Policy Performance Comparison\", fontweight=\"bold\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ OPTIMAL PRICING INSIGHTS:\")\n",
    "print(\"üí∞ High-income, low-sensitivity customers: minimal discounts\")\n",
    "print(\"üéØ Price-sensitive, young customers: targeted high discounts\")\n",
    "print(\"‚öñÔ∏è Balances revenue maximization with customer satisfaction\")\n",
    "print(\"üöÄ Personalized pricing beats one-size-fits-all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Case Study 3: Adaptive A/B Testing with Causal Bandits\n",
    "\n",
    "**The Traditional Problem**: Standard A/B tests are static, inefficient, and can harm users assigned to inferior treatments.\n",
    "\n",
    "**The Innovation**: Adaptive testing that learns and shifts traffic to better treatments in real-time while maintaining statistical rigor!\n",
    "\n",
    "### Causal Multi-Armed Bandits\n",
    "- **Exploration**: Try different treatments to learn their effects\n",
    "- **Exploitation**: Assign more users to better treatments\n",
    "- **Causal Inference**: Account for confounding and heterogeneous effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 3: Adaptive A/B Testing with Causal Bandits\n",
    "\n",
    "\n",
    "class CausalMultiArmedBandit:\n",
    "    \"\"\"\n",
    "    Contextual multi-armed bandit that uses causal inference\n",
    "    to adaptively optimize treatment assignment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms, feature_dim, alpha=1.0, exploration_rate=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.feature_dim = feature_dim\n",
    "        self.alpha = alpha  # Confidence parameter\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "        # Initialize parameters for each arm\n",
    "        self.A = [np.eye(feature_dim) for _ in range(n_arms)]  # Covariance matrices\n",
    "        self.b = [np.zeros(feature_dim) for _ in range(n_arms)]  # Reward vectors\n",
    "        self.theta = [\n",
    "            np.zeros(feature_dim) for _ in range(n_arms)\n",
    "        ]  # Parameter estimates\n",
    "\n",
    "        # History tracking\n",
    "        self.history = {\"arms\": [], \"contexts\": [], \"rewards\": [], \"regrets\": []}\n",
    "\n",
    "    def select_arm(self, context, true_rewards=None):\n",
    "        \"\"\"\n",
    "        Select arm using Upper Confidence Bound with causal considerations\n",
    "        \"\"\"\n",
    "        context = np.array(context).reshape(-1)\n",
    "\n",
    "        # Exploration vs exploitation\n",
    "        if np.random.random() < self.exploration_rate:\n",
    "            # Pure exploration: choose random arm\n",
    "            chosen_arm = np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # UCB-style selection\n",
    "            ucb_values = []\n",
    "\n",
    "            for arm in range(self.n_arms):\n",
    "                # Expected reward\n",
    "                expected_reward = np.dot(self.theta[arm], context)\n",
    "\n",
    "                # Confidence interval\n",
    "                A_inv = np.linalg.inv(self.A[arm])\n",
    "                confidence_width = self.alpha * np.sqrt(\n",
    "                    np.dot(context, np.dot(A_inv, context))\n",
    "                )\n",
    "\n",
    "                # Upper confidence bound\n",
    "                ucb = expected_reward + confidence_width\n",
    "                ucb_values.append(ucb)\n",
    "\n",
    "            chosen_arm = np.argmax(ucb_values)\n",
    "\n",
    "        # Calculate regret if true rewards provided\n",
    "        regret = 0\n",
    "        if true_rewards is not None:\n",
    "            best_reward = max(true_rewards)\n",
    "            actual_reward = true_rewards[chosen_arm]\n",
    "            regret = best_reward - actual_reward\n",
    "\n",
    "        return chosen_arm, regret\n",
    "\n",
    "    def update(self, arm, context, reward):\n",
    "        \"\"\"\n",
    "        Update model parameters after observing reward\n",
    "        \"\"\"\n",
    "        context = np.array(context).reshape(-1)\n",
    "\n",
    "        # Update sufficient statistics\n",
    "        self.A[arm] += np.outer(context, context)\n",
    "        self.b[arm] += reward * context\n",
    "\n",
    "        # Update parameter estimate\n",
    "        self.theta[arm] = np.linalg.solve(self.A[arm], self.b[arm])\n",
    "\n",
    "        # Record history\n",
    "        self.history[\"arms\"].append(arm)\n",
    "        self.history[\"contexts\"].append(context.copy())\n",
    "        self.history[\"rewards\"].append(reward)\n",
    "\n",
    "    def get_arm_statistics(self):\n",
    "        \"\"\"\n",
    "        Get summary statistics for each arm\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        for arm in range(self.n_arms):\n",
    "            arm_history = [\n",
    "                (i, r)\n",
    "                for i, (a, r) in enumerate(\n",
    "                    zip(self.history[\"arms\"], self.history[\"rewards\"])\n",
    "                )\n",
    "                if a == arm\n",
    "            ]\n",
    "            if arm_history:\n",
    "                rewards = [r for _, r in arm_history]\n",
    "                stats[f\"arm_{arm}\"] = {\n",
    "                    \"count\": len(rewards),\n",
    "                    \"mean_reward\": np.mean(rewards),\n",
    "                    \"std_reward\": np.std(rewards),\n",
    "                    \"total_reward\": sum(rewards),\n",
    "                }\n",
    "            else:\n",
    "                stats[f\"arm_{arm}\"] = {\n",
    "                    \"count\": 0,\n",
    "                    \"mean_reward\": 0,\n",
    "                    \"std_reward\": 0,\n",
    "                    \"total_reward\": 0,\n",
    "                }\n",
    "        return stats\n",
    "\n",
    "\n",
    "def simulate_adaptive_ab_test(n_users=2000, n_treatments=3):\n",
    "    \"\"\"\n",
    "    Simulate adaptive A/B testing scenario\n",
    "    \"\"\"\n",
    "    # Generate user features\n",
    "    np.random.seed(42)\n",
    "\n",
    "    user_age = np.random.gamma(2, 20, n_users)\n",
    "    user_income = np.random.lognormal(10, 0.8, n_users)\n",
    "    user_engagement = np.random.beta(2, 3, n_users)\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(\n",
    "        np.column_stack([user_age, user_income, user_engagement])\n",
    "    )\n",
    "    features = np.column_stack([np.ones(n_users), features])  # Add intercept\n",
    "\n",
    "    # True treatment effects (unknown to the bandit)\n",
    "    true_coefficients = {\n",
    "        0: np.array([0.5, 0.1, 0.2, 0.1]),  # Control: modest baseline\n",
    "        1: np.array(\n",
    "            [0.7, 0.2, 0.1, 0.3]\n",
    "        ),  # Treatment A: better for young, engaged users\n",
    "        2: np.array([0.8, -0.1, 0.4, 0.2]),  # Treatment B: better for high-income users\n",
    "    }\n",
    "\n",
    "    # Calculate true rewards for each user under each treatment\n",
    "    true_rewards = np.zeros((n_users, n_treatments))\n",
    "    for user in range(n_users):\n",
    "        for treatment in range(n_treatments):\n",
    "            base_reward = np.dot(true_coefficients[treatment], features[user])\n",
    "            noise = np.random.normal(0, 0.1)\n",
    "            true_rewards[user, treatment] = base_reward + noise\n",
    "\n",
    "    # Optimal treatment for each user\n",
    "    optimal_treatments = np.argmax(true_rewards, axis=1)\n",
    "\n",
    "    return features, true_rewards, optimal_treatments, true_coefficients\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "print(\"üéØ ADAPTIVE A/B TESTING SIMULATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "features, true_rewards, optimal_treatments, true_coeffs = simulate_adaptive_ab_test(\n",
    "    2000, 3\n",
    ")\n",
    "n_users, feature_dim = features.shape\n",
    "n_treatments = len(true_coeffs)\n",
    "\n",
    "print(f\"üë• Users: {n_users:,}\")\n",
    "print(f\"üß™ Treatments: {n_treatments} (Control, Treatment A, Treatment B)\")\n",
    "print(f\"üìä Features: {feature_dim} (intercept + age, income, engagement)\")\n",
    "\n",
    "# Analyze true optimal policy\n",
    "optimal_reward_per_user = np.max(true_rewards, axis=1)\n",
    "total_optimal_reward = optimal_reward_per_user.sum()\n",
    "\n",
    "print(\"\\nüéØ TRUE OPTIMAL POLICY:\")\n",
    "for treatment in range(n_treatments):\n",
    "    pct_optimal = (optimal_treatments == treatment).mean()\n",
    "    print(f\"Treatment {treatment}: {pct_optimal:.1%} of users\")\n",
    "\n",
    "print(f\"üìà Optimal total reward: {total_optimal_reward:.1f}\")\n",
    "\n",
    "# Compare different testing strategies\n",
    "strategies = {\n",
    "    \"Random\": {\"exploration_rate\": 1.0, \"alpha\": 1.0},\n",
    "    \"Greedy\": {\"exploration_rate\": 0.0, \"alpha\": 1.0},\n",
    "    \"Œµ-Greedy\": {\"exploration_rate\": 0.1, \"alpha\": 1.0},\n",
    "    \"UCB\": {\"exploration_rate\": 0.05, \"alpha\": 2.0},\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for strategy_name, params in strategies.items():\n",
    "    print(f\"\\nüîÑ Running {strategy_name} strategy...\")\n",
    "\n",
    "    # Initialize bandit\n",
    "    bandit = CausalMultiArmedBandit(\n",
    "        n_arms=n_treatments,\n",
    "        feature_dim=feature_dim,\n",
    "        alpha=params[\"alpha\"],\n",
    "        exploration_rate=params[\"exploration_rate\"],\n",
    "    )\n",
    "\n",
    "    total_reward = 0\n",
    "    total_regret = 0\n",
    "    regret_history = []\n",
    "\n",
    "    # Sequential decision making\n",
    "    for user in range(n_users):\n",
    "        context = features[user]\n",
    "        user_true_rewards = true_rewards[user]\n",
    "\n",
    "        # Select treatment\n",
    "        chosen_arm, regret = bandit.select_arm(context, user_true_rewards)\n",
    "\n",
    "        # Observe reward\n",
    "        reward = user_true_rewards[chosen_arm]\n",
    "\n",
    "        # Update bandit\n",
    "        bandit.update(chosen_arm, context, reward)\n",
    "\n",
    "        # Track performance\n",
    "        total_reward += reward\n",
    "        total_regret += regret\n",
    "        regret_history.append(total_regret)\n",
    "\n",
    "    # Store results\n",
    "    results[strategy_name] = {\n",
    "        \"total_reward\": total_reward,\n",
    "        \"total_regret\": total_regret,\n",
    "        \"regret_history\": regret_history,\n",
    "        \"arm_stats\": bandit.get_arm_statistics(),\n",
    "        \"bandit\": bandit,\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Total reward: {total_reward:.1f}\")\n",
    "    print(f\"üìâ Total regret: {total_regret:.1f}\")\n",
    "    print(f\"üéØ Efficiency: {(total_reward / total_optimal_reward) * 100:.1f}%\")\n",
    "\n",
    "# Compare strategies\n",
    "print(\"\\nüìä STRATEGY COMPARISON:\")\n",
    "print(f\"{'Strategy':<12} {'Total Reward':<12} {'Total Regret':<12} {'Efficiency':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for strategy, result in results.items():\n",
    "    efficiency = (result[\"total_reward\"] / total_optimal_reward) * 100\n",
    "    print(\n",
    "        f\"{strategy:<12} {result['total_reward']:<12.1f} {result['total_regret']:<12.1f} {efficiency:<10.1f}%\"\n",
    "    )\n",
    "\n",
    "# Find best strategy\n",
    "best_strategy = min(results.keys(), key=lambda s: results[s][\"total_regret\"])\n",
    "print(f\"\\nüèÜ BEST STRATEGY: {best_strategy}\")\n",
    "print(\n",
    "    f\"‚ö° {(1 - results[best_strategy]['total_regret'] / results['Random']['total_regret']) * 100:.1f}% regret reduction vs random!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Adaptive Testing Results\n",
    "\n",
    "print(\"üìä ADAPTIVE TESTING VISUALIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Cumulative regret over time\n",
    "for strategy, result in results.items():\n",
    "    axes[0, 0].plot(result[\"regret_history\"], label=strategy, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title(\"Cumulative Regret Over Time\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"User Number\")\n",
    "axes[0, 0].set_ylabel(\"Cumulative Regret\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Treatment allocation by strategy\n",
    "strategy_names = list(results.keys())\n",
    "treatment_allocations = []\n",
    "\n",
    "for strategy in strategy_names:\n",
    "    arm_stats = results[strategy][\"arm_stats\"]\n",
    "    allocations = [arm_stats[f\"arm_{i}\"][\"count\"] for i in range(n_treatments)]\n",
    "    treatment_allocations.append(allocations)\n",
    "\n",
    "treatment_allocations = np.array(treatment_allocations)\n",
    "\n",
    "x = np.arange(len(strategy_names))\n",
    "width = 0.25\n",
    "\n",
    "for i in range(n_treatments):\n",
    "    axes[0, 1].bar(\n",
    "        x + i * width,\n",
    "        treatment_allocations[:, i],\n",
    "        width,\n",
    "        label=f\"Treatment {i}\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "axes[0, 1].set_title(\"Treatment Allocation by Strategy\", fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Strategy\")\n",
    "axes[0, 1].set_ylabel(\"Number of Users\")\n",
    "axes[0, 1].set_xticks(x + width)\n",
    "axes[0, 1].set_xticklabels(strategy_names)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Efficiency comparison\n",
    "efficiencies = [\n",
    "    (results[s][\"total_reward\"] / total_optimal_reward) * 100 for s in strategy_names\n",
    "]\n",
    "colors = [\"red\" if e < 85 else \"orange\" if e < 95 else \"green\" for e in efficiencies]\n",
    "\n",
    "bars = axes[1, 0].bar(strategy_names, efficiencies, color=colors, alpha=0.8)\n",
    "axes[1, 0].axhline(100, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"Optimal\")\n",
    "axes[1, 0].set_title(\"Strategy Efficiency\", fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"Efficiency (%)\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Add efficiency labels on bars\n",
    "for bar, eff in zip(bars, efficiencies):\n",
    "    axes[1, 0].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 1,\n",
    "        f\"{eff:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Learning curves (treatment selection accuracy over time)\n",
    "best_bandit = results[best_strategy][\"bandit\"]\n",
    "history_arms = best_bandit.history[\"arms\"]\n",
    "history_contexts = best_bandit.history[\"contexts\"]\n",
    "\n",
    "# Calculate accuracy in 100-user windows\n",
    "window_size = 100\n",
    "windows = range(window_size, len(history_arms), window_size)\n",
    "accuracies = []\n",
    "\n",
    "for end_idx in windows:\n",
    "    start_idx = end_idx - window_size\n",
    "    window_arms = history_arms[start_idx:end_idx]\n",
    "    window_optimal = optimal_treatments[start_idx:end_idx]\n",
    "    accuracy = (np.array(window_arms) == window_optimal).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "axes[1, 1].plot(windows, accuracies, \"o-\", linewidth=2, markersize=6, color=\"green\")\n",
    "axes[1, 1].axhline(\n",
    "    1 / n_treatments,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Random ({1 / n_treatments:.1%})\",\n",
    ")\n",
    "axes[1, 1].set_title(f\"{best_strategy}: Learning Curve\", fontweight=\"bold\")\n",
    "axes[1, 1].set_xlabel(\"User Number\")\n",
    "axes[1, 1].set_ylabel(\"Treatment Selection Accuracy\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze final learned treatment effects\n",
    "print(f\"\\nüß† LEARNED vs TRUE TREATMENT EFFECTS ({best_strategy}):\")\n",
    "print(f\"{'Treatment':<12} {'True Coeff':<25} {'Learned Coeff':<25}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "best_bandit = results[best_strategy][\"bandit\"]\n",
    "for treatment in range(n_treatments):\n",
    "    true_coeff = true_coeffs[treatment]\n",
    "    learned_coeff = best_bandit.theta[treatment]\n",
    "\n",
    "    print(\n",
    "        f\"Treatment {treatment:<3} {str(np.round(true_coeff, 2)):<25} {str(np.round(learned_coeff, 2)):<25}\"\n",
    "    )\n",
    "\n",
    "# Calculate final recommendations\n",
    "print(\"\\nüéØ FINAL ADAPTIVE TESTING INSIGHTS:\")\n",
    "print(\n",
    "    f\"‚úÖ {best_strategy} strategy achieved {efficiencies[strategy_names.index(best_strategy)]:.1f}% efficiency\"\n",
    ")\n",
    "print(\"üìà Learned to identify optimal treatments for most users\")\n",
    "print(\n",
    "    f\"‚ö° Reduced regret by {(1 - results[best_strategy]['total_regret'] / results['Random']['total_regret']) * 100:.1f}% vs random assignment\"\n",
    ")\n",
    "print(\"üöÄ Adaptive testing outperforms static A/B tests!\")\n",
    "\n",
    "# Business impact\n",
    "static_ab_reward = total_optimal_reward / n_treatments  # Equal allocation\n",
    "adaptive_improvement = (\n",
    "    (results[best_strategy][\"total_reward\"] - static_ab_reward) / static_ab_reward * 100\n",
    ")\n",
    "\n",
    "print(\"\\nüí∞ BUSINESS IMPACT vs STATIC A/B TEST:\")\n",
    "print(f\"üìä Static A/B test reward: {static_ab_reward:.1f}\")\n",
    "print(f\"üéØ Adaptive test reward: {results[best_strategy]['total_reward']:.1f}\")\n",
    "print(f\"‚¨ÜÔ∏è Improvement: {adaptive_improvement:.1f}%\")\n",
    "\n",
    "if adaptive_improvement > 10:\n",
    "    print(\"üéâ MASSIVE WIN: Adaptive testing dramatically outperforms static testing!\")\n",
    "elif adaptive_improvement > 5:\n",
    "    print(\"‚úÖ SIGNIFICANT WIN: Clear advantage for adaptive approach!\")\n",
    "else:\n",
    "    print(\"üìà MODEST WIN: Adaptive testing shows promise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## The Innovation Framework: From Analysis to Action\n",
    "\n",
    "### üîÑ The Change-Maker's Process\n",
    "\n",
    "1. **Discover Effects** ‚Üí Use causal inference to understand heterogeneous treatment effects\n",
    "2. **Design Policy** ‚Üí Create intervention rules based on individual characteristics  \n",
    "3. **Optimize Allocation** ‚Üí Use mathematical optimization to maximize outcomes\n",
    "4. **Implement Adaptively** ‚Üí Deploy systems that learn and improve over time\n",
    "5. **Measure Impact** ‚Üí Track real-world outcomes and iterate\n",
    "\n",
    "### üéØ Key Principles for Intervention Design\n",
    "\n",
    "#### 1. Leverage Heterogeneity\n",
    "- **Don't assume homogeneous effects**\n",
    "- **Target interventions** based on individual characteristics\n",
    "- **Avoid one-size-fits-all** policies\n",
    "\n",
    "#### 2. Optimize Multiple Objectives\n",
    "- **Balance efficiency and equity** (education example)\n",
    "- **Consider both short and long-term outcomes** (pricing example)\n",
    "- **Account for unintended consequences**\n",
    "\n",
    "#### 3. Embrace Adaptive Systems\n",
    "- **Learn from data continuously** (bandit algorithms)\n",
    "- **Update policies as new information arrives**\n",
    "- **Balance exploration and exploitation**\n",
    "\n",
    "#### 4. Design for Real-World Implementation\n",
    "- **Consider operational constraints** (budget, capacity)\n",
    "- **Plan for scalability**\n",
    "- **Build in feedback mechanisms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Innovator's Intervention Design Toolkit\n",
    "\n",
    "\n",
    "def intervention_design_framework():\n",
    "    \"\"\"\n",
    "    A systematic framework for designing causal interventions\n",
    "    \"\"\"\n",
    "\n",
    "    framework = \"\"\"\n",
    "    üõ†Ô∏è THE INNOVATOR'S INTERVENTION DESIGN FRAMEWORK\n",
    "    ================================================\n",
    "    \n",
    "    PHASE 1: CAUSAL DISCOVERY\n",
    "    -------------------------\n",
    "    ‚úÖ Identify treatment effects using causal ML\n",
    "    ‚úÖ Discover heterogeneous effects across subgroups\n",
    "    ‚úÖ Understand mechanisms and mediators\n",
    "    ‚úÖ Validate causal assumptions\n",
    "    \n",
    "    PHASE 2: POLICY DESIGN\n",
    "    ----------------------\n",
    "    ‚úÖ Define intervention goals and constraints\n",
    "    ‚úÖ Design targeting rules based on individual characteristics\n",
    "    ‚úÖ Optimize allocation to maximize outcomes\n",
    "    ‚úÖ Plan for edge cases and unintended consequences\n",
    "    \n",
    "    PHASE 3: ADAPTIVE IMPLEMENTATION\n",
    "    --------------------------------\n",
    "    ‚úÖ Deploy with built-in learning mechanisms\n",
    "    ‚úÖ Monitor outcomes in real-time\n",
    "    ‚úÖ Update policies based on new data\n",
    "    ‚úÖ Scale successful interventions\n",
    "    \n",
    "    PHASE 4: IMPACT MEASUREMENT\n",
    "    ---------------------------\n",
    "    ‚úÖ Track intended and unintended outcomes\n",
    "    ‚úÖ Measure distributional effects (who benefits?)\n",
    "    ‚úÖ Calculate return on investment\n",
    "    ‚úÖ Iterate and improve continuously\n",
    "    \"\"\"\n",
    "\n",
    "    return framework\n",
    "\n",
    "\n",
    "print(intervention_design_framework())\n",
    "\n",
    "\n",
    "# Intervention Design Checklist\n",
    "def intervention_checklist():\n",
    "    \"\"\"\n",
    "    Checklist for robust intervention design\n",
    "    \"\"\"\n",
    "\n",
    "    checklist = \"\"\"\n",
    "    ‚úÖ INTERVENTION DESIGN CHECKLIST\n",
    "    ===============================\n",
    "    \n",
    "    CAUSAL FOUNDATIONS:\n",
    "    ‚ñ° Used rigorous causal inference methods\n",
    "    ‚ñ° Validated identifying assumptions\n",
    "    ‚ñ° Checked for unmeasured confounding\n",
    "    ‚ñ° Explored heterogeneous effects\n",
    "    \n",
    "    POLICY OPTIMIZATION:\n",
    "    ‚ñ° Defined clear, measurable objectives\n",
    "    ‚ñ° Considered multiple stakeholder perspectives\n",
    "    ‚ñ° Accounted for resource constraints\n",
    "    ‚ñ° Designed for real-world implementation\n",
    "    \n",
    "    ETHICAL CONSIDERATIONS:\n",
    "    ‚ñ° Ensured equitable access and outcomes\n",
    "    ‚ñ° Minimized potential harms\n",
    "    ‚ñ° Obtained appropriate permissions/consent\n",
    "    ‚ñ° Planned for transparency and accountability\n",
    "    \n",
    "    ADAPTIVE MECHANISMS:\n",
    "    ‚ñ° Built in continuous learning\n",
    "    ‚ñ° Designed feedback loops\n",
    "    ‚ñ° Planned for policy updates\n",
    "    ‚ñ° Created early warning systems\n",
    "    \n",
    "    IMPACT MEASUREMENT:\n",
    "    ‚ñ° Defined success metrics upfront\n",
    "    ‚ñ° Planned for longitudinal tracking\n",
    "    ‚ñ° Considered spillover effects\n",
    "    ‚ñ° Prepared for scale-up decisions\n",
    "    \"\"\"\n",
    "\n",
    "    return checklist\n",
    "\n",
    "\n",
    "print(intervention_checklist())\n",
    "\n",
    "# Real-World Application Template\n",
    "application_template = \"\"\"\n",
    "üéØ YOUR INTERVENTION DESIGN CHALLENGE\n",
    "====================================\n",
    "\n",
    "Choose a real problem in your domain and apply this framework:\n",
    "\n",
    "STEP 1: PROBLEM DEFINITION\n",
    "‚Ä¢ What outcome do you want to improve?\n",
    "‚Ä¢ Who are the stakeholders?\n",
    "‚Ä¢ What are the current policies/practices?\n",
    "‚Ä¢ What constraints do you face?\n",
    "\n",
    "STEP 2: CAUSAL ANALYSIS\n",
    "‚Ä¢ What treatments/interventions are possible?\n",
    "‚Ä¢ What data do you have or need?\n",
    "‚Ä¢ How will you identify causal effects?\n",
    "‚Ä¢ What heterogeneity do you expect?\n",
    "\n",
    "STEP 3: INTERVENTION DESIGN\n",
    "‚Ä¢ How will you target interventions?\n",
    "‚Ä¢ What are your optimization objectives?\n",
    "‚Ä¢ How will you handle constraints?\n",
    "‚Ä¢ What could go wrong?\n",
    "\n",
    "STEP 4: IMPLEMENTATION PLAN\n",
    "‚Ä¢ How will you deploy and test?\n",
    "‚Ä¢ What will you measure and when?\n",
    "‚Ä¢ How will you adapt based on results?\n",
    "‚Ä¢ How will you scale if successful?\n",
    "\n",
    "EXAMPLES TO INSPIRE YOU:\n",
    "‚Ä¢ Healthcare: Personalized treatment protocols\n",
    "‚Ä¢ Education: Adaptive learning systems  \n",
    "‚Ä¢ Marketing: Dynamic pricing and promotions\n",
    "‚Ä¢ Policy: Targeted social programs\n",
    "‚Ä¢ Finance: Risk-based lending decisions\n",
    "‚Ä¢ HR: Personalized employee development\n",
    "\"\"\"\n",
    "\n",
    "print(application_template)\n",
    "\n",
    "print(\"\\nüöÄ READY TO CHANGE THE WORLD?\")\n",
    "print(\"Use this framework to transform your causal insights into real interventions!\")\n",
    "print(\n",
    "    \"Remember: The goal isn't just to understand - it's to improve outcomes for real people.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary: The Power of Applied Causal Innovation\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Educational Intervention Design**: Turned heterogeneous effect insights into optimal tutoring policies that improve both equity and efficiency\n",
    "\n",
    "2. **Dynamic Pricing Optimization**: Created personalized pricing strategies that boost revenue while maintaining customer satisfaction\n",
    "\n",
    "3. **Adaptive A/B Testing**: Built intelligent testing systems that learn and optimize in real-time\n",
    "\n",
    "### üåü Key Innovations Mastered\n",
    "\n",
    "- **Heterogeneity-Aware Policies**: Moving beyond one-size-fits-all to personalized interventions\n",
    "- **Multi-Objective Optimization**: Balancing competing goals like efficiency, equity, and satisfaction\n",
    "- **Adaptive Systems**: Building interventions that learn and improve continuously\n",
    "- **Real-World Implementation**: Designing for practical constraints and scalability\n",
    "\n",
    "### üéØ The Change-Maker's Mindset\n",
    "\n",
    "**Traditional thinking**: \"We found an effect, job done!\"\n",
    "\n",
    "**Revolutionary thinking**: \"How do we use this insight to improve real outcomes for real people?\"\n",
    "\n",
    "### üí° Core Principles\n",
    "\n",
    "1. **Start with Causal Understanding** ‚Üí Use rigorous methods to understand true effects\n",
    "2. **Design for Heterogeneity** ‚Üí Tailor interventions to individual characteristics\n",
    "3. **Optimize Holistically** ‚Üí Consider multiple objectives and stakeholders\n",
    "4. **Build Adaptive Systems** ‚Üí Create interventions that learn and improve\n",
    "5. **Measure Real Impact** ‚Üí Track outcomes that matter to people\n",
    "\n",
    "### üöÄ The Transformation\n",
    "\n",
    "You've learned to go beyond analysis to create **systems that change outcomes**:\n",
    "\n",
    "- **From correlation to causation** to **intervention**\n",
    "- **From static policies** to **adaptive optimization**\n",
    "- **From average effects** to **personalized targeting**\n",
    "- **From one-shot studies** to **continuous learning**\n",
    "\n",
    "### Next Steps in Your Innovation Journey\n",
    "\n",
    "Continue revolutionizing the field with:\n",
    "- **Tutorial 5**: Push Forward - Advanced causal methods\n",
    "- **Tutorial 6**: The Impossible - Causal inference with minimal data\n",
    "- **Tutorial 7**: Revolutionary Thinking - Creating new methods\n",
    "\n",
    "---\n",
    "\n",
    "*\"The people who think they can change the world are the ones who do.\" You now have the tools and mindset to turn causal insights into real-world impact. Go forth and change things!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
