{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Change Things: From Analysis to Intervention\n",
    "\n",
    "> *\"The people who think they can change the world are the ones who do\"* - This tutorial transforms causal insights into real-world interventions that create impact.\n",
    "\n",
    "## The Innovation Imperative\n",
    "\n",
    "Most data science stops at analysis. You discover relationships, build models, write reports. **Innovators go further.** They use causal insights to design interventions that actually change outcomes in the real world.\n",
    "\n",
    "### What You'll Master\n",
    "\n",
    "- **Intervention design** based on causal mechanisms\n",
    "- **Policy optimization** using causal insights\n",
    "- **A/B testing** with causal principles\n",
    "- **Adaptive interventions** that learn and improve\n",
    "- **Impact measurement** for real-world changes\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Moving from \"What causes what?\" to \"How do we change what?\"\n",
    "- Design interventions that actually work\n",
    "- Navigate complex systems and unintended consequences\n",
    "- Measure impact in messy real-world settings\n",
    "- Scale successful interventions\n",
    "\n",
    "**It's time to change things, not just analyze them!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: The Change-Maker's Arsenal\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, os.path.join(project_root, \"libs\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Our causal intervention toolkit\n",
    "from causal_inference.core.base import CovariateData, OutcomeData, TreatmentData\n",
    "from causal_inference.estimators.causal_forest import CausalForest\n",
    "\n",
    "# Set up the intervention laboratory\n",
    "np.random.seed(42)\n",
    "plt.style.use(\"default\")  # Use default style instead of deprecated seaborn style\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "print(\"🔧 INTERVENTION LABORATORY: Activated\")\n",
    "print(\"💡 From Insights to Action: Ready\")\n",
    "print(\"🚀 Time to change the world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Case Study 1: Designing Educational Interventions\n",
    "\n",
    "**The Setting**: You've discovered that personalized tutoring has heterogeneous effects - it helps struggling students but may actually hurt high-performers who become over-dependent.\n",
    "\n",
    "**The Challenge**: Design an intervention policy that maximizes overall learning outcomes.\n",
    "\n",
    "**The Innovation**: Use causal insights to create smart, targeted interventions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 1: Educational Intervention Design\n",
    "\n",
    "\n",
    "def generate_education_intervention_data(n_students=2000):\n",
    "    \"\"\"\n",
    "    Generate student data with heterogeneous tutoring effects\n",
    "    \"\"\"\n",
    "    # Student characteristics\n",
    "    baseline_ability = np.random.normal(75, 15, n_students)  # Baseline test scores\n",
    "    motivation = np.random.beta(2, 2, n_students)  # 0-1 scale\n",
    "    ses_background = np.random.gamma(2, 3, n_students)  # Socioeconomic status\n",
    "    prior_support = np.random.binomial(1, 0.4, n_students)  # Has support at home\n",
    "    learning_style = np.random.choice(\n",
    "        [\"visual\", \"auditory\", \"kinesthetic\"], n_students, p=[0.4, 0.35, 0.25]\n",
    "    )\n",
    "\n",
    "    # Treatment assignment (currently random, but we'll optimize this!)\n",
    "    receives_tutoring = np.random.binomial(1, 0.3, n_students)  # 30% get tutoring\n",
    "\n",
    "    # HETEROGENEOUS TREATMENT EFFECTS - The key insight!\n",
    "    # Tutoring helps struggling students but may hurt high performers\n",
    "\n",
    "    # Base effect depends on baseline ability (struggling students benefit more)\n",
    "    ability_effect = 15 * (baseline_ability < 70) + 8 * (baseline_ability < 80) * (\n",
    "        baseline_ability >= 70\n",
    "    )\n",
    "\n",
    "    # Motivation interaction (low motivation students benefit more from structure)\n",
    "    motivation_effect = 10 * (motivation < 0.4) + 5 * (motivation < 0.7) * (\n",
    "        motivation >= 0.4\n",
    "    )\n",
    "\n",
    "    # SES interaction (tutoring compensates for lack of home support)\n",
    "    ses_effect = 8 * (ses_background < 3) + 4 * (ses_background < 6) * (\n",
    "        ses_background >= 3\n",
    "    )\n",
    "\n",
    "    # NEGATIVE effects for high performers (over-dependence)\n",
    "    overhelp_penalty = -5 * (baseline_ability > 85) * (motivation > 0.7)\n",
    "\n",
    "    # Learning style match bonus\n",
    "    style_bonus = 3 * (\n",
    "        learning_style == \"visual\"\n",
    "    )  # Visual learners benefit most from tutoring\n",
    "\n",
    "    # Individual treatment effects\n",
    "    individual_effects = (\n",
    "        ability_effect + motivation_effect + ses_effect + overhelp_penalty + style_bonus\n",
    "    )\n",
    "\n",
    "    # Final test scores\n",
    "    noise = np.random.normal(0, 8, n_students)\n",
    "    test_score = (\n",
    "        baseline_ability\n",
    "        + individual_effects * receives_tutoring\n",
    "        + 5 * motivation  # Motivation always helps\n",
    "        + 2 * ses_background  # SES background effect\n",
    "        + 3 * prior_support  # Prior support effect\n",
    "        + noise\n",
    "    )\n",
    "\n",
    "    test_score = np.clip(test_score, 0, 100)  # Realistic test score range\n",
    "\n",
    "    # Create learning style dummies\n",
    "    style_dummies = pd.get_dummies(learning_style, prefix=\"style\")\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"student_id\": range(n_students),\n",
    "            \"baseline_ability\": baseline_ability,\n",
    "            \"motivation\": motivation,\n",
    "            \"ses_background\": ses_background,\n",
    "            \"prior_support\": prior_support,\n",
    "            \"receives_tutoring\": receives_tutoring,\n",
    "            \"test_score\": test_score,\n",
    "            \"true_effect\": individual_effects,\n",
    "            \"learning_style\": learning_style,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add learning style dummies\n",
    "    data = pd.concat([data, style_dummies], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Generate the data\n",
    "edu_data = generate_education_intervention_data(2000)\n",
    "\n",
    "print(\"🎓 Educational Intervention Dataset Generated\")\n",
    "print(f\"👥 Students: {len(edu_data):,}\")\n",
    "print(f\"📊 Current tutoring rate: {edu_data['receives_tutoring'].mean():.1%}\")\n",
    "print(\n",
    "    f\"🎭 Treatment effects range: {edu_data['true_effect'].min():.1f} to {edu_data['true_effect'].max():.1f} points\"\n",
    ")\n",
    "print(\"\\nData preview:\")\n",
    "print(\n",
    "    edu_data[\n",
    "        [\n",
    "            \"baseline_ability\",\n",
    "            \"motivation\",\n",
    "            \"receives_tutoring\",\n",
    "            \"test_score\",\n",
    "            \"true_effect\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "# Analyze current random policy\n",
    "current_avg_score = edu_data[\"test_score\"].mean()\n",
    "tutored_avg = edu_data[edu_data[\"receives_tutoring\"] == 1][\"test_score\"].mean()\n",
    "not_tutored_avg = edu_data[edu_data[\"receives_tutoring\"] == 0][\"test_score\"].mean()\n",
    "current_effect = tutored_avg - not_tutored_avg\n",
    "\n",
    "print(\"\\n📊 CURRENT RANDOM POLICY RESULTS:\")\n",
    "print(f\"📈 Overall average score: {current_avg_score:.1f}\")\n",
    "print(f\"🎓 Tutored students: {tutored_avg:.1f}\")\n",
    "print(f\"📚 Non-tutored students: {not_tutored_avg:.1f}\")\n",
    "print(f\"⚡ Naive treatment effect: {current_effect:.1f} points\")\n",
    "\n",
    "# Visualize the heterogeneity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Treatment effects by baseline ability\n",
    "ability_bins = pd.cut(edu_data[\"baseline_ability\"], bins=10)\n",
    "ability_effects = edu_data.groupby(ability_bins)[\"true_effect\"].mean()\n",
    "axes[0, 0].plot(\n",
    "    range(len(ability_effects)), ability_effects.values, \"o-\", linewidth=2, markersize=8\n",
    ")\n",
    "axes[0, 0].axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[0, 0].set_title(\n",
    "    \"Treatment Effect by Baseline Ability\\n(Negative for High Performers!)\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Ability Decile\")\n",
    "axes[0, 0].set_ylabel(\"True Treatment Effect\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Treatment effects by motivation\n",
    "motivation_bins = pd.cut(\n",
    "    edu_data[\"motivation\"],\n",
    "    bins=5,\n",
    "    labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"],\n",
    ")\n",
    "motivation_effects = edu_data.groupby(motivation_bins)[\"true_effect\"].mean()\n",
    "axes[0, 1].bar(motivation_effects.index, motivation_effects.values, alpha=0.8)\n",
    "axes[0, 1].set_title(\"Treatment Effect by Motivation Level\", fontweight=\"bold\")\n",
    "axes[0, 1].set_ylabel(\"True Treatment Effect\")\n",
    "axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Distribution of treatment effects\n",
    "axes[1, 0].hist(edu_data[\"true_effect\"], bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "axes[1, 0].axvline(\n",
    "    edu_data[\"true_effect\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {edu_data['true_effect'].mean():.1f}\",\n",
    ")\n",
    "axes[1, 0].axvline(0, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"No Effect\")\n",
    "negative_effects = (edu_data[\"true_effect\"] < 0).mean()\n",
    "axes[1, 0].set_title(\n",
    "    f\"Distribution of Treatment Effects\\n({negative_effects:.1%} have negative effects!)\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1, 0].set_xlabel(\"True Treatment Effect\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Learning style effects\n",
    "style_effects = edu_data.groupby(\"learning_style\")[\"true_effect\"].mean()\n",
    "axes[1, 1].bar(\n",
    "    style_effects.index,\n",
    "    style_effects.values,\n",
    "    alpha=0.8,\n",
    "    color=[\"skyblue\", \"lightcoral\", \"lightgreen\"],\n",
    ")\n",
    "axes[1, 1].set_title(\"Treatment Effect by Learning Style\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"True Treatment Effect\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHTS FOR INTERVENTION DESIGN:\")\n",
    "print(\n",
    "    f\"✅ Struggling students (ability < 70) benefit most: +{edu_data[edu_data['baseline_ability'] < 70]['true_effect'].mean():.1f} points\"\n",
    ")\n",
    "print(\n",
    "    f\"⚠️ High performers (ability > 85) may be hurt: {edu_data[edu_data['baseline_ability'] > 85]['true_effect'].mean():.1f} points\"\n",
    ")\n",
    "print(\n",
    "    f\"📊 {negative_effects:.1%} of students have negative treatment effects under current policy\"\n",
    ")\n",
    "print(\"💡 Optimal policy should target tutoring based on student characteristics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Learn Treatment Effects Using Causal ML\n",
    "\n",
    "print(\"🤖 STEP 1: Learning Individual Treatment Effects\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare data for causal learning\n",
    "feature_cols = [\n",
    "    \"baseline_ability\",\n",
    "    \"motivation\",\n",
    "    \"ses_background\",\n",
    "    \"prior_support\",\n",
    "    \"style_auditory\",\n",
    "    \"style_kinesthetic\",\n",
    "    \"style_visual\",\n",
    "]\n",
    "\n",
    "treatment = TreatmentData(\n",
    "    values=edu_data[\"receives_tutoring\"],\n",
    "    name=\"receives_tutoring\",\n",
    "    treatment_type=\"binary\",\n",
    ")\n",
    "\n",
    "outcome = OutcomeData(\n",
    "    values=edu_data[\"test_score\"], name=\"test_score\", outcome_type=\"continuous\"\n",
    ")\n",
    "\n",
    "covariates = CovariateData(values=edu_data[feature_cols], names=feature_cols)\n",
    "\n",
    "print(\"🎯 Training causal forest to predict individual treatment effects\")\n",
    "print(f\"📊 Features: {len(feature_cols)} student characteristics\")\n",
    "print(f\"👥 Sample: {len(edu_data)} students\")\n",
    "\n",
    "# Train causal forest\n",
    "causal_forest = CausalForest(\n",
    "    n_estimators=200, min_samples_leaf=20, bootstrap_samples=100\n",
    ")\n",
    "\n",
    "causal_forest.fit(treatment, outcome, covariates)\n",
    "\n",
    "# Predict individual treatment effects using correct API\n",
    "X_features = edu_data[feature_cols].values\n",
    "predicted_effects, prediction_std = causal_forest.predict_cate(X_features)\n",
    "edu_data[\"predicted_effect\"] = predicted_effects\n",
    "\n",
    "# Evaluate prediction quality\n",
    "true_effects = edu_data[\"true_effect\"].values\n",
    "prediction_corr = np.corrcoef(predicted_effects, true_effects)[0, 1]\n",
    "prediction_rmse = np.sqrt(np.mean((predicted_effects - true_effects) ** 2))\n",
    "\n",
    "print(\"\\n📈 TREATMENT EFFECT PREDICTION QUALITY:\")\n",
    "print(f\"🎯 Correlation with true effects: r = {prediction_corr:.3f}\")\n",
    "print(f\"📊 RMSE: {prediction_rmse:.2f} points\")\n",
    "print(\n",
    "    f\"💯 R²: {prediction_corr**2:.3f} (explains {prediction_corr**2 * 100:.1f}% of variation)\"\n",
    ")\n",
    "\n",
    "if prediction_corr > 0.6:\n",
    "    print(\"🏆 EXCELLENT prediction quality - ready to design optimal policy!\")\n",
    "elif prediction_corr > 0.4:\n",
    "    print(\"✅ GOOD prediction quality - proceeding with optimization!\")\n",
    "else:\n",
    "    print(\"⚠️ Moderate prediction quality - results may be approximate\")\n",
    "\n",
    "# Visualize prediction quality\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Predicted vs true effects\n",
    "axes[0].scatter(true_effects, predicted_effects, alpha=0.6, s=30)\n",
    "axes[0].plot(\n",
    "    [true_effects.min(), true_effects.max()],\n",
    "    [true_effects.min(), true_effects.max()],\n",
    "    \"r--\",\n",
    "    alpha=0.8,\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[0].set_title(\n",
    "    f\"Predicted vs True Effects\\nr = {prediction_corr:.3f}\", fontweight=\"bold\"\n",
    ")\n",
    "axes[0].set_xlabel(\"True Treatment Effect\")\n",
    "axes[0].set_ylabel(\"Predicted Treatment Effect\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution comparison\n",
    "axes[1].hist(true_effects, bins=25, alpha=0.7, label=\"True Effects\", density=True)\n",
    "axes[1].hist(\n",
    "    predicted_effects, bins=25, alpha=0.7, label=\"Predicted Effects\", density=True\n",
    ")\n",
    "axes[1].axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"No Effect\")\n",
    "axes[1].set_title(\"Distribution Comparison\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Treatment Effect\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Now we can design optimal intervention policies using these predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Design Optimal Intervention Policy\n",
    "\n",
    "print(\"🎯 STEP 2: Designing Optimal Intervention Policy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "def evaluate_policy(data, treatment_threshold, budget_constraint=0.3):\n",
    "    \"\"\"\n",
    "    Evaluate an intervention policy that treats students with predicted effect > threshold\n",
    "    \"\"\"\n",
    "    # Apply policy: treat if predicted effect > threshold\n",
    "    should_treat = data[\"predicted_effect\"] > treatment_threshold\n",
    "\n",
    "    # Check budget constraint\n",
    "    treatment_rate = should_treat.mean()\n",
    "    if treatment_rate > budget_constraint:\n",
    "        # If over budget, treat top students by predicted effect\n",
    "        n_to_treat = int(budget_constraint * len(data))\n",
    "        top_students = data.nlargest(n_to_treat, \"predicted_effect\").index\n",
    "        should_treat = data.index.isin(top_students)\n",
    "        treatment_rate = budget_constraint\n",
    "\n",
    "    # Calculate expected outcomes under this policy\n",
    "    # For treated: baseline + treatment effect\n",
    "    # For untreated: just baseline\n",
    "    baseline_scores = (\n",
    "        data[\"test_score\"] - data[\"true_effect\"] * data[\"receives_tutoring\"]\n",
    "    )  # Remove current treatment effect\n",
    "\n",
    "    expected_scores = baseline_scores + data[\"true_effect\"] * should_treat\n",
    "\n",
    "    policy_results = {\n",
    "        \"treatment_rate\": treatment_rate,\n",
    "        \"avg_score\": expected_scores.mean(),\n",
    "        \"treated_avg\": expected_scores[should_treat].mean()\n",
    "        if should_treat.sum() > 0\n",
    "        else 0,\n",
    "        \"untreated_avg\": expected_scores[~should_treat].mean()\n",
    "        if (~should_treat).sum() > 0\n",
    "        else 0,\n",
    "        \"total_benefit\": (data[\"true_effect\"] * should_treat).sum(),\n",
    "        \"students_helped\": (should_treat & (data[\"true_effect\"] > 0)).sum(),\n",
    "        \"students_harmed\": (should_treat & (data[\"true_effect\"] < 0)).sum(),\n",
    "    }\n",
    "\n",
    "    return policy_results, should_treat\n",
    "\n",
    "\n",
    "# Test different policy thresholds\n",
    "print(\"🔍 Testing different intervention thresholds...\")\n",
    "\n",
    "thresholds = np.arange(-5, 15, 1)  # Range of effect thresholds\n",
    "policy_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    results, _ = evaluate_policy(edu_data, threshold, budget_constraint=0.3)\n",
    "    results[\"threshold\"] = threshold\n",
    "    policy_results.append(results)\n",
    "\n",
    "policy_df = pd.DataFrame(policy_results)\n",
    "\n",
    "# Find optimal policy\n",
    "optimal_idx = policy_df[\"avg_score\"].idxmax()\n",
    "optimal_threshold = policy_df.loc[optimal_idx, \"threshold\"]\n",
    "optimal_results, optimal_treatment = evaluate_policy(edu_data, optimal_threshold, 0.3)\n",
    "\n",
    "print(\"\\n🏆 OPTIMAL POLICY DISCOVERED:\")\n",
    "print(f\"🎯 Threshold: Treat students with predicted effect > {optimal_threshold}\")\n",
    "print(f\"👥 Treatment rate: {optimal_results['treatment_rate']:.1%}\")\n",
    "print(f\"📈 Expected average score: {optimal_results['avg_score']:.1f}\")\n",
    "print(f\"✅ Students helped: {optimal_results['students_helped']}\")\n",
    "print(f\"❌ Students harmed: {optimal_results['students_harmed']}\")\n",
    "print(f\"⚡ Total benefit: {optimal_results['total_benefit']:.1f} points\")\n",
    "\n",
    "# Compare with current random policy\n",
    "current_total_benefit = (edu_data[\"true_effect\"] * edu_data[\"receives_tutoring\"]).sum()\n",
    "improvement = optimal_results[\"total_benefit\"] - current_total_benefit\n",
    "avg_improvement = optimal_results[\"avg_score\"] - current_avg_score\n",
    "\n",
    "print(\"\\n📊 IMPROVEMENT OVER RANDOM POLICY:\")\n",
    "print(f\"⬆️ Total benefit increase: {improvement:.1f} points\")\n",
    "print(f\"📈 Average score increase: {avg_improvement:.1f} points\")\n",
    "print(f\"🎯 Relative improvement: {improvement / abs(current_total_benefit) * 100:.1f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\n",
    "        \"🎉 BREAKTHROUGH: Optimal policy significantly outperforms random assignment!\"\n",
    "    )\n",
    "\n",
    "# Visualize policy comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Policy performance curve\n",
    "axes[0, 0].plot(\n",
    "    policy_df[\"threshold\"], policy_df[\"avg_score\"], \"o-\", linewidth=2, markersize=6\n",
    ")\n",
    "axes[0, 0].axvline(\n",
    "    optimal_threshold,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Optimal: {optimal_threshold}\",\n",
    ")\n",
    "axes[0, 0].axhline(\n",
    "    current_avg_score,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Current: {current_avg_score:.1f}\",\n",
    ")\n",
    "axes[0, 0].set_title(\"Policy Performance vs Threshold\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"Treatment Threshold\")\n",
    "axes[0, 0].set_ylabel(\"Expected Average Score\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Treatment rates\n",
    "axes[0, 1].plot(\n",
    "    policy_df[\"threshold\"],\n",
    "    policy_df[\"treatment_rate\"],\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    color=\"green\",\n",
    ")\n",
    "axes[0, 1].axhline(\n",
    "    0.3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Budget Constraint\"\n",
    ")\n",
    "axes[0, 1].set_title(\"Treatment Rate vs Threshold\", fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Treatment Threshold\")\n",
    "axes[0, 1].set_ylabel(\"Treatment Rate\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Who gets treated under optimal policy\n",
    "edu_data[\"optimal_treatment\"] = optimal_treatment\n",
    "treated_ability = edu_data[edu_data[\"optimal_treatment\"]][\"baseline_ability\"]\n",
    "not_treated_ability = edu_data[~edu_data[\"optimal_treatment\"]][\"baseline_ability\"]\n",
    "\n",
    "axes[1, 0].hist(\n",
    "    not_treated_ability, bins=20, alpha=0.7, label=\"Not Treated\", density=True\n",
    ")\n",
    "axes[1, 0].hist(treated_ability, bins=20, alpha=0.7, label=\"Treated\", density=True)\n",
    "axes[1, 0].set_title(\"Optimal Policy: Who Gets Treated?\", fontweight=\"bold\")\n",
    "axes[1, 0].set_xlabel(\"Baseline Ability\")\n",
    "axes[1, 0].set_ylabel(\"Density\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Treatment effects for treated vs not treated\n",
    "treated_effects = edu_data[edu_data[\"optimal_treatment\"]][\"true_effect\"]\n",
    "not_treated_effects = edu_data[~edu_data[\"optimal_treatment\"]][\"true_effect\"]\n",
    "\n",
    "axes[1, 1].boxplot(\n",
    "    [treated_effects, not_treated_effects], labels=[\"Treated\", \"Not Treated\"]\n",
    ")\n",
    "axes[1, 1].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[1, 1].set_title(\"Treatment Effects by Policy Assignment\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"True Treatment Effect\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 POLICY INSIGHTS:\")\n",
    "print(\"🎯 Optimal policy focuses on students with baseline ability 60-80\")\n",
    "print(\"⚠️ High performers (>85) correctly excluded to avoid harm\")\n",
    "print(\"✅ Low performers get priority - equity AND efficiency!\")\n",
    "print(\"🚀 Smart targeting beats random assignment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Case Study 2: Dynamic Pricing Optimization\n",
    "\n",
    "**The Setting**: An e-commerce platform wants to optimize pricing to maximize both revenue and customer satisfaction.\n",
    "\n",
    "**The Challenge**: Price sensitivity varies by customer, product, and context. Static pricing leaves money on the table and may alienate price-sensitive customers.\n",
    "\n",
    "**The Innovation**: Use causal ML to design personalized, dynamic pricing that adapts in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 2: Dynamic Pricing Optimization\n",
    "\n",
    "\n",
    "def generate_pricing_data(n_customers=3000, n_products=50):\n",
    "    \"\"\"\n",
    "    Generate customer purchase data with heterogeneous price sensitivity\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Customer characteristics\n",
    "    customer_income = np.random.lognormal(10, 0.8, n_customers)\n",
    "    customer_age = np.random.gamma(2, 20, n_customers)\n",
    "    customer_loyalty = np.random.beta(2, 3, n_customers)  # 0-1 scale\n",
    "    price_sensitivity = np.random.gamma(2, 0.5, n_customers)  # Higher = more sensitive\n",
    "\n",
    "    # Generate random customer-product interactions\n",
    "    n_interactions = 8000\n",
    "    customer_ids = np.random.choice(n_customers, n_interactions)\n",
    "    product_ids = np.random.choice(n_products, n_interactions)\n",
    "\n",
    "    # Product characteristics\n",
    "    base_prices = np.random.gamma(3, 20, n_products)  # Base product prices\n",
    "    product_quality = np.random.beta(3, 2, n_products)  # 0-1 quality scale\n",
    "\n",
    "    # Current pricing strategy (random discounts)\n",
    "    discount_rates = np.random.uniform(0, 0.3, n_interactions)  # 0-30% discounts\n",
    "    final_prices = base_prices[product_ids] * (1 - discount_rates)\n",
    "\n",
    "    # HETEROGENEOUS PRICE EFFECTS\n",
    "    # Base purchase probability\n",
    "    base_prob = (\n",
    "        0.1\n",
    "        + 0.3 * product_quality[product_ids]  # Quality increases probability\n",
    "        + 0.2 * customer_loyalty[customer_ids]  # Loyalty increases probability\n",
    "        + 0.1 * (customer_age[customer_ids] > 50)\n",
    "    )  # Older customers buy more\n",
    "\n",
    "    # Price effect varies by customer characteristics\n",
    "    # Income effect: Rich customers less price sensitive\n",
    "    income_effect = (\n",
    "        -0.5\n",
    "        * price_sensitivity[customer_ids]\n",
    "        * (1 / (1 + customer_income[customer_ids] / 50000))\n",
    "    )\n",
    "\n",
    "    # Loyalty effect: Loyal customers less price sensitive\n",
    "    loyalty_effect = (\n",
    "        -0.3 * price_sensitivity[customer_ids] * (1 - customer_loyalty[customer_ids])\n",
    "    )\n",
    "\n",
    "    # Age effect: Older customers less price sensitive\n",
    "    age_effect = (\n",
    "        -0.2 * price_sensitivity[customer_ids] * (customer_age[customer_ids] < 30) / 30\n",
    "    )\n",
    "\n",
    "    # Total price effect (discount increases purchase probability)\n",
    "    total_price_effect = (income_effect + loyalty_effect + age_effect) * discount_rates\n",
    "\n",
    "    # Final purchase probability\n",
    "    purchase_prob = np.clip(base_prob + total_price_effect, 0.01, 0.95)\n",
    "    purchased = np.random.binomial(1, purchase_prob, n_interactions)\n",
    "\n",
    "    # Revenue calculation\n",
    "    revenue_per_interaction = final_prices * purchased\n",
    "\n",
    "    # Customer satisfaction (decreases with high prices, increases with discounts)\n",
    "    satisfaction = (\n",
    "        5  # Base satisfaction\n",
    "        + 2 * discount_rates  # Discounts increase satisfaction\n",
    "        + 3 * product_quality[product_ids] * purchased  # Quality matters when purchased\n",
    "        + -1\n",
    "        * (\n",
    "            final_prices > base_prices[product_ids] * 0.8\n",
    "        )  # High prices decrease satisfaction\n",
    "        + np.random.normal(0, 0.5, n_interactions)\n",
    "    )  # Noise\n",
    "\n",
    "    satisfaction = np.clip(satisfaction, 1, 10)  # 1-10 scale\n",
    "\n",
    "    # Create dataset\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"customer_id\": customer_ids,\n",
    "            \"product_id\": product_ids,\n",
    "            \"customer_income\": customer_income[customer_ids],\n",
    "            \"customer_age\": customer_age[customer_ids],\n",
    "            \"customer_loyalty\": customer_loyalty[customer_ids],\n",
    "            \"price_sensitivity\": price_sensitivity[customer_ids],\n",
    "            \"base_price\": base_prices[product_ids],\n",
    "            \"product_quality\": product_quality[product_ids],\n",
    "            \"discount_rate\": discount_rates,\n",
    "            \"final_price\": final_prices,\n",
    "            \"purchased\": purchased,\n",
    "            \"revenue\": revenue_per_interaction,\n",
    "            \"satisfaction\": satisfaction,\n",
    "            \"true_price_effect\": total_price_effect\n",
    "            / discount_rates,  # Effect per unit discount\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Generate pricing data\n",
    "pricing_data = generate_pricing_data(3000, 50)\n",
    "\n",
    "print(\"💰 Dynamic Pricing Dataset Generated\")\n",
    "print(f\"👥 Customers: {pricing_data['customer_id'].nunique():,}\")\n",
    "print(f\"🛍️ Products: {pricing_data['product_id'].nunique()}\")\n",
    "print(f\"📊 Interactions: {len(pricing_data):,}\")\n",
    "print(f\"💵 Current purchase rate: {pricing_data['purchased'].mean():.1%}\")\n",
    "print(f\"💰 Current revenue per interaction: ${pricing_data['revenue'].mean():.2f}\")\n",
    "print(f\"😊 Current satisfaction: {pricing_data['satisfaction'].mean():.2f}/10\")\n",
    "\n",
    "print(\"\\nData preview:\")\n",
    "print(\n",
    "    pricing_data[\n",
    "        [\n",
    "            \"customer_income\",\n",
    "            \"customer_age\",\n",
    "            \"price_sensitivity\",\n",
    "            \"discount_rate\",\n",
    "            \"purchased\",\n",
    "            \"revenue\",\n",
    "            \"satisfaction\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "# Analyze current pricing strategy\n",
    "print(\"\\n📊 CURRENT RANDOM PRICING ANALYSIS:\")\n",
    "\n",
    "# Revenue by discount level\n",
    "discount_bins = pd.cut(\n",
    "    pricing_data[\"discount_rate\"],\n",
    "    bins=5,\n",
    "    labels=[\"0-6%\", \"6-12%\", \"12-18%\", \"18-24%\", \"24-30%\"],\n",
    ")\n",
    "revenue_by_discount = pricing_data.groupby(discount_bins).agg(\n",
    "    {\"revenue\": \"mean\", \"purchased\": \"mean\", \"satisfaction\": \"mean\"}\n",
    ")\n",
    "\n",
    "print(\"Revenue by discount level:\")\n",
    "print(revenue_by_discount)\n",
    "\n",
    "# Visualize the pricing landscape\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Purchase rate by discount and customer income\n",
    "income_quintiles = pd.qcut(\n",
    "    pricing_data[\"customer_income\"], 5, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\"]\n",
    ")\n",
    "purchase_by_income_discount = (\n",
    "    pricing_data.groupby([income_quintiles, discount_bins])[\"purchased\"]\n",
    "    .mean()\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "sns.heatmap(purchase_by_income_discount, annot=True, cmap=\"Blues\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Purchase Rate by Income & Discount\", fontweight=\"bold\")\n",
    "axes[0, 0].set_ylabel(\"Income Quintile\")\n",
    "axes[0, 0].set_xlabel(\"Discount Rate\")\n",
    "\n",
    "# Price sensitivity distribution\n",
    "axes[0, 1].hist(\n",
    "    pricing_data[\"price_sensitivity\"], bins=30, alpha=0.7, edgecolor=\"black\"\n",
    ")\n",
    "axes[0, 1].set_title(\"Distribution of Price Sensitivity\", fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Price Sensitivity\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Revenue vs satisfaction tradeoff\n",
    "axes[1, 0].scatter(\n",
    "    pricing_data[\"satisfaction\"], pricing_data[\"revenue\"], alpha=0.5, s=20\n",
    ")\n",
    "axes[1, 0].set_title(\"Revenue vs Satisfaction Tradeoff\", fontweight=\"bold\")\n",
    "axes[1, 0].set_xlabel(\"Customer Satisfaction\")\n",
    "axes[1, 0].set_ylabel(\"Revenue per Interaction\")\n",
    "\n",
    "# Discount effect by customer loyalty\n",
    "loyalty_bins = pd.cut(\n",
    "    pricing_data[\"customer_loyalty\"], bins=3, labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "discount_effect_by_loyalty = (\n",
    "    pricing_data.groupby([loyalty_bins, discount_bins])[\"purchased\"].mean().unstack()\n",
    ")\n",
    "\n",
    "discount_effect_by_loyalty.plot(kind=\"bar\", ax=axes[1, 1], alpha=0.8)\n",
    "axes[1, 1].set_title(\"Discount Effect by Customer Loyalty\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"Purchase Rate\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 PRICING OPTIMIZATION OPPORTUNITIES:\")\n",
    "print(\"💰 High-income customers less price sensitive - can charge premium\")\n",
    "print(\"🎯 Loyal customers need fewer discounts\")\n",
    "print(\"⚡ Young customers very price sensitive - need targeted discounts\")\n",
    "print(\"📊 Current one-size-fits-all approach is suboptimal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Optimal Pricing Policy\n",
    "\n",
    "print(\"🎯 DESIGNING OPTIMAL DYNAMIC PRICING POLICY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Learn price effects using causal ML\n",
    "print(\"🤖 Step 1: Learning heterogeneous price effects...\")\n",
    "\n",
    "# Prepare data for causal learning\n",
    "pricing_features = [\n",
    "    \"customer_income\",\n",
    "    \"customer_age\",\n",
    "    \"customer_loyalty\",\n",
    "    \"price_sensitivity\",\n",
    "    \"base_price\",\n",
    "    \"product_quality\",\n",
    "]\n",
    "\n",
    "# Discretize discount into treatment (high vs low discount)\n",
    "median_discount = pricing_data[\"discount_rate\"].median()\n",
    "pricing_data[\"high_discount\"] = (\n",
    "    pricing_data[\"discount_rate\"] > median_discount\n",
    ").astype(int)\n",
    "\n",
    "# Use revenue as outcome (combines purchase probability and price)\n",
    "treatment_pricing = TreatmentData(\n",
    "    values=pricing_data[\"high_discount\"], name=\"high_discount\", treatment_type=\"binary\"\n",
    ")\n",
    "\n",
    "outcome_pricing = OutcomeData(\n",
    "    values=pricing_data[\"revenue\"], name=\"revenue\", outcome_type=\"continuous\"\n",
    ")\n",
    "\n",
    "covariates_pricing = CovariateData(\n",
    "    values=pricing_data[pricing_features], names=pricing_features\n",
    ")\n",
    "\n",
    "# Train causal forest for pricing effects\n",
    "pricing_forest = CausalForest(\n",
    "    n_estimators=200, min_samples_leaf=20, bootstrap_samples=100\n",
    ")\n",
    "\n",
    "pricing_forest.fit(treatment_pricing, outcome_pricing, covariates_pricing)\n",
    "\n",
    "# Predict discount effects for each interaction using correct API\n",
    "X_pricing_features = pricing_data[pricing_features].values\n",
    "discount_effects, discount_std = pricing_forest.predict_cate(X_pricing_features)\n",
    "pricing_data[\"predicted_discount_effect\"] = discount_effects\n",
    "\n",
    "print(\"✅ Trained pricing model\")\n",
    "print(\n",
    "    f\"📊 Discount effects range: ${discount_effects.min():.2f} to ${discount_effects.max():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"📈 Mean effect: ${discount_effects.mean():.2f} (positive = discounts increase revenue)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Design optimal pricing policy\n",
    "def optimal_pricing_policy(customer_data):\n",
    "    \"\"\"\n",
    "    Determine optimal discount for each customer-product interaction\n",
    "    \"\"\"\n",
    "    # Simple policy: Give high discount if predicted effect > threshold\n",
    "    discount_threshold = 0  # Only discount if it increases expected revenue\n",
    "\n",
    "    should_discount = customer_data[\"predicted_discount_effect\"] > discount_threshold\n",
    "\n",
    "    # Assign discount levels\n",
    "    optimal_discounts = np.where(should_discount, 0.25, 0.05)  # 25% vs 5%\n",
    "\n",
    "    return optimal_discounts, should_discount\n",
    "\n",
    "\n",
    "# Apply optimal policy\n",
    "optimal_discounts, should_discount = optimal_pricing_policy(pricing_data)\n",
    "pricing_data[\"optimal_discount\"] = optimal_discounts\n",
    "pricing_data[\"should_discount\"] = should_discount\n",
    "\n",
    "\n",
    "# Simulate outcomes under optimal policy\n",
    "def simulate_policy_outcome(data, discount_col=\"optimal_discount\"):\n",
    "    \"\"\"\n",
    "    Simulate revenue and satisfaction under a given discount policy\n",
    "    \"\"\"\n",
    "    discounts = data[discount_col].values\n",
    "\n",
    "    # Recalculate purchase probabilities\n",
    "    base_prob = (\n",
    "        0.1\n",
    "        + 0.3 * data[\"product_quality\"]\n",
    "        + 0.2 * data[\"customer_loyalty\"]\n",
    "        + 0.1 * (data[\"customer_age\"] > 50)\n",
    "    )\n",
    "\n",
    "    # Price effects\n",
    "    income_effect = (\n",
    "        -0.5 * data[\"price_sensitivity\"] * (1 / (1 + data[\"customer_income\"] / 50000))\n",
    "    )\n",
    "    loyalty_effect = -0.3 * data[\"price_sensitivity\"] * (1 - data[\"customer_loyalty\"])\n",
    "    age_effect = -0.2 * data[\"price_sensitivity\"] * (data[\"customer_age\"] < 30) / 30\n",
    "\n",
    "    total_price_effect = (income_effect + loyalty_effect + age_effect) * discounts\n",
    "\n",
    "    purchase_prob = np.clip(base_prob + total_price_effect, 0.01, 0.95)\n",
    "    expected_purchases = purchase_prob  # Expected value\n",
    "\n",
    "    # Revenue calculation\n",
    "    final_prices = data[\"base_price\"] * (1 - discounts)\n",
    "    expected_revenue = final_prices * expected_purchases\n",
    "\n",
    "    # Satisfaction calculation\n",
    "    expected_satisfaction = (\n",
    "        5\n",
    "        + 2 * discounts\n",
    "        + 3 * data[\"product_quality\"] * expected_purchases\n",
    "        + -1 * (final_prices > data[\"base_price\"] * 0.8)\n",
    "    )\n",
    "    expected_satisfaction = np.clip(expected_satisfaction, 1, 10)\n",
    "\n",
    "    return {\n",
    "        \"avg_revenue\": expected_revenue.mean(),\n",
    "        \"total_revenue\": expected_revenue.sum(),\n",
    "        \"purchase_rate\": expected_purchases.mean(),\n",
    "        \"avg_satisfaction\": expected_satisfaction.mean(),\n",
    "        \"discount_rate\": discounts.mean(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare policies\n",
    "current_results = {\n",
    "    \"avg_revenue\": pricing_data[\"revenue\"].mean(),\n",
    "    \"total_revenue\": pricing_data[\"revenue\"].sum(),\n",
    "    \"purchase_rate\": pricing_data[\"purchased\"].mean(),\n",
    "    \"avg_satisfaction\": pricing_data[\"satisfaction\"].mean(),\n",
    "    \"discount_rate\": pricing_data[\"discount_rate\"].mean(),\n",
    "}\n",
    "\n",
    "optimal_results = simulate_policy_outcome(pricing_data, \"optimal_discount\")\n",
    "\n",
    "print(\"\\n📊 POLICY COMPARISON:\")\n",
    "print(f\"{'Metric':<20} {'Current':<12} {'Optimal':<12} {'Improvement':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in [\"avg_revenue\", \"purchase_rate\", \"avg_satisfaction\", \"discount_rate\"]:\n",
    "    current = current_results[metric]\n",
    "    optimal = optimal_results[metric]\n",
    "    improvement = ((optimal - current) / current) * 100 if current != 0 else 0\n",
    "\n",
    "    if metric in [\"avg_revenue\"]:\n",
    "        print(f\"{metric:<20} ${current:<11.2f} ${optimal:<11.2f} {improvement:+.1f}%\")\n",
    "    elif metric in [\"purchase_rate\", \"discount_rate\"]:\n",
    "        print(f\"{metric:<20} {current:<11.1%} {optimal:<11.1%} {improvement:+.1f}%\")\n",
    "    else:\n",
    "        print(f\"{metric:<20} {current:<11.2f} {optimal:<11.2f} {improvement:+.1f}%\")\n",
    "\n",
    "# Calculate total business impact\n",
    "revenue_improvement = (\n",
    "    optimal_results[\"total_revenue\"] - current_results[\"total_revenue\"]\n",
    ")\n",
    "print(\"\\n💰 TOTAL BUSINESS IMPACT:\")\n",
    "print(f\"📈 Additional revenue: ${revenue_improvement:,.0f}\")\n",
    "print(\n",
    "    f\"📊 Revenue lift: {(revenue_improvement / current_results['total_revenue']) * 100:.1f}%\"\n",
    ")\n",
    "\n",
    "if revenue_improvement > 0:\n",
    "    print(\"🎉 SUCCESS: Optimal pricing increases both revenue and satisfaction!\")\n",
    "\n",
    "# Visualize the optimal policy\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Who gets high discounts under optimal policy?\n",
    "high_discount_customers = pricing_data[pricing_data[\"should_discount\"]]\n",
    "low_discount_customers = pricing_data[~pricing_data[\"should_discount\"]]\n",
    "\n",
    "axes[0, 0].hist(\n",
    "    low_discount_customers[\"customer_income\"],\n",
    "    bins=20,\n",
    "    alpha=0.7,\n",
    "    label=\"Low Discount (5%)\",\n",
    "    density=True,\n",
    ")\n",
    "axes[0, 0].hist(\n",
    "    high_discount_customers[\"customer_income\"],\n",
    "    bins=20,\n",
    "    alpha=0.7,\n",
    "    label=\"High Discount (25%)\",\n",
    "    density=True,\n",
    ")\n",
    "axes[0, 0].set_title(\"Optimal Policy: Discounts by Income\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"Customer Income\")\n",
    "axes[0, 0].set_ylabel(\"Density\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Price sensitivity patterns\n",
    "axes[0, 1].boxplot(\n",
    "    [\n",
    "        low_discount_customers[\"price_sensitivity\"],\n",
    "        high_discount_customers[\"price_sensitivity\"],\n",
    "    ],\n",
    "    labels=[\"Low Discount\", \"High Discount\"],\n",
    ")\n",
    "axes[0, 1].set_title(\"Price Sensitivity by Discount Policy\", fontweight=\"bold\")\n",
    "axes[0, 1].set_ylabel(\"Price Sensitivity\")\n",
    "\n",
    "# Expected revenue improvement by customer segment\n",
    "pricing_data[\"revenue_improvement\"] = (\n",
    "    simulate_policy_outcome(pricing_data, \"optimal_discount\")[\"avg_revenue\"]\n",
    "    - pricing_data[\"revenue\"]\n",
    ")\n",
    "\n",
    "loyalty_segments = pd.cut(\n",
    "    pricing_data[\"customer_loyalty\"], bins=3, labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "improvement_by_loyalty = pricing_data.groupby(loyalty_segments)[\n",
    "    \"predicted_discount_effect\"\n",
    "].mean()\n",
    "\n",
    "axes[1, 0].bar(improvement_by_loyalty.index, improvement_by_loyalty.values, alpha=0.8)\n",
    "axes[1, 0].set_title(\"Expected Revenue Effect by Loyalty\", fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"Predicted Discount Effect ($)\")\n",
    "\n",
    "# Policy performance summary\n",
    "policy_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Current\": [\n",
    "            current_results[\"avg_revenue\"],\n",
    "            current_results[\"avg_satisfaction\"],\n",
    "            current_results[\"discount_rate\"],\n",
    "        ],\n",
    "        \"Optimal\": [\n",
    "            optimal_results[\"avg_revenue\"],\n",
    "            optimal_results[\"avg_satisfaction\"],\n",
    "            optimal_results[\"discount_rate\"],\n",
    "        ],\n",
    "    },\n",
    "    index=[\"Revenue ($)\", \"Satisfaction\", \"Discount Rate\"],\n",
    ")\n",
    "\n",
    "policy_comparison.plot(kind=\"bar\", ax=axes[1, 1], alpha=0.8)\n",
    "axes[1, 1].set_title(\"Policy Performance Comparison\", fontweight=\"bold\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 OPTIMAL PRICING INSIGHTS:\")\n",
    "print(\"💰 High-income, low-sensitivity customers: minimal discounts\")\n",
    "print(\"🎯 Price-sensitive, young customers: targeted high discounts\")\n",
    "print(\"⚖️ Balances revenue maximization with customer satisfaction\")\n",
    "print(\"🚀 Personalized pricing beats one-size-fits-all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Case Study 3: Adaptive A/B Testing with Causal Bandits\n",
    "\n",
    "**The Traditional Problem**: Standard A/B tests are static, inefficient, and can harm users assigned to inferior treatments.\n",
    "\n",
    "**The Innovation**: Adaptive testing that learns and shifts traffic to better treatments in real-time while maintaining statistical rigor!\n",
    "\n",
    "### Causal Multi-Armed Bandits\n",
    "- **Exploration**: Try different treatments to learn their effects\n",
    "- **Exploitation**: Assign more users to better treatments\n",
    "- **Causal Inference**: Account for confounding and heterogeneous effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study 3: Adaptive A/B Testing with Causal Bandits\n",
    "\n",
    "\n",
    "class CausalMultiArmedBandit:\n",
    "    \"\"\"\n",
    "    Contextual multi-armed bandit that uses causal inference\n",
    "    to adaptively optimize treatment assignment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms, feature_dim, alpha=1.0, exploration_rate=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.feature_dim = feature_dim\n",
    "        self.alpha = alpha  # Confidence parameter\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "        # Initialize parameters for each arm\n",
    "        self.A = [np.eye(feature_dim) for _ in range(n_arms)]  # Covariance matrices\n",
    "        self.b = [np.zeros(feature_dim) for _ in range(n_arms)]  # Reward vectors\n",
    "        self.theta = [\n",
    "            np.zeros(feature_dim) for _ in range(n_arms)\n",
    "        ]  # Parameter estimates\n",
    "\n",
    "        # History tracking\n",
    "        self.history = {\"arms\": [], \"contexts\": [], \"rewards\": [], \"regrets\": []}\n",
    "\n",
    "    def select_arm(self, context, true_rewards=None):\n",
    "        \"\"\"\n",
    "        Select arm using Upper Confidence Bound with causal considerations\n",
    "        \"\"\"\n",
    "        context = np.array(context).reshape(-1)\n",
    "\n",
    "        # Exploration vs exploitation\n",
    "        if np.random.random() < self.exploration_rate:\n",
    "            # Pure exploration: choose random arm\n",
    "            chosen_arm = np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # UCB-style selection\n",
    "            ucb_values = []\n",
    "\n",
    "            for arm in range(self.n_arms):\n",
    "                # Expected reward\n",
    "                expected_reward = np.dot(self.theta[arm], context)\n",
    "\n",
    "                # Confidence interval\n",
    "                A_inv = np.linalg.inv(self.A[arm])\n",
    "                confidence_width = self.alpha * np.sqrt(\n",
    "                    np.dot(context, np.dot(A_inv, context))\n",
    "                )\n",
    "\n",
    "                # Upper confidence bound\n",
    "                ucb = expected_reward + confidence_width\n",
    "                ucb_values.append(ucb)\n",
    "\n",
    "            chosen_arm = np.argmax(ucb_values)\n",
    "\n",
    "        # Calculate regret if true rewards provided\n",
    "        regret = 0\n",
    "        if true_rewards is not None:\n",
    "            best_reward = max(true_rewards)\n",
    "            actual_reward = true_rewards[chosen_arm]\n",
    "            regret = best_reward - actual_reward\n",
    "\n",
    "        return chosen_arm, regret\n",
    "\n",
    "    def update(self, arm, context, reward):\n",
    "        \"\"\"\n",
    "        Update model parameters after observing reward\n",
    "        \"\"\"\n",
    "        context = np.array(context).reshape(-1)\n",
    "\n",
    "        # Update sufficient statistics\n",
    "        self.A[arm] += np.outer(context, context)\n",
    "        self.b[arm] += reward * context\n",
    "\n",
    "        # Update parameter estimate\n",
    "        self.theta[arm] = np.linalg.solve(self.A[arm], self.b[arm])\n",
    "\n",
    "        # Record history\n",
    "        self.history[\"arms\"].append(arm)\n",
    "        self.history[\"contexts\"].append(context.copy())\n",
    "        self.history[\"rewards\"].append(reward)\n",
    "\n",
    "    def get_arm_statistics(self):\n",
    "        \"\"\"\n",
    "        Get summary statistics for each arm\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        for arm in range(self.n_arms):\n",
    "            arm_history = [\n",
    "                (i, r)\n",
    "                for i, (a, r) in enumerate(\n",
    "                    zip(self.history[\"arms\"], self.history[\"rewards\"])\n",
    "                )\n",
    "                if a == arm\n",
    "            ]\n",
    "            if arm_history:\n",
    "                rewards = [r for _, r in arm_history]\n",
    "                stats[f\"arm_{arm}\"] = {\n",
    "                    \"count\": len(rewards),\n",
    "                    \"mean_reward\": np.mean(rewards),\n",
    "                    \"std_reward\": np.std(rewards),\n",
    "                    \"total_reward\": sum(rewards),\n",
    "                }\n",
    "            else:\n",
    "                stats[f\"arm_{arm}\"] = {\n",
    "                    \"count\": 0,\n",
    "                    \"mean_reward\": 0,\n",
    "                    \"std_reward\": 0,\n",
    "                    \"total_reward\": 0,\n",
    "                }\n",
    "        return stats\n",
    "\n",
    "\n",
    "def simulate_adaptive_ab_test(n_users=2000, n_treatments=3):\n",
    "    \"\"\"\n",
    "    Simulate adaptive A/B testing scenario\n",
    "    \"\"\"\n",
    "    # Generate user features\n",
    "    np.random.seed(42)\n",
    "\n",
    "    user_age = np.random.gamma(2, 20, n_users)\n",
    "    user_income = np.random.lognormal(10, 0.8, n_users)\n",
    "    user_engagement = np.random.beta(2, 3, n_users)\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(\n",
    "        np.column_stack([user_age, user_income, user_engagement])\n",
    "    )\n",
    "    features = np.column_stack([np.ones(n_users), features])  # Add intercept\n",
    "\n",
    "    # True treatment effects (unknown to the bandit)\n",
    "    true_coefficients = {\n",
    "        0: np.array([0.5, 0.1, 0.2, 0.1]),  # Control: modest baseline\n",
    "        1: np.array(\n",
    "            [0.7, 0.2, 0.1, 0.3]\n",
    "        ),  # Treatment A: better for young, engaged users\n",
    "        2: np.array([0.8, -0.1, 0.4, 0.2]),  # Treatment B: better for high-income users\n",
    "    }\n",
    "\n",
    "    # Calculate true rewards for each user under each treatment\n",
    "    true_rewards = np.zeros((n_users, n_treatments))\n",
    "    for user in range(n_users):\n",
    "        for treatment in range(n_treatments):\n",
    "            base_reward = np.dot(true_coefficients[treatment], features[user])\n",
    "            noise = np.random.normal(0, 0.1)\n",
    "            true_rewards[user, treatment] = base_reward + noise\n",
    "\n",
    "    # Optimal treatment for each user\n",
    "    optimal_treatments = np.argmax(true_rewards, axis=1)\n",
    "\n",
    "    return features, true_rewards, optimal_treatments, true_coefficients\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "print(\"🎯 ADAPTIVE A/B TESTING SIMULATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "features, true_rewards, optimal_treatments, true_coeffs = simulate_adaptive_ab_test(\n",
    "    2000, 3\n",
    ")\n",
    "n_users, feature_dim = features.shape\n",
    "n_treatments = len(true_coeffs)\n",
    "\n",
    "print(f\"👥 Users: {n_users:,}\")\n",
    "print(f\"🧪 Treatments: {n_treatments} (Control, Treatment A, Treatment B)\")\n",
    "print(f\"📊 Features: {feature_dim} (intercept + age, income, engagement)\")\n",
    "\n",
    "# Analyze true optimal policy\n",
    "optimal_reward_per_user = np.max(true_rewards, axis=1)\n",
    "total_optimal_reward = optimal_reward_per_user.sum()\n",
    "\n",
    "print(\"\\n🎯 TRUE OPTIMAL POLICY:\")\n",
    "for treatment in range(n_treatments):\n",
    "    pct_optimal = (optimal_treatments == treatment).mean()\n",
    "    print(f\"Treatment {treatment}: {pct_optimal:.1%} of users\")\n",
    "\n",
    "print(f\"📈 Optimal total reward: {total_optimal_reward:.1f}\")\n",
    "\n",
    "# Compare different testing strategies\n",
    "strategies = {\n",
    "    \"Random\": {\"exploration_rate\": 1.0, \"alpha\": 1.0},\n",
    "    \"Greedy\": {\"exploration_rate\": 0.0, \"alpha\": 1.0},\n",
    "    \"ε-Greedy\": {\"exploration_rate\": 0.1, \"alpha\": 1.0},\n",
    "    \"UCB\": {\"exploration_rate\": 0.05, \"alpha\": 2.0},\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for strategy_name, params in strategies.items():\n",
    "    print(f\"\\n🔄 Running {strategy_name} strategy...\")\n",
    "\n",
    "    # Initialize bandit\n",
    "    bandit = CausalMultiArmedBandit(\n",
    "        n_arms=n_treatments,\n",
    "        feature_dim=feature_dim,\n",
    "        alpha=params[\"alpha\"],\n",
    "        exploration_rate=params[\"exploration_rate\"],\n",
    "    )\n",
    "\n",
    "    total_reward = 0\n",
    "    total_regret = 0\n",
    "    regret_history = []\n",
    "\n",
    "    # Sequential decision making\n",
    "    for user in range(n_users):\n",
    "        context = features[user]\n",
    "        user_true_rewards = true_rewards[user]\n",
    "\n",
    "        # Select treatment\n",
    "        chosen_arm, regret = bandit.select_arm(context, user_true_rewards)\n",
    "\n",
    "        # Observe reward\n",
    "        reward = user_true_rewards[chosen_arm]\n",
    "\n",
    "        # Update bandit\n",
    "        bandit.update(chosen_arm, context, reward)\n",
    "\n",
    "        # Track performance\n",
    "        total_reward += reward\n",
    "        total_regret += regret\n",
    "        regret_history.append(total_regret)\n",
    "\n",
    "    # Store results\n",
    "    results[strategy_name] = {\n",
    "        \"total_reward\": total_reward,\n",
    "        \"total_regret\": total_regret,\n",
    "        \"regret_history\": regret_history,\n",
    "        \"arm_stats\": bandit.get_arm_statistics(),\n",
    "        \"bandit\": bandit,\n",
    "    }\n",
    "\n",
    "    print(f\"✅ Total reward: {total_reward:.1f}\")\n",
    "    print(f\"📉 Total regret: {total_regret:.1f}\")\n",
    "    print(f\"🎯 Efficiency: {(total_reward / total_optimal_reward) * 100:.1f}%\")\n",
    "\n",
    "# Compare strategies\n",
    "print(\"\\n📊 STRATEGY COMPARISON:\")\n",
    "print(f\"{'Strategy':<12} {'Total Reward':<12} {'Total Regret':<12} {'Efficiency':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for strategy, result in results.items():\n",
    "    efficiency = (result[\"total_reward\"] / total_optimal_reward) * 100\n",
    "    print(\n",
    "        f\"{strategy:<12} {result['total_reward']:<12.1f} {result['total_regret']:<12.1f} {efficiency:<10.1f}%\"\n",
    "    )\n",
    "\n",
    "# Find best strategy\n",
    "best_strategy = min(results.keys(), key=lambda s: results[s][\"total_regret\"])\n",
    "print(f\"\\n🏆 BEST STRATEGY: {best_strategy}\")\n",
    "print(\n",
    "    f\"⚡ {(1 - results[best_strategy]['total_regret'] / results['Random']['total_regret']) * 100:.1f}% regret reduction vs random!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Adaptive Testing Results\n",
    "\n",
    "print(\"📊 ADAPTIVE TESTING VISUALIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Cumulative regret over time\n",
    "for strategy, result in results.items():\n",
    "    axes[0, 0].plot(result[\"regret_history\"], label=strategy, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title(\"Cumulative Regret Over Time\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"User Number\")\n",
    "axes[0, 0].set_ylabel(\"Cumulative Regret\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Treatment allocation by strategy\n",
    "strategy_names = list(results.keys())\n",
    "treatment_allocations = []\n",
    "\n",
    "for strategy in strategy_names:\n",
    "    arm_stats = results[strategy][\"arm_stats\"]\n",
    "    allocations = [arm_stats[f\"arm_{i}\"][\"count\"] for i in range(n_treatments)]\n",
    "    treatment_allocations.append(allocations)\n",
    "\n",
    "treatment_allocations = np.array(treatment_allocations)\n",
    "\n",
    "x = np.arange(len(strategy_names))\n",
    "width = 0.25\n",
    "\n",
    "for i in range(n_treatments):\n",
    "    axes[0, 1].bar(\n",
    "        x + i * width,\n",
    "        treatment_allocations[:, i],\n",
    "        width,\n",
    "        label=f\"Treatment {i}\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "axes[0, 1].set_title(\"Treatment Allocation by Strategy\", fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Strategy\")\n",
    "axes[0, 1].set_ylabel(\"Number of Users\")\n",
    "axes[0, 1].set_xticks(x + width)\n",
    "axes[0, 1].set_xticklabels(strategy_names)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Efficiency comparison\n",
    "efficiencies = [\n",
    "    (results[s][\"total_reward\"] / total_optimal_reward) * 100 for s in strategy_names\n",
    "]\n",
    "colors = [\"red\" if e < 85 else \"orange\" if e < 95 else \"green\" for e in efficiencies]\n",
    "\n",
    "bars = axes[1, 0].bar(strategy_names, efficiencies, color=colors, alpha=0.8)\n",
    "axes[1, 0].axhline(100, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"Optimal\")\n",
    "axes[1, 0].set_title(\"Strategy Efficiency\", fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"Efficiency (%)\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Add efficiency labels on bars\n",
    "for bar, eff in zip(bars, efficiencies):\n",
    "    axes[1, 0].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 1,\n",
    "        f\"{eff:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Learning curves (treatment selection accuracy over time)\n",
    "best_bandit = results[best_strategy][\"bandit\"]\n",
    "history_arms = best_bandit.history[\"arms\"]\n",
    "history_contexts = best_bandit.history[\"contexts\"]\n",
    "\n",
    "# Calculate accuracy in 100-user windows\n",
    "window_size = 100\n",
    "windows = range(window_size, len(history_arms), window_size)\n",
    "accuracies = []\n",
    "\n",
    "for end_idx in windows:\n",
    "    start_idx = end_idx - window_size\n",
    "    window_arms = history_arms[start_idx:end_idx]\n",
    "    window_optimal = optimal_treatments[start_idx:end_idx]\n",
    "    accuracy = (np.array(window_arms) == window_optimal).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "axes[1, 1].plot(windows, accuracies, \"o-\", linewidth=2, markersize=6, color=\"green\")\n",
    "axes[1, 1].axhline(\n",
    "    1 / n_treatments,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Random ({1 / n_treatments:.1%})\",\n",
    ")\n",
    "axes[1, 1].set_title(f\"{best_strategy}: Learning Curve\", fontweight=\"bold\")\n",
    "axes[1, 1].set_xlabel(\"User Number\")\n",
    "axes[1, 1].set_ylabel(\"Treatment Selection Accuracy\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze final learned treatment effects\n",
    "print(f\"\\n🧠 LEARNED vs TRUE TREATMENT EFFECTS ({best_strategy}):\")\n",
    "print(f\"{'Treatment':<12} {'True Coeff':<25} {'Learned Coeff':<25}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "best_bandit = results[best_strategy][\"bandit\"]\n",
    "for treatment in range(n_treatments):\n",
    "    true_coeff = true_coeffs[treatment]\n",
    "    learned_coeff = best_bandit.theta[treatment]\n",
    "\n",
    "    print(\n",
    "        f\"Treatment {treatment:<3} {str(np.round(true_coeff, 2)):<25} {str(np.round(learned_coeff, 2)):<25}\"\n",
    "    )\n",
    "\n",
    "# Calculate final recommendations\n",
    "print(\"\\n🎯 FINAL ADAPTIVE TESTING INSIGHTS:\")\n",
    "print(\n",
    "    f\"✅ {best_strategy} strategy achieved {efficiencies[strategy_names.index(best_strategy)]:.1f}% efficiency\"\n",
    ")\n",
    "print(\"📈 Learned to identify optimal treatments for most users\")\n",
    "print(\n",
    "    f\"⚡ Reduced regret by {(1 - results[best_strategy]['total_regret'] / results['Random']['total_regret']) * 100:.1f}% vs random assignment\"\n",
    ")\n",
    "print(\"🚀 Adaptive testing outperforms static A/B tests!\")\n",
    "\n",
    "# Business impact\n",
    "static_ab_reward = total_optimal_reward / n_treatments  # Equal allocation\n",
    "adaptive_improvement = (\n",
    "    (results[best_strategy][\"total_reward\"] - static_ab_reward) / static_ab_reward * 100\n",
    ")\n",
    "\n",
    "print(\"\\n💰 BUSINESS IMPACT vs STATIC A/B TEST:\")\n",
    "print(f\"📊 Static A/B test reward: {static_ab_reward:.1f}\")\n",
    "print(f\"🎯 Adaptive test reward: {results[best_strategy]['total_reward']:.1f}\")\n",
    "print(f\"⬆️ Improvement: {adaptive_improvement:.1f}%\")\n",
    "\n",
    "if adaptive_improvement > 10:\n",
    "    print(\"🎉 MASSIVE WIN: Adaptive testing dramatically outperforms static testing!\")\n",
    "elif adaptive_improvement > 5:\n",
    "    print(\"✅ SIGNIFICANT WIN: Clear advantage for adaptive approach!\")\n",
    "else:\n",
    "    print(\"📈 MODEST WIN: Adaptive testing shows promise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## The Innovation Framework: From Analysis to Action\n",
    "\n",
    "### 🔄 The Change-Maker's Process\n",
    "\n",
    "1. **Discover Effects** → Use causal inference to understand heterogeneous treatment effects\n",
    "2. **Design Policy** → Create intervention rules based on individual characteristics  \n",
    "3. **Optimize Allocation** → Use mathematical optimization to maximize outcomes\n",
    "4. **Implement Adaptively** → Deploy systems that learn and improve over time\n",
    "5. **Measure Impact** → Track real-world outcomes and iterate\n",
    "\n",
    "### 🎯 Key Principles for Intervention Design\n",
    "\n",
    "#### 1. Leverage Heterogeneity\n",
    "- **Don't assume homogeneous effects**\n",
    "- **Target interventions** based on individual characteristics\n",
    "- **Avoid one-size-fits-all** policies\n",
    "\n",
    "#### 2. Optimize Multiple Objectives\n",
    "- **Balance efficiency and equity** (education example)\n",
    "- **Consider both short and long-term outcomes** (pricing example)\n",
    "- **Account for unintended consequences**\n",
    "\n",
    "#### 3. Embrace Adaptive Systems\n",
    "- **Learn from data continuously** (bandit algorithms)\n",
    "- **Update policies as new information arrives**\n",
    "- **Balance exploration and exploitation**\n",
    "\n",
    "#### 4. Design for Real-World Implementation\n",
    "- **Consider operational constraints** (budget, capacity)\n",
    "- **Plan for scalability**\n",
    "- **Build in feedback mechanisms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Innovator's Intervention Design Toolkit\n",
    "\n",
    "\n",
    "def intervention_design_framework():\n",
    "    \"\"\"\n",
    "    A systematic framework for designing causal interventions\n",
    "    \"\"\"\n",
    "\n",
    "    framework = \"\"\"\n",
    "    🛠️ THE INNOVATOR'S INTERVENTION DESIGN FRAMEWORK\n",
    "    ================================================\n",
    "    \n",
    "    PHASE 1: CAUSAL DISCOVERY\n",
    "    -------------------------\n",
    "    ✅ Identify treatment effects using causal ML\n",
    "    ✅ Discover heterogeneous effects across subgroups\n",
    "    ✅ Understand mechanisms and mediators\n",
    "    ✅ Validate causal assumptions\n",
    "    \n",
    "    PHASE 2: POLICY DESIGN\n",
    "    ----------------------\n",
    "    ✅ Define intervention goals and constraints\n",
    "    ✅ Design targeting rules based on individual characteristics\n",
    "    ✅ Optimize allocation to maximize outcomes\n",
    "    ✅ Plan for edge cases and unintended consequences\n",
    "    \n",
    "    PHASE 3: ADAPTIVE IMPLEMENTATION\n",
    "    --------------------------------\n",
    "    ✅ Deploy with built-in learning mechanisms\n",
    "    ✅ Monitor outcomes in real-time\n",
    "    ✅ Update policies based on new data\n",
    "    ✅ Scale successful interventions\n",
    "    \n",
    "    PHASE 4: IMPACT MEASUREMENT\n",
    "    ---------------------------\n",
    "    ✅ Track intended and unintended outcomes\n",
    "    ✅ Measure distributional effects (who benefits?)\n",
    "    ✅ Calculate return on investment\n",
    "    ✅ Iterate and improve continuously\n",
    "    \"\"\"\n",
    "\n",
    "    return framework\n",
    "\n",
    "\n",
    "print(intervention_design_framework())\n",
    "\n",
    "\n",
    "# Intervention Design Checklist\n",
    "def intervention_checklist():\n",
    "    \"\"\"\n",
    "    Checklist for robust intervention design\n",
    "    \"\"\"\n",
    "\n",
    "    checklist = \"\"\"\n",
    "    ✅ INTERVENTION DESIGN CHECKLIST\n",
    "    ===============================\n",
    "    \n",
    "    CAUSAL FOUNDATIONS:\n",
    "    □ Used rigorous causal inference methods\n",
    "    □ Validated identifying assumptions\n",
    "    □ Checked for unmeasured confounding\n",
    "    □ Explored heterogeneous effects\n",
    "    \n",
    "    POLICY OPTIMIZATION:\n",
    "    □ Defined clear, measurable objectives\n",
    "    □ Considered multiple stakeholder perspectives\n",
    "    □ Accounted for resource constraints\n",
    "    □ Designed for real-world implementation\n",
    "    \n",
    "    ETHICAL CONSIDERATIONS:\n",
    "    □ Ensured equitable access and outcomes\n",
    "    □ Minimized potential harms\n",
    "    □ Obtained appropriate permissions/consent\n",
    "    □ Planned for transparency and accountability\n",
    "    \n",
    "    ADAPTIVE MECHANISMS:\n",
    "    □ Built in continuous learning\n",
    "    □ Designed feedback loops\n",
    "    □ Planned for policy updates\n",
    "    □ Created early warning systems\n",
    "    \n",
    "    IMPACT MEASUREMENT:\n",
    "    □ Defined success metrics upfront\n",
    "    □ Planned for longitudinal tracking\n",
    "    □ Considered spillover effects\n",
    "    □ Prepared for scale-up decisions\n",
    "    \"\"\"\n",
    "\n",
    "    return checklist\n",
    "\n",
    "\n",
    "print(intervention_checklist())\n",
    "\n",
    "# Real-World Application Template\n",
    "application_template = \"\"\"\n",
    "🎯 YOUR INTERVENTION DESIGN CHALLENGE\n",
    "====================================\n",
    "\n",
    "Choose a real problem in your domain and apply this framework:\n",
    "\n",
    "STEP 1: PROBLEM DEFINITION\n",
    "• What outcome do you want to improve?\n",
    "• Who are the stakeholders?\n",
    "• What are the current policies/practices?\n",
    "• What constraints do you face?\n",
    "\n",
    "STEP 2: CAUSAL ANALYSIS\n",
    "• What treatments/interventions are possible?\n",
    "• What data do you have or need?\n",
    "• How will you identify causal effects?\n",
    "• What heterogeneity do you expect?\n",
    "\n",
    "STEP 3: INTERVENTION DESIGN\n",
    "• How will you target interventions?\n",
    "• What are your optimization objectives?\n",
    "• How will you handle constraints?\n",
    "• What could go wrong?\n",
    "\n",
    "STEP 4: IMPLEMENTATION PLAN\n",
    "• How will you deploy and test?\n",
    "• What will you measure and when?\n",
    "• How will you adapt based on results?\n",
    "• How will you scale if successful?\n",
    "\n",
    "EXAMPLES TO INSPIRE YOU:\n",
    "• Healthcare: Personalized treatment protocols\n",
    "• Education: Adaptive learning systems  \n",
    "• Marketing: Dynamic pricing and promotions\n",
    "• Policy: Targeted social programs\n",
    "• Finance: Risk-based lending decisions\n",
    "• HR: Personalized employee development\n",
    "\"\"\"\n",
    "\n",
    "print(application_template)\n",
    "\n",
    "print(\"\\n🚀 READY TO CHANGE THE WORLD?\")\n",
    "print(\"Use this framework to transform your causal insights into real interventions!\")\n",
    "print(\n",
    "    \"Remember: The goal isn't just to understand - it's to improve outcomes for real people.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary: The Power of Applied Causal Innovation\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Educational Intervention Design**: Turned heterogeneous effect insights into optimal tutoring policies that improve both equity and efficiency\n",
    "\n",
    "2. **Dynamic Pricing Optimization**: Created personalized pricing strategies that boost revenue while maintaining customer satisfaction\n",
    "\n",
    "3. **Adaptive A/B Testing**: Built intelligent testing systems that learn and optimize in real-time\n",
    "\n",
    "### 🌟 Key Innovations Mastered\n",
    "\n",
    "- **Heterogeneity-Aware Policies**: Moving beyond one-size-fits-all to personalized interventions\n",
    "- **Multi-Objective Optimization**: Balancing competing goals like efficiency, equity, and satisfaction\n",
    "- **Adaptive Systems**: Building interventions that learn and improve continuously\n",
    "- **Real-World Implementation**: Designing for practical constraints and scalability\n",
    "\n",
    "### 🎯 The Change-Maker's Mindset\n",
    "\n",
    "**Traditional thinking**: \"We found an effect, job done!\"\n",
    "\n",
    "**Revolutionary thinking**: \"How do we use this insight to improve real outcomes for real people?\"\n",
    "\n",
    "### 💡 Core Principles\n",
    "\n",
    "1. **Start with Causal Understanding** → Use rigorous methods to understand true effects\n",
    "2. **Design for Heterogeneity** → Tailor interventions to individual characteristics\n",
    "3. **Optimize Holistically** → Consider multiple objectives and stakeholders\n",
    "4. **Build Adaptive Systems** → Create interventions that learn and improve\n",
    "5. **Measure Real Impact** → Track outcomes that matter to people\n",
    "\n",
    "### 🚀 The Transformation\n",
    "\n",
    "You've learned to go beyond analysis to create **systems that change outcomes**:\n",
    "\n",
    "- **From correlation to causation** to **intervention**\n",
    "- **From static policies** to **adaptive optimization**\n",
    "- **From average effects** to **personalized targeting**\n",
    "- **From one-shot studies** to **continuous learning**\n",
    "\n",
    "### Next Steps in Your Innovation Journey\n",
    "\n",
    "Continue revolutionizing the field with:\n",
    "- **Tutorial 5**: Push Forward - Advanced causal methods\n",
    "- **Tutorial 6**: The Impossible - Causal inference with minimal data\n",
    "- **Tutorial 7**: Revolutionary Thinking - Creating new methods\n",
    "\n",
    "---\n",
    "\n",
    "*\"The people who think they can change the world are the ones who do.\" You now have the tools and mindset to turn causal insights into real-world impact. Go forth and change things!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
